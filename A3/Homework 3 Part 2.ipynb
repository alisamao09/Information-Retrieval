{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9egyhuvU3_GI"
   },
   "source": [
    "## SI 650 / EECS 549: Homework 3 Part 2\n",
    "\n",
    "Homework 3 Part 2 will have you working with deep learning models in a variety of ways. You will likely need to run this on Great Lakes unless you have access to a GPU elsewhere (or be prepared to wait a long time). You should have completed Parts 1 and 2 before attempting this notebook to familiarize yourself with how PyTerrier works.\n",
    "\n",
    "In Part 3, you'll try the following tasks:\n",
    " - Use a large language model to re-rank content\n",
    " - Use a text-to-text model to perform query augmentation\n",
    " - Train a deep learning IR model and compare its performance.\n",
    " \n",
    "The first two of these tasks will rely on models that we've pretrained for you. However, we've also provided code for how to train these. In the third task, you'll do one simple training in evaluate.\n",
    "\n",
    "For the first two tasks, we've provided most of the code. *You are expected to submit results showing that you have successfully executed it*. You'll need to understand some of the code to complete task 3, which requires you to write new code.\n",
    "\n",
    "As with the past notebooks, you'll need to have `JAVA_HOME` set, which will need to be run on Great Lakes. You can potentially set it in the notebook with a Jupyter command like\n",
    "```\n",
    "!export JAVA_HOME=/fill/in/the/path/to/the/JDK/here\n",
    "```\n",
    "by first figuring out where the JDK is installed. (This is setting the `JAVA_HOME` environment variable in the unix way). \n",
    "\n",
    "You can work on Parts 1 and 2 separately, so feel free to do the GPU-part (Part 2) when the resources are available on Great Lakes and the non-GPU part on your laptop or on a non-GPU Great Lakes machine (which likely will be much faster). Note that there will be no extensions due to Great Lakes bottlenecks, so you are encouraged to complete Part 2 as soon as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV0C6jJvqhMR"
   },
   "source": [
    "### Install the PyTerrier extensions\n",
    "\n",
    "You'll need two extensions for [OpenNIR](https://opennir.net/) and [doc2query](https://github.com/terrierteam/pyterrier_doc2query). We've provided the package install commands in comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AkIR_PXdet7R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/Georgetown-IR-Lab/OpenNIR\n",
      "  Cloning https://github.com/Georgetown-IR-Lab/OpenNIR to /tmp/pip-req-build-kqj247bw\n",
      "  Running command git clone -q https://github.com/Georgetown-IR-Lab/OpenNIR /tmp/pip-req-build-kqj247bw\n",
      "  Resolved https://github.com/Georgetown-IR-Lab/OpenNIR to commit 88a4679372f471a04d284a99404ffce2b7a1dc49\n",
      "Collecting torch>=1.3.1\n",
      "  Downloading torch-1.12.1-cp39-cp39-manylinux1_x86_64.whl (776.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 776.4 MB 29 kB/s /s eta 0:00:01\n",
      "\u001b[?25hCollecting pytorch-pretrained-bert==0.6.1\n",
      "  Downloading pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 62.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytorch-transformers==1.1.0\n",
      "  Downloading pytorch_transformers-1.1.0-py3-none-any.whl (158 kB)\n",
      "\u001b[K     |████████████████████████████████| 158 kB 14.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers>=0.1.1\n",
      "  Downloading tokenizers-0.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 87.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers>=4.3.3\n",
      "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.3 MB 36.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.38.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from OpenNIR==0.1.0) (4.62.3)\n",
      "Collecting colorlog==4.0.2\n",
      "  Downloading colorlog-4.0.2-py2.py3-none-any.whl (17 kB)\n",
      "Collecting terminaltables>=3.1.0\n",
      "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: matplotlib>=3.0.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from OpenNIR==0.1.0) (3.4.3)\n",
      "Requirement already satisfied: seaborn>=0.9.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from OpenNIR==0.1.0) (0.11.2)\n",
      "Collecting python-ternary>=1.0.6\n",
      "  Downloading python_ternary-1.0.8-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9.3 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from OpenNIR==0.1.0) (4.10.0)\n",
      "Requirement already satisfied: html5lib>=1.0.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from OpenNIR==0.1.0) (1.1)\n",
      "Requirement already satisfied: Unidecode>=1.0.22 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from OpenNIR==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: nltk>=3.4.5 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from OpenNIR==0.1.0) (3.6.5)\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Downloading sqlitedict-2.0.0.tar.gz (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 1.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pytrec-eval-terrier>=0.5.1\n",
      "  Downloading pytrec_eval_terrier-0.5.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "\u001b[K     |████████████████████████████████| 287 kB 77.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gensim>=3.7.3\n",
      "  Downloading gensim-4.2.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.0 MB 108.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Cython>=0.29.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from OpenNIR==0.1.0) (0.29.24)\n",
      "Collecting pyjnius>=1.2.1\n",
      "  Downloading pyjnius-1.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 89.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ir_datasets>=0.2.0\n",
      "  Downloading ir_datasets-0.5.4-py3-none-any.whl (311 kB)\n",
      "\u001b[K     |████████████████████████████████| 311 kB 43.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ir_measures>=0.2.1\n",
      "  Downloading ir_measures-0.3.1.tar.gz (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 2.1 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting pytools>=2018.5.2\n",
      "  Downloading pytools-2022.1.12.tar.gz (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 4.7 MB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cached-property>=1.5.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from OpenNIR==0.1.0) (1.5.2)\n",
      "Requirement already satisfied: numpy in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pytorch-pretrained-bert==0.6.1->OpenNIR==0.1.0) (1.20.3)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.25.1-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 69.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pytorch-pretrained-bert==0.6.1->OpenNIR==0.1.0) (2021.8.3)\n",
      "Requirement already satisfied: requests in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pytorch-pretrained-bert==0.6.1->OpenNIR==0.1.0) (2.26.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 49.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from beautifulsoup4>=4.9.3->OpenNIR==0.1.0) (2.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from gensim>=3.7.3->OpenNIR==0.1.0) (1.7.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/maojy/.local/lib/python3.9/site-packages (from gensim>=3.7.3->OpenNIR==0.1.0) (5.2.1)\n",
      "Requirement already satisfied: six>=1.9 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from html5lib>=1.0.1->OpenNIR==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from html5lib>=1.0.1->OpenNIR==0.1.0) (0.5.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from ir_datasets>=0.2.0->OpenNIR==0.1.0) (6.0)\n",
      "Collecting trec-car-tools>=2.5.4\n",
      "  Downloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
      "Collecting zlib-state>=0.1.3\n",
      "  Downloading zlib_state-0.1.5-cp39-cp39-manylinux2010_x86_64.whl (71 kB)\n",
      "\u001b[K     |████████████████████████████████| 71 kB 200 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting unlzw3>=0.2.1\n",
      "  Downloading unlzw3-0.2.1.tar.gz (5.8 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyautocorpus>=0.1.1\n",
      "  Downloading pyautocorpus-0.1.9-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (293 kB)\n",
      "\u001b[K     |████████████████████████████████| 293 kB 63.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting warc3-wet-clueweb09>=0.2.5\n",
      "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
      "Collecting lz4>=3.1.1\n",
      "  Downloading lz4-4.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 69.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting warc3-wet>=0.2.3\n",
      "  Downloading warc3_wet-0.2.3-py3-none-any.whl (13 kB)\n",
      "Collecting ijson>=3.1.3\n",
      "  Downloading ijson-3.1.4-cp39-cp39-manylinux2010_x86_64.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 106.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: lxml>=4.5.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from ir_datasets>=0.2.0->OpenNIR==0.1.0) (4.6.3)\n",
      "Collecting cwl-eval>=1.0.10\n",
      "  Downloading cwl-eval-1.0.12.tar.gz (31 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from matplotlib>=3.0.2->OpenNIR==0.1.0) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from matplotlib>=3.0.2->OpenNIR==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from matplotlib>=3.0.2->OpenNIR==0.1.0) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from matplotlib>=3.0.2->OpenNIR==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from matplotlib>=3.0.2->OpenNIR==0.1.0) (8.4.0)\n",
      "Requirement already satisfied: click in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from nltk>=3.4.5->OpenNIR==0.1.0) (8.0.3)\n",
      "Requirement already satisfied: joblib in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from nltk>=3.4.5->OpenNIR==0.1.0) (1.1.0)\n",
      "Collecting platformdirs>=2.2.0\n",
      "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
      "Collecting typing_extensions>=4.0\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->pytorch-pretrained-bert==0.6.1->OpenNIR==0.1.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->pytorch-pretrained-bert==0.6.1->OpenNIR==0.1.0) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->pytorch-pretrained-bert==0.6.1->OpenNIR==0.1.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->pytorch-pretrained-bert==0.6.1->OpenNIR==0.1.0) (3.2)\n",
      "Requirement already satisfied: pandas>=0.23 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from seaborn>=0.9.0->OpenNIR==0.1.0) (1.3.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pandas>=0.23->seaborn>=0.9.0->OpenNIR==0.1.0) (2021.3)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 74.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers>=4.3.3->OpenNIR==0.1.0) (3.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers>=4.3.3->OpenNIR==0.1.0) (21.0)\n",
      "Collecting cbor>=1.0.0\n",
      "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
      "Collecting botocore<1.29.0,>=1.28.1\n",
      "  Downloading botocore-1.28.1-py3-none-any.whl (9.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.3 MB 68.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 4.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: OpenNIR, ir-measures, cwl-eval, pytools, sqlitedict, cbor, unlzw3, warc3-wet-clueweb09\n",
      "  Building wheel for OpenNIR (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for OpenNIR: filename=OpenNIR-0.1.0-py3-none-any.whl size=55851485 sha256=462ab637ef721a7f2601e8a4c3a901ec72c2a6dfb3bfbc897a49cb0568c5e7dd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ec9fnii1/wheels/d1/f8/fa/f842dc002e8a009eef13f7b049ed65b1bb40e3aa68b0ec5f16\n",
      "  Building wheel for ir-measures (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ir-measures: filename=ir_measures-0.3.1-py3-none-any.whl size=60192 sha256=48c69a89561cb1148921396ca83032e9a8e37872b44a1e61627feb492c7259a9\n",
      "  Stored in directory: /home/maojy/.cache/pip/wheels/00/fa/7d/0ba22fce7d11d69df68c31b7c247fbbb28fe2d4c610f8c653f\n",
      "  Building wheel for cwl-eval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cwl-eval: filename=cwl_eval-1.0.12-py3-none-any.whl size=38084 sha256=aae3c4f061804f4c03663028900eec3a5d39ee17c60d75e11aa3beb5c77a7256\n",
      "  Stored in directory: /home/maojy/.cache/pip/wheels/0e/52/dc/9d448ff093ccae9b248a41b265d31e2c930eea2d0e84aa29e0\n",
      "  Building wheel for pytools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytools: filename=pytools-2022.1.12-py2.py3-none-any.whl size=65031 sha256=8ef4f43e7588c2df85b40b42d5889fbbb3fff1a2b13eff5ffaf63d64a0221365\n",
      "  Stored in directory: /home/maojy/.cache/pip/wheels/0a/54/93/a4118fdc50f7eb54fce7ce54678967ae2aff53e82bd3762ebb\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.0.0-py3-none-any.whl size=15736 sha256=92922243d18a3e9f46a257bdf9867c943df7cb19c2bedcba2ac5a7074cb033c2\n",
      "  Stored in directory: /home/maojy/.cache/pip/wheels/48/a5/80/fa89dc26af0f4c280b500f5529978552379c1ce8907e0a281c\n",
      "  Building wheel for cbor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cbor: filename=cbor-1.0.0-cp39-cp39-linux_x86_64.whl size=22865 sha256=14a9da3740aef840056c72e2b9a24759c87c9db84d789e70cd65b2a924e05f5c\n",
      "  Stored in directory: /home/maojy/.cache/pip/wheels/ec/10/03/a281e0682ddd4b310431fb25d1a4f53987105267cf46c417f3\n",
      "  Building wheel for unlzw3 (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unlzw3: filename=unlzw3-0.2.1-py3-none-any.whl size=6066 sha256=cf376d6d34fe09daa874f143af8cf95163f39fbcb0e5707ca76e1f75e99d29c5\n",
      "  Stored in directory: /home/maojy/.cache/pip/wheels/35/28/21/5aa5b407ef6ae9fcf709b2d90b9a1cfae43c2988cc4c39843e\n",
      "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18921 sha256=f7de7358201ffd1b58fb0db28d65b2e890d405de8b24541034006f2bb9468172\n",
      "  Stored in directory: /home/maojy/.cache/pip/wheels/7f/22/ed/a11944d7fdf4e94c4206a3f760d385122a4d34d8acc12f71a3\n",
      "Successfully built OpenNIR ir-measures cwl-eval pytools sqlitedict cbor unlzw3 warc3-wet-clueweb09\n",
      "Installing collected packages: jmespath, botocore, typing-extensions, s3transfer, cbor, zlib-state, warc3-wet-clueweb09, warc3-wet, unlzw3, trec-car-tools, torch, tokenizers, sentencepiece, pytrec-eval-terrier, pyautocorpus, platformdirs, lz4, ijson, huggingface-hub, cwl-eval, boto3, transformers, terminaltables, sqlitedict, pytorch-transformers, pytorch-pretrained-bert, pytools, python-ternary, pyjnius, ir-measures, ir-datasets, gensim, colorlog, OpenNIR\n",
      "Successfully installed OpenNIR-0.1.0 boto3-1.25.1 botocore-1.28.1 cbor-1.0.0 colorlog-4.0.2 cwl-eval-1.0.12 gensim-4.2.0 huggingface-hub-0.10.1 ijson-3.1.4 ir-datasets-0.5.4 ir-measures-0.3.1 jmespath-1.0.1 lz4-4.0.2 platformdirs-2.5.2 pyautocorpus-0.1.9 pyjnius-1.4.2 python-ternary-1.0.8 pytools-2022.1.12 pytorch-pretrained-bert-0.6.1 pytorch-transformers-1.1.0 pytrec-eval-terrier-0.5.5 s3transfer-0.6.0 sentencepiece-0.1.97 sqlitedict-2.0.0 terminaltables-3.1.10 tokenizers-0.13.1 torch-1.12.1 transformers-4.23.1 trec-car-tools-2.6 typing-extensions-4.4.0 unlzw3-0.2.1 warc3-wet-0.2.3 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.5\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/terrierteam/pyterrier_doc2query.git\n",
      "  Cloning https://github.com/terrierteam/pyterrier_doc2query.git to /tmp/pip-req-build-8_wqn8q3\n",
      "  Running command git clone -q https://github.com/terrierteam/pyterrier_doc2query.git /tmp/pip-req-build-8_wqn8q3\n",
      "  Resolved https://github.com/terrierteam/pyterrier_doc2query.git to commit 2e9504dfe5df78e7eec741f5075cd8799edeb7b3\n",
      "Collecting python-terrier>=0.5.0\n",
      "  Downloading python-terrier-0.8.1.tar.gz (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 3.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pyterrier-doc2query==0.0.1) (1.3.4)\n",
      "Requirement already satisfied: transformers in /home/maojy/.local/lib/python3.9/site-packages (from pyterrier-doc2query==0.0.1) (4.23.1)\n",
      "Requirement already satisfied: torch in /home/maojy/.local/lib/python3.9/site-packages (from pyterrier-doc2query==0.0.1) (1.12.1)\n",
      "Requirement already satisfied: sentencepiece in /home/maojy/.local/lib/python3.9/site-packages (from pyterrier-doc2query==0.0.1) (0.1.97)\n",
      "Requirement already satisfied: numpy in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.20.3)\n",
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Requirement already satisfied: tqdm in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (4.62.3)\n",
      "Collecting pyjnius~=1.3.0\n",
      "  Downloading pyjnius-1.3.0.0.tar.gz (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 2.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting matchpy\n",
      "  Downloading matchpy-0.5.5-py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 4.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting deprecation\n",
      "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting chest\n",
      "  Downloading chest-0.2.3.tar.gz (9.6 kB)\n",
      "Requirement already satisfied: scipy in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.7.1)\n",
      "Requirement already satisfied: requests in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (2.26.0)\n",
      "Requirement already satisfied: joblib in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.1.0)\n",
      "Collecting nptyping==1.4.4\n",
      "  Downloading nptyping-1.4.4-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: more_itertools in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (8.10.0)\n",
      "Requirement already satisfied: ir_datasets>=0.3.2 in /home/maojy/.local/lib/python3.9/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.5.4)\n",
      "Requirement already satisfied: jinja2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (2.11.3)\n",
      "Requirement already satisfied: statsmodels in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.12.2)\n",
      "Requirement already satisfied: ir_measures>=0.2.0 in /home/maojy/.local/lib/python3.9/site-packages (from python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.3.1)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[K     |████████████████████████████████| 110 kB 57.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typish>=1.7.0\n",
      "  Downloading typish-1.9.3-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 2.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from ir_datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (4.10.0)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /home/maojy/.local/lib/python3.9/site-packages (from ir_datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.2.5)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /home/maojy/.local/lib/python3.9/site-packages (from ir_datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.1.5)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /home/maojy/.local/lib/python3.9/site-packages (from ir_datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.1.9)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from ir_datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (6.0)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /home/maojy/.local/lib/python3.9/site-packages (from ir_datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (2.6)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /home/maojy/.local/lib/python3.9/site-packages (from ir_datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.2.1)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /home/maojy/.local/lib/python3.9/site-packages (from ir_datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.2.3)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from ir_datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (4.6.3)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /home/maojy/.local/lib/python3.9/site-packages (from ir_datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (3.1.4)\n",
      "Requirement already satisfied: lz4>=3.1.1 in /home/maojy/.local/lib/python3.9/site-packages (from ir_datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (4.0.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from beautifulsoup4>=4.4.1->ir_datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (2.2.1)\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /home/maojy/.local/lib/python3.9/site-packages (from ir_measures>=0.2.0->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.0.12)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.2 in /home/maojy/.local/lib/python3.9/site-packages (from ir_measures>=0.2.0->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.5.5)\n",
      "Requirement already satisfied: six>=1.7.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pyjnius~=1.3.0->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.16.0)\n",
      "Requirement already satisfied: cython in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pyjnius~=1.3.0->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.29.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.26.7)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /home/maojy/.local/lib/python3.9/site-packages (from trec-car-tools>=2.5.4->ir_datasets>=0.3.2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.0.0)\n",
      "Requirement already satisfied: heapdict in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from chest->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.0.1)\n",
      "Requirement already satisfied: packaging in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from deprecation->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (21.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from jinja2->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (1.1.1)\n",
      "Collecting multiset<3.0,>=2.0\n",
      "  Downloading multiset-2.1.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from packaging->deprecation->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pandas->pyterrier-doc2query==0.0.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from pandas->pyterrier-doc2query==0.0.1) (2021.3)\n",
      "Requirement already satisfied: scikit-learn in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from sklearn->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from scikit-learn->sklearn->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (2.2.0)\n",
      "Requirement already satisfied: patsy>=0.5 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from statsmodels->python-terrier>=0.5.0->pyterrier-doc2query==0.0.1) (0.5.2)\n",
      "Requirement already satisfied: typing-extensions in /home/maojy/.local/lib/python3.9/site-packages (from torch->pyterrier-doc2query==0.0.1) (4.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/maojy/.local/lib/python3.9/site-packages (from transformers->pyterrier-doc2query==0.0.1) (0.10.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/maojy/.local/lib/python3.9/site-packages (from transformers->pyterrier-doc2query==0.0.1) (0.13.1)\n",
      "Requirement already satisfied: filelock in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers->pyterrier-doc2query==0.0.1) (3.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from transformers->pyterrier-doc2query==0.0.1) (2021.8.3)\n",
      "Building wheels for collected packages: pyterrier-doc2query, python-terrier, pyjnius, chest, sklearn, wget\n",
      "  Building wheel for pyterrier-doc2query (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyterrier-doc2query: filename=pyterrier_doc2query-0.0.1-py3-none-any.whl size=4071 sha256=7f6b054c831fb34d16434b4783d2e87e3147239b3383f46285f106e4d2296174\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fh30yv1m/wheels/e1/cd/4b/83f62b769941ba473f703c0eb5a44b2b5ef05d174fe47a359c\n",
      "  Building wheel for python-terrier (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-terrier: filename=python_terrier-0.8.1-py3-none-any.whl size=104091 sha256=77cd98136e6c1b79955c49475c9dbc55d13d37b783293e3025f7d5574c891059\n",
      "  Stored in directory: /home/maojy/.cache/pip/wheels/38/7b/e5/8fe61e65e1e5f62bf0baa18ee88b7db9c924e860771f65a582\n",
      "  Building wheel for pyjnius (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyjnius: filename=pyjnius-1.3.0-cp39-cp39-linux_x86_64.whl size=271183 sha256=9941e54f4e4c76f645d21c6352912dabb5831244e636f03ce92fbcb6512ec1b6\n",
      "  Stored in directory: /home/maojy/.cache/pip/wheels/b8/cd/a6/035dae0cadefe3cae91fc91a704b5c2d9ef09d2e37b00bc573\n",
      "  Building wheel for chest (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for chest: filename=chest-0.2.3-py3-none-any.whl size=7632 sha256=42c2aabe50f009e4716e06d0f30dee5208083ff077b10d92b38230fd06581ce3\n",
      "  Stored in directory: /home/maojy/.cache/pip/wheels/ce/05/8e/2ecd3728ed5d938ee57161c8acf2ee74f55f894964bb3d31ef\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=d1cc6a39f4ad0774797c36b90fd73490fdfcfe95c1f1417b6468406935dcb46d\n",
      "  Stored in directory: /home/maojy/.cache/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=dc51b2f85df99b09f4f4eddde42230f5d643f007529f148e303458a4a2aa9e78\n",
      "  Stored in directory: /home/maojy/.cache/pip/wheels/04/5f/3e/46cc37c5d698415694d83f607f833f83f0149e49b3af9d0f38\n",
      "Successfully built pyterrier-doc2query python-terrier pyjnius chest sklearn wget\n",
      "Installing collected packages: typish, multiset, wget, sklearn, pyjnius, nptyping, matchpy, dill, deprecation, chest, python-terrier, pyterrier-doc2query\n",
      "  Attempting uninstall: pyjnius\n",
      "    Found existing installation: pyjnius 1.4.2\n",
      "    Uninstalling pyjnius-1.4.2:\n",
      "      Successfully uninstalled pyjnius-1.4.2\n",
      "Successfully installed chest-0.2.3 deprecation-2.1.0 dill-0.3.6 matchpy-0.5.5 multiset-2.1.1 nptyping-1.4.4 pyjnius-1.3.0 pyterrier-doc2query-0.0.1 python-terrier-0.8.1 sklearn-0.0 typish-1.9.3 wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade git+https://github.com/Georgetown-IR-Lab/OpenNIR\n",
    "!pip install --upgrade git+https://github.com/terrierteam/pyterrier_doc2query.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-nQrpNP5pN7"
   },
   "source": [
    "## Getting started\n",
    "\n",
    "Start PyTerrier as we have in past notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export JAVA_HOME='/sw/pkgs/arc/openjdk/jdk-18.0.1.1'\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = '/sw/pkgs/arc/openjdk/jdk-18.0.1.1'\n",
    "os.environ['JVM_PATH']='/sw/pkgs/arc/openjdk/jdk-18.0.1.1/lib/server/libjvm.so'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6FegcyWr5lja"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.8.1 has loaded Terrier 5.6 (built by craigmacdonald on 2021-09-17 13:27)\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init(tqdm='notebook')\n",
    "import onir_pt\n",
    "import pyterrier_doc2query\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hPVjr448rIPc"
   },
   "source": [
    "### [TREC-COVID19](https://ir.nist.gov/covidSubmit/) Dataset download\n",
    "\n",
    "The following cell downloads the [TREC-COVID19](https://ir.nist.gov/covidSubmit/) dataset that we will use periodically throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IJMHFRfArd7O"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [starting] https://ir.nist.gov/covidSubmit/data/topics-rnd5.xml\n",
      "[INFO] [finished] https://ir.nist.gov/covidSubmit/data/topics-rnd5.xml: [1ms] [18.7kB] [17.3MB/s]\n",
      "/home/maojy/.local/lib/python3.9/site-packages/pyterrier/datasets.py:435: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df.drop(df.columns.difference(['qid','query']), 1, inplace=True)\n",
      "[INFO] [starting] https://ir.nist.gov/covidSubmit/data/qrels-covid_d5_j0.5-5.txt\n",
      "[INFO] [finished] https://ir.nist.gov/covidSubmit/data/qrels-covid_d5_j0.5-5.txt: [174ms] [1.14MB] [6.56MB/s]\n",
      "                                                                                           \r"
     ]
    }
   ],
   "source": [
    "dataset = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "topics = dataset.get_topics(variant='description')\n",
    "qrels = dataset.get_qrels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF3HIPhtrqOH"
   },
   "source": [
    "# Task 1: Build the inverted index for the TREC-COVID19 collection (5 points)\n",
    "\n",
    "Build the index for the TREC Covid-19 (CORD19) data like we have in past notebooks but without any fancy options (e.g., no positional indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [starting] building docstore\n",
      "[INFO] If you have a local copy of https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv, you can symlink it here to avoid downloading it again: /home/maojy/.ir_datasets/downloads/80d664e496b8b7e50a39c6f6bb92e0ef\n",
      "[INFO] [starting] https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv\n",
      "docs_iter:   0%|                                    | 0/192509 [399ms<?, ?doc/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 0.0%| 0.00/269M [0ms<?, ?B/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 0.0%| 49.2k/269M [120ms<10:57, 410kB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 0.1%| 270k/269M [222ms<03:41, 1.22MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 0.5%| 1.21M/269M [346ms<01:16, 3.51MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 1.4%| 3.82M/269M [469ms<32.61s, 8.14MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 3.3%| 8.99M/269M [569ms<16.48s, 15.8MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 5.3%| 14.2M/269M [687ms<12.38s, 20.6MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 7.2%| 19.3M/269M [805ms<10.41s, 24.0MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 9.1%| 24.5M/269M [908ms<9.07s, 27.0MB/s] \n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 11.2%| 30.2M/269M [1.01s<7.98s, 30.0MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 13.3%| 35.9M/269M [1.13s<7.34s, 31.8MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 15.5%| 41.6M/269M [1.25s<6.83s, 33.3MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 17.6%| 47.3M/269M [1.37s<6.43s, 34.5MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 19.7%| 53.0M/269M [1.49s<6.08s, 35.6MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 21.8%| 58.7M/269M [1.60s<5.74s, 36.7MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 23.9%| 64.4M/269M [1.72s<5.48s, 37.4MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 26.0%| 70.1M/269M [1.84s<5.23s, 38.0MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 28.2%| 75.9M/269M [1.94s<4.95s, 39.1MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 30.4%| 81.8M/269M [2.07s<4.74s, 39.6MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 32.5%| 87.6M/269M [2.19s<4.53s, 40.1MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 34.7%| 93.4M/269M [2.31s<4.34s, 40.5MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 36.9%| 99.3M/269M [2.43s<4.15s, 40.9MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 39.0%| 105M/269M [2.55s<3.99s, 41.2MB/s] \n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 41.2%| 111M/269M [2.66s<3.79s, 41.8MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 43.4%| 117M/269M [2.77s<3.62s, 42.1MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 45.5%| 123M/269M [2.89s<3.46s, 42.4MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 47.7%| 128M/269M [3.01s<3.30s, 42.6MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 49.9%| 134M/269M [3.14s<3.15s, 42.8MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 52.0%| 140M/269M [3.26s<3.00s, 43.0MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 54.2%| 146M/269M [3.38s<2.85s, 43.2MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 56.4%| 152M/269M [3.50s<2.71s, 43.4MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 58.5%| 158M/269M [3.62s<2.56s, 43.5MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 60.7%| 164M/269M [3.74s<2.42s, 43.7MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 63.1%| 170M/269M [3.87s<2.27s, 43.9MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 65.4%| 176M/269M [4.00s<2.11s, 44.1MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 67.7%| 182M/269M [4.12s<1.96s, 44.3MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 70.1%| 189M/269M [4.24s<1.81s, 44.4MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 72.4%| 195M/269M [4.37s<1.67s, 44.6MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 74.7%| 201M/269M [4.49s<1.52s, 44.8MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 77.1%| 208M/269M [4.62s<1.37s, 44.9MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 79.4%| 214M/269M [4.74s<1.23s, 45.1MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 81.8%| 220M/269M [4.88s<1.09s, 45.1MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 84.1%| 226M/269M [5.00s<946ms, 45.3MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 86.4%| 233M/269M [5.13s<805ms, 45.4MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 88.8%| 239M/269M [5.25s<664ms, 45.5MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 91.1%| 245M/269M [5.37s<525ms, 45.6MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 93.4%| 252M/269M [5.50s<386ms, 45.8MB/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: 95.8%| 258M/269M [5.62s<248ms, 45.9MB/s]\n",
      "                                                                                ta.csv: 98.1%| 264M/269M [5.75s<110ms, 46.0MB/s]\n",
      "[INFO] [finished] https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: [5.85s] [269MB] [46.0MB/s]\n",
      "docs_iter:   0%|                                    | 0/192509 [6.25s<?, ?doc/s]\n",
      "https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/2020-07-16/metadata.csv: [5.85s] [269MB] [46.0MB/s]\n",
      "docs_iter: 100%|█████████████████████| 192509/192509 [16.95s<0ms, 11357.90doc/s]                                  \n",
      "[INFO] [finished] docs_iter: [16.95s] [192509doc] [11357.19doc/s]\n",
      "[INFO] [finished] building docstore [16.97s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946f5e802d1b48e592a8636deb932286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cord19/trec-covid documents:   0%|          | 0/192509 [16ms<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_427917/1126882663.py:7: DeprecationWarning: specifying meta and meta_lengths in IterDictIndexer.index() is deprecated, use constructor instead\n",
      "  index_ref = indexer.index(cord19.get_corpus_iter(),fields=('abstract',),meta=('docno',))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:17:25.746 [ForkJoinPool-1-worker-1] WARN org.terrier.structures.indexing.Indexer - Adding an empty document to the index (6iu1dtyl) - further warnings are suppressed\n",
      "17:18:14.468 [ForkJoinPool-1-worker-1] WARN org.terrier.structures.indexing.Indexer - Indexed 54937 empty documents\n",
      "17:18:14.642 [ForkJoinPool-1-worker-1] ERROR org.terrier.structures.indexing.Indexer - Could not finish MetaIndexBuilder: \n",
      "java.io.IOException: Key 8lqzfj2e is not unique: 37597,11755\n",
      "For MetaIndex, to suppress, set metaindex.compressed.reverse.allow.duplicates=true\n",
      "\tat org.terrier.structures.collections.FSOrderedMapFile$MultiFSOMapWriter.mergeTwo(FSOrderedMapFile.java:1374)\n",
      "\tat org.terrier.structures.collections.FSOrderedMapFile$MultiFSOMapWriter.close(FSOrderedMapFile.java:1308)\n",
      "\tat org.terrier.structures.indexing.BaseMetaIndexBuilder.close(BaseMetaIndexBuilder.java:321)\n",
      "\tat org.terrier.structures.indexing.classical.BasicIndexer.createDirectIndex(BasicIndexer.java:346)\n",
      "\tat org.terrier.structures.indexing.Indexer.index(Indexer.java:369)\n",
      "\tat org.terrier.python.ParallelIndexer$1.apply(ParallelIndexer.java:63)\n",
      "\tat org.terrier.python.ParallelIndexer$1.apply(ParallelIndexer.java:52)\n",
      "\tat java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)\n",
      "\tat java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:992)\n",
      "\tat java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)\n",
      "\tat java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)\n",
      "\tat java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:960)\n",
      "\tat java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:934)\n",
      "\tat java.base/java.util.stream.AbstractTask.compute(AbstractTask.java:327)\n",
      "\tat java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:754)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:686)\n",
      "\tat java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:927)\n",
      "\tat java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)\n",
      "\tat java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:662)\n",
      "\tat org.terrier.python.ParallelIndexer$3.call(ParallelIndexer.java:133)\n",
      "\tat org.terrier.python.ParallelIndexer$3.call(ParallelIndexer.java:130)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1428)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\n"
     ]
    }
   ],
   "source": [
    "cord19 = pt.datasets.get_dataset('irds:cord19/trec-covid')\n",
    "\n",
    "pt_index_path = './cord19'\n",
    "if not os.path.exists(pt_index_path + \"/data.properties\"):\n",
    "    indexer = pt.index.IterDictIndexer(pt_index_path, overwrite=True)\n",
    "    indexer.setProperty(\"termpipelines\", \"Stopwords\")\n",
    "    index_ref = indexer.index(cord19.get_corpus_iter(),fields=('abstract',),meta=('docno',))\n",
    "\n",
    "else:\n",
    "    # if you already have the index, create an IndexRef from the data in pt_index_path\n",
    "    # that we can use to load using the IndexFactory\n",
    "    index_ref = pt.IndexRef.of(pt_index_path + \"/data.properties\")\n",
    "\n",
    "index = pt.IndexFactory.of(index_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwDams5M7g6c"
   },
   "source": [
    "## Using an untuned Re-rankers\n",
    "\n",
    "This notebook will have you work with few neural re-ranking methods that you've used in class. We can build them from scratch using `onir_pt.reranker` or load them from pretrained models. The models we load from scratch won't have been trained to do IR (yet), however.\n",
    "\n",
    "And OpenNIR reranking model consists of:\n",
    " - `ranker` (e.g., `drmm`, `knrm`, or `pacrr`). This defines the neural ranking architecture. We discussed the `knrm` approach in class.\n",
    " - `vocab` (e.g., `wordvec_hash`, or `bert`). This defines how text is encoded by the model. This approach makes it easy to swap out different text representations. \n",
    " \n",
    "Let's start with the `knrm` method we discussed in class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "F0O79K2K6fvn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file not found: config\n",
      "[2022-11-01 17:18:41,594][WordvecHashVocab][DEBUG] [starting] downloading https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-01 17:18:58,897][onir.util.download][WARNING] no hash provided for https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip; consider adding expected_md5=\"3cc8839ac3fa9a6187149b1e73328b2a\" to ensure data integrity.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-01 17:18:58,902][onir.util.download][DEBUG] downloaded https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip [16.71s] [682M] [34.9MB/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-01 17:18:58,907][WordvecHashVocab][DEBUG] [finished] downloading https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip [17.32s]\n",
      "[2022-11-01 17:18:58,908][WordvecHashVocab][DEBUG] [starting] extracting vecs\n",
      "[2022-11-01 17:19:10,239][WordvecHashVocab][DEBUG] [finished] extracting vecs [11.33s]\n",
      "[2022-11-01 17:19:10,243][WordvecHashVocab][DEBUG] [starting] loading vecs into memory\n",
      "[2022-11-01 17:21:51,886][WordvecHashVocab][DEBUG] [finished] loading vecs into memory [02:42]\n",
      "[2022-11-01 17:21:52,118][WordvecHashVocab][DEBUG] [starting] writing cached at /home/maojy/data/onir/vocab/wordvec_hash/fasttext-wiki-news-300d-1M.p\n",
      "[2022-11-01 17:21:58,498][WordvecHashVocab][DEBUG] [finished] writing cached at /home/maojy/data/onir/vocab/wordvec_hash/fasttext-wiki-news-300d-1M.p [6.38s]\n"
     ]
    }
   ],
   "source": [
    "knrm = onir_pt.reranker('knrm', 'wordvec_hash', text_field='abstract')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1avVTpxDORN"
   },
   "source": [
    "Let's look at how well this model work at ranking compared with our default `BatchRetrieve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4FWUIN577v1O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-01 17:24:17,533][onir_pt][ERROR] gpu=True, but CUDA is not available. Falling back on CPU.\n",
      "[2022-11-01 17:24:17,665][onir_pt][DEBUG] [starting] batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batches:   0%|          | 0/1250 [14ms<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-01 17:24:22,425][onir_pt][DEBUG] [finished] batches: [4.76s] [1250it] [262.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>map</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>ndcg_cut.10</th>\n",
       "      <th>P.10</th>\n",
       "      <th>mrt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DPH</td>\n",
       "      <td>0.058707</td>\n",
       "      <td>0.151733</td>\n",
       "      <td>0.576693</td>\n",
       "      <td>0.622</td>\n",
       "      <td>59.544833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DPH &gt;&gt; KNRM</td>\n",
       "      <td>0.045271</td>\n",
       "      <td>0.132041</td>\n",
       "      <td>0.320171</td>\n",
       "      <td>0.372</td>\n",
       "      <td>134.598549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name       map      ndcg  ndcg_cut.10   P.10         mrt\n",
       "0          DPH  0.058707  0.151733     0.576693  0.622   59.544833\n",
       "1  DPH >> KNRM  0.045271  0.132041     0.320171  0.372  134.598549"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br = pt.BatchRetrieve(index) % 100\n",
    "pipeline = br >> pt.text.get_text(dataset, 'abstract') >> knrm\n",
    "pt.Experiment(\n",
    "    [br, pipeline],\n",
    "    topics,\n",
    "    qrels,\n",
    "    names=['DPH', 'DPH >> KNRM'],\n",
    "    eval_metrics=[\"map\", \"ndcg\", 'ndcg_cut.10', 'P.10', 'mrt']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhquobQypVNJ"
   },
   "source": [
    "The `knrm` models' performance is lower! The mode doesn't work very well because it hasn't yet been trained for IR; it's using random weights to combine the scores from the similarity matrix--but this is at least a start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLv7quA43yAP"
   },
   "source": [
    "## Loading a trained re-ranker\n",
    "\n",
    "You can train re-ranking models in PyTerrier using the `fit` method. Here's an example of how to train the `knrm` model on the MS MARCO dataset, which is a large IR collection.\n",
    "\n",
    "```python\n",
    "# transfer training signals from a medical sample of MS MARCO\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_ds = pt.datasets.get_dataset('irds:msmarco-passage/train/medical')\n",
    "train_topics, valid_topics = train_test_split(train_ds.get_topics(), test_size=50, random_state=42) # split into training and validation sets\n",
    "\n",
    "# Index MS MARCO\n",
    "indexer = pt.index.IterDictIndexer('./terrier_msmarco-passage')\n",
    "tr_index_ref = indexer.index(train_ds.get_corpus_iter(), fields=('text',), meta=('docno',))\n",
    "\n",
    "pipeline = (pt.BatchRetrieve(tr_index_ref) % 100 # get top 100 results\n",
    "            >> pt.text.get_text(train_ds, 'text') # fetch the document text\n",
    "            >> pt.apply.generic(lambda df: df.rename(columns={'text': 'abstract'})) # rename columns\n",
    "            >> knrm) # apply neural re-ranker\n",
    "\n",
    "pipeline.fit(\n",
    "    train_topics,\n",
    "    train_ds.get_qrels(),\n",
    "    valid_topics,\n",
    "    train_ds.get_qrels())\n",
    "```\n",
    "\n",
    "Training deep learning models takes a bit of time (especially for large datasets like MS MARCO), so we've provided a model that's already been trained for you to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Yk7FBOgvDa8V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-01 17:24:37,886][onir.util.download][DEBUG] downloaded https://macavaney.us/knrm.medmarco.tar.gz [1ms] [1.43k] [1.66MB/s] [md5 hash verified]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-01 17:24:37,897][WordvecHashVocab][DEBUG] [starting] reading cached at /home/maojy/data/onir/vocab/wordvec_hash/fasttext-wiki-news-300d-1M.p\n",
      "[2022-11-01 17:24:46,423][WordvecHashVocab][DEBUG] [finished] reading cached at /home/maojy/data/onir/vocab/wordvec_hash/fasttext-wiki-news-300d-1M.p [8.53s]\n"
     ]
    }
   ],
   "source": [
    "del knrm # free up the memory before loading a new version of the ranker (helpful for the GPU)\n",
    "knrm = onir_pt.reranker.from_checkpoint('https://macavaney.us/knrm.medmarco.tar.gz', text_field='abstract', \n",
    "                                        expected_md5=\"d70b1d4f899690dae51161537e69ed5a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1BQQKv8lL0Ta"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-01 17:24:54,822][onir_pt][ERROR] gpu=True, but CUDA is not available. Falling back on CPU.\n",
      "[2022-11-01 17:24:54,824][onir_pt][DEBUG] [starting] batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batches:   0%|          | 0/1250 [15ms<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-01 17:24:58,858][onir_pt][DEBUG] [finished] batches: [4.03s] [1250it] [309.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>map</th>\n",
       "      <th>P.10</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>ndcg_cut.10</th>\n",
       "      <th>mrt</th>\n",
       "      <th>map +</th>\n",
       "      <th>map -</th>\n",
       "      <th>map p-value</th>\n",
       "      <th>P.10 +</th>\n",
       "      <th>P.10 -</th>\n",
       "      <th>P.10 p-value</th>\n",
       "      <th>ndcg +</th>\n",
       "      <th>ndcg -</th>\n",
       "      <th>ndcg p-value</th>\n",
       "      <th>ndcg_cut.10 +</th>\n",
       "      <th>ndcg_cut.10 -</th>\n",
       "      <th>ndcg_cut.10 p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DPH</td>\n",
       "      <td>0.058707</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.151733</td>\n",
       "      <td>0.576693</td>\n",
       "      <td>38.694083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DPH &gt;&gt; KNRM</td>\n",
       "      <td>0.056452</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.147442</td>\n",
       "      <td>0.523946</td>\n",
       "      <td>117.816260</td>\n",
       "      <td>18.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.169177</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.138233</td>\n",
       "      <td>16.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.052831</td>\n",
       "      <td>20.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.038403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name       map   P.10      ndcg  ndcg_cut.10         mrt  map +  \\\n",
       "0          DPH  0.058707  0.622  0.151733     0.576693   38.694083    NaN   \n",
       "1  DPH >> KNRM  0.056452  0.584  0.147442     0.523946  117.816260   18.0   \n",
       "\n",
       "   map -  map p-value  P.10 +  P.10 -  P.10 p-value  ndcg +  ndcg -  \\\n",
       "0    NaN          NaN     NaN     NaN           NaN     NaN     NaN   \n",
       "1   32.0     0.169177    18.0    21.0      0.138233    16.0    34.0   \n",
       "\n",
       "   ndcg p-value  ndcg_cut.10 +  ndcg_cut.10 -  ndcg_cut.10 p-value  \n",
       "0           NaN            NaN            NaN                  NaN  \n",
       "1      0.052831           20.0           29.0             0.038403  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline2 = br >> pt.text.get_text(dataset, 'abstract') >> knrm\n",
    "pt.Experiment(\n",
    "    [br, pipeline2],\n",
    "    topics,\n",
    "    qrels,\n",
    "    names=['DPH', 'DPH >> KNRM'],\n",
    "    baseline=0,\n",
    "    eval_metrics=[\"map\", \"ndcg\", 'ndcg_cut.10', 'P.10', 'mrt']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aI1_8O8rtXKK"
   },
   "source": [
    "The tuned performance is a little better than before, but `knrm` still underperforms our first-stage ranking model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79l1jn0pRQEY"
   },
   "source": [
    "## Reranking with BERT\n",
    "\n",
    "Large language models such as [BERT](https://arxiv.org/abs/1810.04805) are much more powerful neural models that have been shown to be effective for ranking like we discussed in class. \n",
    "\n",
    "Like with `knrm`, we'll start by using BERT for re-ranking with its initial parameters. These parameters have been turned for the masked language modeling (i.e., filling a word in the blank) and predicting the next sentence--but have not been tuned for IR at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-qlXPHqN3iO0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [66ms<0ms, 3526462.34B/s]\n",
      "100%|██████████| 433/433 [1ms<0ms, 647693.88B/s]\n",
      "100%|██████████| 440473133/440473133 [5.49s<0ms, 80298554.35B/s]  \n"
     ]
    }
   ],
   "source": [
    "del knrm # clear out memory from KNRM (useful for GPU)\n",
    "vbert = onir_pt.reranker('vanilla_transformer', 'bert', text_field='abstract', vocab_config={'train': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "progrVwaunrn"
   },
   "source": [
    "Let's see how this non-IR trained model does on CORD10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "PkasovrjQjy0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-01 17:25:25,032][onir_pt][ERROR] gpu=True, but CUDA is not available. Falling back on CPU.\n",
      "[2022-11-01 17:25:25,037][onir_pt][DEBUG] [starting] batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batches:   0%|          | 0/1250 [14ms<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-01 17:33:16,958][onir_pt][DEBUG] [finished] batches: [07:52] [1250it] [ 2.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>map</th>\n",
       "      <th>P.10</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>ndcg_cut.10</th>\n",
       "      <th>mrt</th>\n",
       "      <th>map +</th>\n",
       "      <th>map -</th>\n",
       "      <th>map p-value</th>\n",
       "      <th>P.10 +</th>\n",
       "      <th>P.10 -</th>\n",
       "      <th>P.10 p-value</th>\n",
       "      <th>ndcg +</th>\n",
       "      <th>ndcg -</th>\n",
       "      <th>ndcg p-value</th>\n",
       "      <th>ndcg_cut.10 +</th>\n",
       "      <th>ndcg_cut.10 -</th>\n",
       "      <th>ndcg_cut.10 p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DPH</td>\n",
       "      <td>0.058707</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.151733</td>\n",
       "      <td>0.576693</td>\n",
       "      <td>34.342954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DPH &gt;&gt; VBERT</td>\n",
       "      <td>0.046275</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.131818</td>\n",
       "      <td>0.310408</td>\n",
       "      <td>9477.003205</td>\n",
       "      <td>7.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>4.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.698767e-08</td>\n",
       "      <td>5.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>9.261981e-08</td>\n",
       "      <td>4.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.048371e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name       map   P.10      ndcg  ndcg_cut.10          mrt  map +  \\\n",
       "0           DPH  0.058707  0.622  0.151733     0.576693    34.342954    NaN   \n",
       "1  DPH >> VBERT  0.046275  0.380  0.131818     0.310408  9477.003205    7.0   \n",
       "\n",
       "   map -  map p-value  P.10 +  P.10 -  P.10 p-value  ndcg +  ndcg -  \\\n",
       "0    NaN          NaN     NaN     NaN           NaN     NaN     NaN   \n",
       "1   43.0     0.000001     4.0    41.0  1.698767e-08     5.0    45.0   \n",
       "\n",
       "   ndcg p-value  ndcg_cut.10 +  ndcg_cut.10 -  ndcg_cut.10 p-value  \n",
       "0           NaN            NaN            NaN                  NaN  \n",
       "1  9.261981e-08            4.0           45.0         1.048371e-09  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline3 = br % 100 >> pt.text.get_text(dataset, 'abstract') >> vbert\n",
    "pt.Experiment(\n",
    "    [br, pipeline3],\n",
    "    topics,\n",
    "    qrels,\n",
    "    names=['DPH', 'DPH >> VBERT'],\n",
    "    baseline=0,\n",
    "    eval_metrics=[\"map\", \"ndcg\", 'ndcg_cut.10', 'P.10', 'mrt']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBrfNZ_1u_pD"
   },
   "source": [
    "As we see, although the ERT model is pre-trained for recognizing language, it doesn't do very well at ranking on our benchmark. To get better performance, we'll need to tune for the task of relevance ranking.\n",
    "\n",
    "We can train the model for ranking (as shown above for KNRM) or we can download a trained model. Here, we will use the [SLEDGE](https://arxiv.org/abs/2010.05987) model, which is a BERT model trained on scientific text and tuned on medical queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VsXQKNyYSXOj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-01 17:33:23,083][onir.util.download][DEBUG] downloaded https://macavaney.us/scibert-medmarco.tar.gz [5.86s] [499M] [92.8MB/s] [md5 hash verified]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-01 17:33:40,469][onir.util.download][DEBUG] downloaded https://s3-us-west-2.amazonaws.com/ai2-s2-research/scibert/pytorch_models/scibert_scivocab_uncased.tar [11.00s] [411M] [42.4MB/s] [md5 hash verified]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extracting: 411MB [2.54s, 162MB/s]                                                                                                                   \n",
      "extracting: 821MB [7.19s, 114MB/s]  \n"
     ]
    }
   ],
   "source": [
    "vbert = onir_pt.reranker.from_checkpoint('https://macavaney.us/scibert-medmarco.tar.gz', \n",
    "                                         text_field='abstract', expected_md5=\"854966d0b61543ffffa44cea627ab63b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run another experiment to see how this new model trained for IR does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dUH-daNJSoNy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-01 17:34:04,772][onir_pt][ERROR] gpu=True, but CUDA is not available. Falling back on CPU.\n",
      "[2022-11-01 17:34:04,776][onir_pt][DEBUG] [starting] batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "batches:   0%|          | 0/1250 [14ms<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-01 17:41:00,991][onir_pt][DEBUG] [finished] batches: [06:56] [1250it] [ 3.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>map</th>\n",
       "      <th>P.10</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>ndcg_cut.10</th>\n",
       "      <th>mrt</th>\n",
       "      <th>map +</th>\n",
       "      <th>map -</th>\n",
       "      <th>map p-value</th>\n",
       "      <th>P.10 +</th>\n",
       "      <th>P.10 -</th>\n",
       "      <th>P.10 p-value</th>\n",
       "      <th>ndcg +</th>\n",
       "      <th>ndcg -</th>\n",
       "      <th>ndcg p-value</th>\n",
       "      <th>ndcg_cut.10 +</th>\n",
       "      <th>ndcg_cut.10 -</th>\n",
       "      <th>ndcg_cut.10 p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DPH</td>\n",
       "      <td>0.058707</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.151733</td>\n",
       "      <td>0.576693</td>\n",
       "      <td>32.964087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DPH &gt;&gt; Trained-BERT</td>\n",
       "      <td>0.066863</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.159980</td>\n",
       "      <td>0.683735</td>\n",
       "      <td>8362.906671</td>\n",
       "      <td>36.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>31.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>35.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.004615</td>\n",
       "      <td>32.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.007476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name       map   P.10      ndcg  ndcg_cut.10          mrt  \\\n",
       "0                  DPH  0.058707  0.622  0.151733     0.576693    32.964087   \n",
       "1  DPH >> Trained-BERT  0.066863  0.758  0.159980     0.683735  8362.906671   \n",
       "\n",
       "   map +  map -  map p-value  P.10 +  P.10 -  P.10 p-value  ndcg +  ndcg -  \\\n",
       "0    NaN    NaN          NaN     NaN     NaN           NaN     NaN     NaN   \n",
       "1   36.0   14.0     0.000264    31.0    13.0      0.000171    35.0    15.0   \n",
       "\n",
       "   ndcg p-value  ndcg_cut.10 +  ndcg_cut.10 -  ndcg_cut.10 p-value  \n",
       "0           NaN            NaN            NaN                  NaN  \n",
       "1      0.004615           32.0           18.0             0.007476  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline4 = br % 100 >> pt.text.get_text(dataset, 'abstract') >> vbert\n",
    "pt.Experiment(\n",
    "    [br, pipeline4],\n",
    "    topics,\n",
    "    qrels,\n",
    "    names=['DPH', 'DPH >> Trained-BERT'],\n",
    "    baseline=0,\n",
    "    eval_metrics=[\"map\", \"ndcg\", 'ndcg_cut.10', 'P.10', 'mrt']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtAxDQHyv4ON"
   },
   "source": [
    "Training helped a lot! We're able to improve upon the initial ranking from `BatchRetrieve`. However, from looking at `mrt` we can see that this is pretty slow to run--and this was using a GPU! This performance time underscores the trade-off in using large language models at retrieval time: they may perform better, but could be much slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning at indexing time: doc2query\n",
    "\n",
    "Instead of using our large language models to rerank, another option is to use them at _indexing time_ to augment our documents. In class, we discussed one such option, doc2query, that augments an inverted index structure by predicting queries that may be used to search for the document, and appending those to the document text.\n",
    "\n",
    "We can use doc2query using the `pyterrier_doc2query` package, which was loaded at the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a pre-trained model\n",
    "\n",
    "We'll start by using a version of the doc2query model released by the authors that is trained on the MS MARCO collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-01 18:02:07--  https://git.uwaterloo.ca/jimmylin/doc2query-data/raw/master/T5-passage/t5-base.zip\n",
      "Resolving proxy.arc-ts.umich.edu (proxy.arc-ts.umich.edu)... 141.213.136.200\n",
      "Connecting to proxy.arc-ts.umich.edu (proxy.arc-ts.umich.edu)|141.213.136.200|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 357139559 (341M) [application/octet-stream]\n",
      "Saving to: ‘t5-base.zip’\n",
      "\n",
      "t5-base.zip         100%[===================>] 340.59M  90.6MB/s    in 3.9s    \n",
      "\n",
      "2022-11-01 18:02:13 (87.8 MB/s) - ‘t5-base.zip’ saved [357139559/357139559]\n",
      "\n",
      "Archive:  t5-base.zip\n",
      "  inflating: model.ckpt-1004000.data-00000-of-00002  \n",
      "  inflating: model.ckpt-1004000.data-00001-of-00002  \n",
      "  inflating: model.ckpt-1004000.index  \n",
      "  inflating: model.ckpt-1004000.meta  \n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"t5-base.zip\"):\n",
    "  !wget https://git.uwaterloo.ca/jimmylin/doc2query-data/raw/master/T5-passage/t5-base.zip\n",
    "  !unzip t5-base.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the model weights by specifying the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
      "     |█▊                              | 30.6 MB 10.3 MB/s eta 0:00:54\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |████▋                           | 82.7 MB 166.6 MB/s eta 0:00:03\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |███████▋                        | 138.1 MB 109.3 MB/s eta 0:00:05\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |██████████▊                     | 194.2 MB 367 kB/s eta 0:17:2404\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |█████████████▉                  | 250.5 MB 379 kB/s eta 0:14:24\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |█████████████████               | 307.7 MB 168.2 MB/s eta 0:00:02\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |██████████████████████▏         | 400.8 MB 420 kB/s eta 0:07:02     |████████████████████▉           | 375.8 MB 408 kB/s eta 0:08:16     |█████████████████████▎          | 384.0 MB 408 kB/s eta 0:07:56     |█████████████████████▊          | 392.2 MB 420 kB/s eta 0:07:23\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |███████████████████████████     | 487.9 MB 445 kB/s eta 0:03:23\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |███████████████████████████████▋| 571.4 MB 447 kB/s eta 0:00:16     |████████████████████████████████| 578.1 MB 32 kB/s \n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.50.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "     |██████████████████████          | 3.2 MB 96.4 MB/s eta 0:00:01\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |████████████████████████████████| 5.9 MB 106.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.14.0-py2.py3-none-any.whl (175 kB)\n",
      "     |████████████████████████████████| 175 kB 113.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     |████████████████████████████████| 781 kB 122.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     |████████████████████████████████| 4.9 MB 119.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.0.2)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "     |████████████████████████████████| 93 kB 432 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.26.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     |████████████████████████████████| 155 kB 119.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (3.6.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     |████████████████████████████████| 77 kB 3.3 MB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.2)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     |████████████████████████████████| 151 kB 121.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.3.0 astunparse-1.6.3 cachetools-5.2.0 flatbuffers-22.10.26 gast-0.4.0 google-auth-2.14.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.50.0 keras-2.10.0 keras-preprocessing-1.1.2 libclang-14.0.6 markdown-3.4.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.27.0 termcolor-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-01 18:05:28.925233: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-01 18:05:29.949690: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/lib64:\n",
      "2022-11-01 18:05:29.949711: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-01 18:05:30.033203: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-01 18:05:32.085073: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/lib64:\n",
      "2022-11-01 18:05:32.085671: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/slurm/lib64:\n",
      "2022-11-01 18:05:32.085680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2query using cpu\n"
     ]
    }
   ],
   "source": [
    "doc2query = pyterrier_doc2query.Doc2Query('model.ckpt-1004000', batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running doc2queries on sample text\n",
    "\n",
    "Let's see what queries it predicts for the sample document that we've made up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The University of Michigan School of Information (UMSI) delivers innovative, elegant and ethical solutions connecting people, information and technology. The school was one of the first iSchools in the nation and is the premier institution studying and using technology to improve human computer interactions.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([{\"docno\" : \"d1\", \"text\" :\"The University of Michigan School of Information (UMSI) delivers innovative, elegant and ethical solutions connecting people, information and technology. The school was one of the first iSchools in the nation and is the premier institution studying and using technology to improve human computer interactions.\"}])\n",
    "df.iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is umsi what is the name of university of michigan school of information what is umsi'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2query_df = doc2query(df)\n",
    "doc2query_df.iloc[0].querygen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad, though the questions are somewhat generic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an index of doc2query documents\n",
    "\n",
    "Let's see how doc2query does on improving the performance in the TREC COVID data. Since indexing with doc2query takes a while (due to needing to run the deep learning models), we've provide an index with the text already added.\n",
    "\n",
    "If you would like to index the collection with doc2query yourself (or use doc2query for your course project), you can use the following code:\n",
    "\n",
    "```python\n",
    "dataset = pt.get_dataset(\"irds:cord19/trec-covid\")\n",
    "indexer = (\n",
    "  pyterrier_doc2query.Doc2Query('model.ckpt-1004000', doc_attr='abstract', batch_size=8, append=True) # aply doc2query on abstracts and append\n",
    "  >> pt.apply.generic(lambda df: df.rename(columns={'abstract': 'text'}) # rename \"abstract\" column to \"text\" for indexing\n",
    "  >> pt.IterDictIndexer(\"./doc2query_index_path\")) # index the expanded documents\n",
    "indexref = indexer.index(dataset.get_corpus_iter())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-01 18:05:47--  http://www.dcs.gla.ac.uk/~craigm/ecir2021-tutorial/doc2query_marco_cord19.zip\n",
      "Resolving proxy.arc-ts.umich.edu (proxy.arc-ts.umich.edu)... 141.213.136.200\n",
      "Connecting to proxy.arc-ts.umich.edu (proxy.arc-ts.umich.edu)|141.213.136.200|:3128... connected.\n",
      "Proxy request sent, awaiting response... 302 Found\n",
      "Location: https://www.dcs.gla.ac.uk/~craigm/ecir2021-tutorial/doc2query_marco_cord19.zip [following]\n",
      "--2022-11-01 18:05:48--  https://www.dcs.gla.ac.uk/~craigm/ecir2021-tutorial/doc2query_marco_cord19.zip\n",
      "Connecting to proxy.arc-ts.umich.edu (proxy.arc-ts.umich.edu)|141.213.136.200|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 45804576 (44M) [application/zip]\n",
      "Saving to: ‘doc2query_marco_cord19.zip’\n",
      "\n",
      "doc2query_marco_cor 100%[===================>]  43.68M  18.4MB/s    in 2.4s    \n",
      "\n",
      "2022-11-01 18:05:51 (18.4 MB/s) - ‘doc2query_marco_cord19.zip’ saved [45804576/45804576]\n",
      "\n",
      "Archive:  doc2query_marco_cord19.zip\n",
      "   creating: doc2query_index_path/\n",
      "  inflating: doc2query_index_path/data.document.fsarrayfile  \n",
      "  inflating: doc2query_index_path/data.inverted.bf  \n",
      "  inflating: doc2query_index_path/data.direct.bf  \n",
      "  inflating: doc2query_index_path/data.lexicon.fsomapid  \n",
      "  inflating: doc2query_index_path/data.lexicon.fsomaphash  \n",
      "  inflating: doc2query_index_path/data.lexicon.fsomapfile  \n",
      "  inflating: doc2query_index_path/data.meta.zdata  \n",
      "  inflating: doc2query_index_path/data.properties  \n",
      "  inflating: doc2query_index_path/data.meta.idx  \n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('doc2query_marco_cord19.zip'):\n",
    "  !wget http://www.dcs.gla.ac.uk/~craigm/ecir2021-tutorial/doc2query_marco_cord19.zip\n",
    "  !unzip doc2query_marco_cord19.zip\n",
    "doc2query_indexref = pt.IndexRef.of('./doc2query_index_path/data.properties')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the results on TREC COVID by first merging the scores with the rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maojy/.local/lib/python3.9/site-packages/pyterrier/datasets.py:435: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df.drop(df.columns.difference(['qid','query']), 1, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>query</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>iteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>101299</td>\n",
       "      <td>jwmrgy5d</td>\n",
       "      <td>0</td>\n",
       "      <td>8.427298</td>\n",
       "      <td>coronavirus origin</td>\n",
       "      <td>COVID-19 in the heart and the lungs: could we ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>182167</td>\n",
       "      <td>g8grcy5j</td>\n",
       "      <td>0</td>\n",
       "      <td>13.922648</td>\n",
       "      <td>coronavirus response to weather changes</td>\n",
       "      <td>The Stirling Protocol – Putting the environmen...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>85678</td>\n",
       "      <td>tl30wlpy</td>\n",
       "      <td>0</td>\n",
       "      <td>7.224180</td>\n",
       "      <td>coronavirus immunity</td>\n",
       "      <td>Receptor-dependent coronavirus infection of de...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>145871</td>\n",
       "      <td>l5fxswfz</td>\n",
       "      <td>0</td>\n",
       "      <td>12.773362</td>\n",
       "      <td>how do people die from the coronavirus</td>\n",
       "      <td>Analysis on 54 Mortality Cases of Coronavirus ...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>180990</td>\n",
       "      <td>3sepefqa</td>\n",
       "      <td>0</td>\n",
       "      <td>12.995980</td>\n",
       "      <td>animal models of covid 19</td>\n",
       "      <td>Current global vaccine and drug efforts agains...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid   docid     docno  rank      score  \\\n",
       "0   1  101299  jwmrgy5d     0   8.427298   \n",
       "1   2  182167  g8grcy5j     0  13.922648   \n",
       "2   3   85678  tl30wlpy     0   7.224180   \n",
       "3   4  145871  l5fxswfz     0  12.773362   \n",
       "4   5  180990  3sepefqa     0  12.995980   \n",
       "\n",
       "                                     query  \\\n",
       "0                       coronavirus origin   \n",
       "1  coronavirus response to weather changes   \n",
       "2                     coronavirus immunity   \n",
       "3   how do people die from the coronavirus   \n",
       "4                animal models of covid 19   \n",
       "\n",
       "                                               title  label iteration  \n",
       "0  COVID-19 in the heart and the lungs: could we ...    0.0         5  \n",
       "1  The Stirling Protocol – Putting the environmen...    0.0         4  \n",
       "2  Receptor-dependent coronavirus infection of de...    NaN       NaN  \n",
       "3  Analysis on 54 Mortality Cases of Coronavirus ...    2.0       1.5  \n",
       "4  Current global vaccine and drug efforts agains...    0.0         4  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pt.get_dataset('irds:cord19/trec-covid')\n",
    "pipeline = pt.BatchRetrieve(doc2query_indexref) % 1 >> pt.text.get_text(dataset, 'title')\n",
    "res = pipeline(dataset.get_topics('title'))\n",
    "res.merge(dataset.get_qrels(), how='left').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of queries does doc2query generate for the CORD19 documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591a9747ff4a4a0d821eb9e731791913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cord19/trec-covid documents:   0%|          | 0/192509 [15ms<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l5fxswfz\n",
      "number of cases of coronavirus how many people are alive from the coronavirus coronavirus deaths\n",
      "Since the identification of the first case of coronavirus disease 2019 (COVID-19), the global number of confirmed cases as of March 15, 2020, is 156,400, with total death in 5,833 (3.7%) worldwide. Here, we summarize the morality data from February 19 when the first mortality occurred to 0 am, March 10, 2020, in Korea with comparison to other countries. The overall case fatality rate of COVID-19 in Korea was 0.7% as of 0 am, March 10, 2020.\n",
      "3sepefqa\n",
      "what is copid medicine covid 19 causes and treatment covid19 is a disease that causes symptoms and effects\n",
      "COVID-19 has become one of the biggest health concern, along with huge economic burden. With no clear remedies to treat the disease, doctors are repurposing drugs like chloroquine and remdesivir to treat COVID-19 patients. In parallel, research institutes in collaboration with biotech companies have identified strategies to use viral proteins as vaccine candidates for COVID-19. Although this looks promising, they still need to pass the test of challenge studies in animal models. As various models for SARS-CoV-2 are under testing phase, biotech companies have bypassed animal studies and moved to Phase I clinical trials. In view of the present outbreak, this looks a justified approach, but the problem is that in the absence of animal studies, we can never predict the outcomes in humans. Since animal models are critical for vaccine development and SARS-CoV-2 has different transmission dynamics, in this review we compare different animal models of SARS-CoV-2 with humans for their pathogenic, immune response and transmission dynamics that make them ideal models for vaccine testing for COVID-19. Another issue of using animal model is the ethics of using animals for research; thus, we also discuss the pros and cons of using animals for vaccine development studies.\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(doc for doc in dataset.get_corpus_iter() if doc['docno'] in ('3sepefqa', 'l5fxswfz'))\n",
    "df = df.rename(columns={'abstract': 'text'})\n",
    "doc2query_df = doc2query(df)\n",
    "for querygen, docno, text in zip(doc2query_df['querygen'], doc2query_df['docno'], df['text']):\n",
    "    print(docno)\n",
    "    print(querygen)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the effects of doc2query\n",
    "\n",
    "Here, we'll change our evaluation setup a bit from what we did before. Rather than compare two models for the same index, we'll instead compare the same model (BM25) with two different ways of indexing (two indices)! Our baseline will be an index of CORD19 without the doc2query additions.\n",
    "\n",
    "Let's load a copy of the CORD19 index that we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexref = pt.IndexRef.of('./terrier_cord19/data.properties')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Write the `Experiment` to compare indices (5 points)\n",
    "Run an `Experiment` using a `BM25` ranker that compares the indices `indexref` and `doc2query_indexref`.  Note that our doc2query model was trained on MS MARCO, which isn't the same kind of collection as CORD19, so this performance tells us how well that model can transfer to a new setting.\n",
    "\n",
    "You should evaluate using the metrics \"map\", \"ndcg\", \"ndcg_cut.10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "JavaException",
     "evalue": "JVM exception occurred: No IndexLoaders were supported for indexref ./terrier_cord19/data.properties; It may be your ref has the wrong location. Alternatively, Terrier is misconfigured - did you import the correct package to deal with this indexref? java.lang.UnsupportedOperationException",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJavaException\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_427917/2039189901.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexFactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mindex2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexFactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc2query_indexref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchRetrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BM25\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdoc2q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchRetrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BM25\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m pt.Experiment(\n",
      "\u001b[0;32mjnius/jnius_export_class.pxi\u001b[0m in \u001b[0;36mjnius.JavaMultipleMethod.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mjnius/jnius_export_class.pxi\u001b[0m in \u001b[0;36mjnius.JavaMethod.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mjnius/jnius_export_class.pxi\u001b[0m in \u001b[0;36mjnius.JavaMethod.call_staticmethod\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mjnius/jnius_jvm_dlopen.pxi\u001b[0m in \u001b[0;36mjnius.create_jnienv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mJavaException\u001b[0m: JVM exception occurred: No IndexLoaders were supported for indexref ./terrier_cord19/data.properties; It may be your ref has the wrong location. Alternatively, Terrier is misconfigured - did you import the correct package to deal with this indexref? java.lang.UnsupportedOperationException"
     ]
    }
   ],
   "source": [
    "index1 = pt.IndexFactory.of(indexref)\n",
    "index2 = pt.IndexFactory.of(doc2query_indexref)\n",
    "idx = pt.BatchRetrieve(index1, wmodel=\"BM25\")\n",
    "doc2q = pt.BatchRetrieve(index2, wmodel=\"BM25\")\n",
    "pt.Experiment(\n",
    "    [doc2q, idx],\n",
    "    topics,\n",
    "    qrels,\n",
    "    eval_metrics=[\"map\", \"ndcg\", 'ndcg_cut.10']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Train a new model! (30 points)\n",
    "\n",
    "All of the prior exercises have had you working with either off-the-shelf models (not trained for IR) or models that someone else has trained for you. To give you a sense of how to train a model, your primary task in this notebook is to train a simple `knrm` model, which should be relatively efficient to train on a GPU. \n",
    "\n",
    "To keep thinsg simple, we'll use the same setup for CORD19 that we did in Part 2 (30 queries in train, 5 in dev, 15 queries in test) which is still relatively small for training a deep learning model but will get you started on the process. \n",
    "\n",
    "Your tasks are the following:\n",
    "- Load the CORD19 dataset and split it into train, dev, and test. Your test set should have 15 queries, dev set 5 queries, and the rest in train. use a seed of 42 for the `random_state` to ensure consistent results with what we expected.\n",
    "- Create a new `knrm` ranker and a pipeline that uses it\n",
    "- Run an `Experiment` comparing four models:\n",
    "  - a default `BatchRetrieve`, filtering to the top 100 results\n",
    "  - BM25, filtering to the top 100 results\n",
    "  - a pipeline that feeds the top 100 results of the default `BatchRetrieve` to your `knrm` model\n",
    "  - a pipeline that feeds the top 100 results of BM25 to your `knrm` model\n",
    "  \n",
    "Your `Experiment` should evaluate with metrics `\"map\", \"ndcg\", 'ndcg_cut.10', 'P.10', 'mrt'`\n",
    "  \n",
    "We expect to see the `Experiment`'s results in the final cell. You are, of course, welcome to try training any of the fancier models to see how they do as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANK_CUTOFF = 10\n",
    "SEED=42\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tr_va_topics, test_topics = train_test_split(topics, test_size=15, random_state=SEED)\n",
    "train_topics, valid_topics =  train_test_split(tr_va_topics, test_size=5, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "names should be the same length as retr_systems",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_427917/4265435756.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpipeline1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbr\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'abstract'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mknrm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpipeline2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbm25\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'abstract'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mknrm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m pt.Experiment(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mbr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbm25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpipeline2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pyterrier/pipelines.py\u001b[0m in \u001b[0;36mExperiment\u001b[0;34m(retr_systems, topics, qrels, eval_metrics, names, perquery, dataframe, batch_size, filter_by_qrels, filter_by_topics, baseline, test, correction, correction_alpha, highlight, round, verbose, save_dir, save_mode, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msystem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mretr_systems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretr_systems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"names should be the same length as retr_systems\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;31m# validate save_dir and resulting filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: names should be the same length as retr_systems"
     ]
    }
   ],
   "source": [
    "knrm = onir_pt.reranker('knrm', 'bert', text_field='abstract', vocab_config={'train': True})\n",
    "br = pt.BatchRetrieve(index) % 100\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")% 100\n",
    "pipeline1 = br >> pt.text.get_text(dataset, 'abstract') >> knrm\n",
    "pipeline2 = bm25 >> pt.text.get_text(dataset, 'abstract') >> knrm\n",
    "pt.Experiment(\n",
    "    [br,bm25, pipeline1,pipeline2],\n",
    "    topics,\n",
    "    qrels,\n",
    "    names=['DPH', 'DPH >> KNRM'],\n",
    "    eval_metrics=[\"map\", \"ndcg\", 'ndcg_cut.10', 'P.10', 'mrt']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "neural ir.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

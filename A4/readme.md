This homework provides a substantial amount of code to get you started and has you focus on just a few implementation pieces.

* Part 1 of the homework will be training the bi-encoder model. You will need a GPU for this, so you'll want to run this on a Great Lakes machine with a GPU. We've done testing and set the hyperparameters below so that it should run quickly enough. _But_ it will still take an hour to complete training once you get it up and running. Once you have the model trained, you'll export the query-document relevance scores from the bi-encoder to a file for use later.

* Part 2 will have you learning how to do Learning to Rank (L2R) again and incorporating the bi-encoder's predictions. For this part, you will need to have access to Java to run Pyterrier. However, you will not need a GPU and this part can be run either on a non-GPU Great Lakes machine or on your local machine. You will still need access to the outputs of the bi-encoder which are in a file, so if you work locally, you'll need to copy those still. Part 2 will show you how to set up new pipelines that make use of precomputed features.

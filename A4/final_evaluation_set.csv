,Language,Query,GitHubUrl,code,relevance
54,Python,write csv,https://github.com/sentinel-hub/sentinelhub-py/blob/08a83b7f1e289187159a643336995d8369860fea/sentinelhub/io_utils.py#L258-L271,"def write_csv(filename, data, delimiter=CSV_DELIMITER): """""" Write image data to CSV file :param filename: name of CSV file to write data to :type filename: str :param data: image data to write to CSV file :type data: numpy array :param delimiter: delimiter used in CSV file. Default is ``;`` :type delimiter: str """""" with open(filename, 'w') as file: csv_writer = csv.writer(file, delimiter=delimiter) for line in data: csv_writer.writerow(line)",3
176,Python,write csv,https://github.com/jordanjoz1/flickr-views-counter/blob/cba89285823ab39afd9f40a19db82371d45bd830/count_views.py#L130-L138,"def write_to_csv(fname, header, rows): with open(fname, 'wb') as csvfile: csvwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL) csvwriter.writerow(header) for row in rows: csvwriter.writerow( [s.encode(""utf-8"").replace(',', '').replace('\n', '') for s in row])",3
357,Python,write csv,https://github.com/fastai/fastai/blob/9fb84a5cdefe5a766cdb792b8f5d8971737b7e67/fastai/widgets/image_cleaner.py#L209-L218,"def write_csv(self): # Get first element's file path so we write CSV to same directory as our data csv_path = self._path/'cleaned.csv' with open(csv_path, 'w') as f: csv_writer = csv.writer(f) csv_writer.writerow(['name','label']) for pair in self._csv_dict.items(): pair = [os.path.relpath(pair[0], self._path), pair[1]] csv_writer.writerow(pair) return csv_path",3
615,Python,write csv,https://github.com/majerteam/sylk_parser/blob/0f62f9d3ca1c273017d7ea728f9db809f2241389/sylk_parser/sylk_parser.py#L27-L40,"def to_csv(self, fbuf, quotechar='""', delimiter=','): csvwriter = csv.writer( fbuf, quotechar=quotechar, delimiter=delimiter, lineterminator=""\n"", quoting=csv.QUOTE_ALL ) if self.headers: csvwriter.writerow(self.headers) for line in self.sylk_handler.stream_rows(): csvwriter.writerow(line)",3
722,Python,write csv,https://github.com/gem/oq-engine/blob/8294553a0b8aba33fd96437a35065d03547d0040/openquake/hmtk/seismicity/smoothing/smoothed_seismicity.py#L491-L518,"def write_to_csv(self, filename): ''' Exports to simple csv :param str filename: Path to file for export ''' fid = open(filename, 'wt') # Create header list header_info = ['Longitude', 'Latitude', 'Depth', 'Observed Count', 'Smoothed Rate', 'b-value'] writer = csv.DictWriter(fid, fieldnames=header_info) headers = dict((name0, name0) for name0 in header_info) # Write to file writer.writerow(headers) for row in self.data: # institute crude compression by omitting points with no seismicity # and taking advantage of the %g format if row[4] == 0: continue row_dict = {'Longitude': '%g' % row[0], 'Latitude': '%g' % row[1], 'Depth': '%g' % row[2], 'Observed Count': '%d' % row[3], 'Smoothed Rate': '%.6g' % row[4], 'b-value': '%g' % self.bval} writer.writerow(row_dict) fid.close()",3
1271,Python,write csv,https://github.com/probcomp/crosscat/blob/4a05bddb06a45f3b7b3e05e095720f16257d1535/src/utils/data_utils.py#L306-L311,"def write_csv(filename, T, header = None): with open(filename,'w') as fh: csv_writer = csv.writer(fh, delimiter=',') if header != None: csv_writer.writerow(header) [csv_writer.writerow(T[i]) for i in range(len(T))]",3
1883,Python,write csv,https://github.com/rvswift/EB/blob/341880b79faf8147dc9fa6e90438531cd09fabcc/EB/builder/splitter/splitter.py#L37-L57,"def write_csv_header(mol, csv_writer): """""" Write the csv header """""" # create line list where line elements for writing will be stored line = [] # ID line.append('id') # status line.append('status') # query labels queryList = mol.properties.keys() for queryLabel in queryList: line.append(queryLabel) # write line csv_writer.writerow(line)",3
1946,Python,write csv,https://github.com/quantopian/zipline/blob/77ad15e6dc4c1cbcdc133653bac8a63fc704f7fe/zipline/data/bcolz_daily_bars.py#L209-L237,"def write_csvs(self, asset_map, show_progress=False, invalid_data_behavior='warn'): """"""Read CSVs as DataFrames from our asset map. Parameters ---------- asset_map : dict[int -> str] A mapping from asset id to file path with the CSV data for that asset show_progress : bool Whether or not to show a progress bar while writing. invalid_data_behavior : {'warn', 'raise', 'ignore'} What to do when data is encountered that is outside the range of a uint32. """""" read = partial( read_csv, parse_dates=['day'], index_col='day', dtype=self._csv_dtypes, ) return self.write( ((asset, read(path)) for asset, path in iteritems(asset_map)), assets=viewkeys(asset_map), show_progress=show_progress, invalid_data_behavior=invalid_data_behavior, )",3
187,Python,write csv,https://github.com/googledatalab/pydatalab/blob/d9031901d5bca22fe0d5925d204e6698df9852e1/solutionbox/structured_data/mltoolbox/_structured_data/prediction/predict.py#L307-L346,"def expand(self, datasets): import json tf_graph_predictions, errors = datasets if self._output_format == 'json': (tf_graph_predictions | 'Write Raw JSON' >> beam.io.textio.WriteToText(os.path.join(self._output_dir, 'predictions'), file_name_suffix='.json', coder=RawJsonCoder(), shard_name_template=self._shard_name_template)) elif self._output_format == 'csv': # make a csv header file header = [col['name'] for col in self._schema] csv_coder = CSVCoder(header) (tf_graph_predictions.pipeline | 'Make CSV Header' >> beam.Create([json.dumps(self._schema, indent=2)]) | 'Write CSV Schema File' >> beam.io.textio.WriteToText(os.path.join(self._output_dir, 'csv_schema'), file_name_suffix='.json', shard_name_template='')) # Write the csv predictions (tf_graph_predictions | 'Write CSV' >> beam.io.textio.WriteToText(os.path.join(self._output_dir, 'predictions'), file_name_suffix='.csv', coder=csv_coder, shard_name_template=self._shard_name_template)) else: raise ValueError('FormatAndSave: unknown format %s', self._output_format) # Write the errors to a text file. (errors | 'Write Errors' >> beam.io.textio.WriteToText(os.path.join(self._output_dir, 'errors'), file_name_suffix='.txt', shard_name_template=self._shard_name_template))",2
646,Python,write csv,https://github.com/rvswift/EB/blob/341880b79faf8147dc9fa6e90438531cd09fabcc/EB/builder/splitter/splitter.py#L90-L113,"def csv_writer(molecules, options, prefix): """""" Write a csv file. """""" # output file outdir = os.getcwd() filename = prefix + '.csv' outfile = os.path.join(outdir, filename) # initiate csv writer object f = open(outfile, 'w') csv_writer = csv.writer(f) # write csv header mol = molecules[0] write_csv_header(mol, csv_writer) # write csv lines for mol in molecules: write_csv_line(mol, csv_writer, options) # close file f.close()",2
94,Python,unzipping large files,https://github.com/patarapolw/AnkiTools/blob/fab6836dfd9cf5171d9cbff5c55fbb14d2786f05/AnkiTools/editor.py#L53-L59,"def unzip(self, in_file, out_file): with ZipFile(in_file) as zf: zf.extract('collection.anki2', path=self.tempdir) shutil.move(os.path.join(self.tempdir, 'collection.anki2'), out_file) return out_file",3
167,Python,unzipping large files,https://github.com/milinda/htrc-sdk-for-python/blob/d08dbba1a441dcb2bade47deaebc07be68c3a173/htrc/utils.py#L45-L62,"def unzip(zip_content, dest_dir): # From http://stackoverflow.com/a/12886818 with zipfile.ZipFile(zip_content, ""r"") as zf: for member in zf.infolist(): # Path traversal defense copied from # http://hg.python.org/cpython/file/tip/Lib/http/server.py#l789 words = member.filename.split('/') path = dest_dir for word in words[:-1]: drive, word = os.path.splitdrive(word) head, word = os.path.split(word) if word in (os.curdir, os.pardir, ''): continue path = os.path.join(path, word) zf.extract(member, path)",3
348,Python,unzipping large files,https://github.com/JIC-CSB/jicbioimage.transform/blob/494c282d964c3a9b54c2a1b3730f5625ea2a494b/appveyor/install.py#L19-L23,"def unzip_file(zip_fname): """"""Unzip the zip_fname in the current directory."""""" print(""Unzipping {}"".format(zip_fname)) with zipfile.ZipFile(zip_fname) as zf: zf.extractall()",3
349,Python,unzipping large files,https://github.com/uuazed/numerapi/blob/fc9dcc53b32ede95bfda1ceeb62aec1d67d26697/numerapi/numerapi.py#L76-L88,"def _unzip_file(self, src_path, dest_path, filename): """"""unzips file located at src_path into destination_path"""""" self.logger.info(""unzipping file..."") # construct full path (including file name) for unzipping unzip_path = os.path.join(dest_path, filename) utils.ensure_directory_exists(unzip_path) # extract data with zipfile.ZipFile(src_path, ""r"") as z: z.extractall(unzip_path) return True",3
81,Python,unzipping large files,https://github.com/OLC-Bioinformatics/sipprverse/blob/d4f10cdf8e1a39dac0953db61c21c97efc6006de/cgecore/utility.py#L471-L484,"def file_unzipper(directory): """""" This function will unzip all files in the runroot directory and subdirectories """""" debug.log(""Unzipping directory (%s)...""%directory) #FINDING AND UNZIPPING ZIPPED FILES for root, dirs, files in os.walk(directory, topdown=False): if root != """": orig_dir = os.getcwd() os.chdir(directory) Popen('gunzip -q -f *.gz > /dev/null 2>&1', shell=True).wait() Popen('unzip -qq -o ""*.zip"" > /dev/null 2>&1', shell=True).wait() Popen('rm -f *.zip > /dev/null 2>&1', shell=True).wait() os.chdir(orig_dir)",2
637,Python,unzipping large files,https://github.com/nvbn/thefuck/blob/40ab4eb62db57627bff10cf029d29c94704086a2/thefuck/rules/dirty_unzip.py#L15-L25,def _zip_file(command): # unzip works that way: # unzip [-flags] file[.zip] [file(s) ...] [-x file(s) ...] # ^ ^ files to unzip from the archive # archive to unzip for c in command.script_parts[1:]: if not c.startswith('-'): if c.endswith('.zip'): return c else: return u'{}.zip'.format(c),2
787,Python,unzipping large files,https://github.com/nerdvegas/rez/blob/1d3b846d53b5b5404edfe8ddb9083f9ceec8c5e7/src/rez/backport/zipfile.py#L808-L819,"def testzip(self): """"""Read all the files and check the CRC."""""" chunk_size = 2 ** 20 for zinfo in self.filelist: try: # Read by chunks, to avoid an OverflowError or a # MemoryError with very large embedded files. f = self.open(zinfo.filename, ""r"") while f.read(chunk_size): # Check CRC-32 pass except BadZipfile: return zinfo.filename",2
848,Python,unzipping large files,https://github.com/mjirik/io3d/blob/ccaf3e378dcc967f2565d477fc27583fd0f61fcc/io3d/datasets.py#L821-L833,"def unzip_recursive(zip_file_name): """"""Unzip file with all recursive zip files inside and delete zip files after that. :param zip_file_name: file name of zip file :return: list of archive members by name. """""" logger.debug(""unzipping "" + zip_file_name) fnlist = unzip_one(zip_file_name) for fn in fnlist: if zipfile.is_zipfile(fn): local_fnlist = unzip_recursive(fn) fnlist.extend(local_fnlist) return fnlist",2
989,Python,unzipping large files,https://github.com/facelessuser/backrefs/blob/3b3d60f5d57b02044f880aa29c9c5add0e31a34f/tools/unidatadownload.py#L32-L44,"def unzip_unicode(output, version): """"""Unzip the Unicode files."""""" unzipper = zipfile.ZipFile(os.path.join(output, 'unicodedata', '%s.zip' % version)) target = os.path.join(output, 'unicodedata', version) print('Unzipping %s.zip...' % version) os.makedirs(target) for f in unzipper.namelist(): # Do I need backslash on windows? Or is it forward as well? unzipper.extract(f, target)",2
1443,Python,unzipping large files,https://github.com/cs50/lib50/blob/941767f6c0a3b81af0cdea48c25c8d5a761086eb/lib50/_api.py#L570-L611,"def _lfs_add(files, git): """""" Add any oversized files with lfs. Throws error if a file is bigger than 2GB or git-lfs is not installed. """""" # Check for large files > 100 MB (and huge files > 2 GB) # https://help.github.com/articles/conditions-for-large-files/ # https://help.github.com/articles/about-git-large-file-storage/ larges, huges = [], [] for file in files: size = os.path.getsize(file) if size > (100 * 1024 * 1024): larges.append(file) elif size > (2 * 1024 * 1024 * 1024): huges.append(file) # Raise Error if a file is >2GB if huges: raise Error(_(""These files are too large to be submitted:\n{}\n"" ""Remove these files from your directory "" ""and then re-run {}!"").format(""\n"".join(huges), org)) # Add large files (>100MB) with git-lfs if larges: # Raise Error if git-lfs not installed if not shutil.which(""git-lfs""): raise Error(_(""These files are too large to be submitted:\n{}\n"" ""Install git-lfs (or remove these files from your directory) "" ""and then re-run!"").format(""\n"".join(larges))) # Install git-lfs for this repo _run(git(""lfs install --local"")) # For pre-push hook _run(git(""config credential.helper cache"")) # Rm previously added file, have lfs track file, add file again for large in larges: _run(git(""rm --cached {}"".format(shlex.quote(large)))) _run(git(""lfs track {}"".format(shlex.quote(large)))) _run(git(""add {}"".format(shlex.quote(large)))) _run(git(""add --force .gitattributes""))",1
228,Python,unique elements,https://github.com/shoebot/shoebot/blob/d554c1765c1899fa25727c9fc6805d221585562b/lib/graph/cluster.py#L16-L20,"def unique(list): """""" Returns a copy of the list without duplicates. """""" unique = []; [unique.append(x) for x in list if x not in unique] return unique",3
494,Python,unique elements,https://github.com/matthew-brett/delocate/blob/ed48de15fce31c3f52f1a9f32cae1b02fc55aa60/delocate/tools.py#L72-L89,"def unique_by_index(sequence): """""" unique elements in `sequence` in the order in which they occur Parameters ---------- sequence : iterable Returns ------- uniques : list unique elements of sequence, ordered by the order in which the element occurs in `sequence` """""" uniques = [] for element in sequence: if element not in uniques: uniques.append(element) return uniques",3
931,Python,unique elements,https://github.com/wdbm/shijian/blob/ad6aea877e1eb99fe148127ea185f39f1413ed4f/shijian.py#L990-L995,def unique_list_elements(x): unique_elements = [] for element in x: if element not in unique_elements: unique_elements.append(element) return unique_elements,3
1413,Python,unique elements,https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/IPython/utils/data.py#L22-L42,"def uniq_stable(elems): """"""uniq_stable(elems) -> list Return from an iterable, a list of all the unique elements in the input, but maintaining the order in which they first appear. A naive solution to this problem which just makes a dictionary with the elements as keys fails to respect the stability condition, since dictionaries are unsorted by nature. Note: All elements in the input must be valid dictionary keys for this routine to work, as it internally uses a dictionary for efficiency reasons."""""" unique = [] unique_dict = {} for nn in elems: if nn not in unique_dict: unique.append(nn) unique_dict[nn] = None return unique",3
605,Python,unique elements,https://github.com/radujica/baloo/blob/f6e05e35b73a75e8a300754c6bdc575e5f2d53b9/baloo/weld/weld_ops.py#L529-L569,"def weld_unique(array, weld_type): """"""Return the unique elements of the array. Parameters ---------- array : numpy.ndarray or WeldObject Input array. weld_type : WeldType Type of each element in the input array. Returns ------- WeldObject Representation of this computation. """""" obj_id, weld_obj = create_weld_object(array) weld_template = """"""map( tovec( result( for( map( {array}, |e| {{e, 0si}} ), dictmerger[{type}, i16, +], |b: dictmerger[{type}, i16, +], i: i64, e: {{{type}, i16}}| merge(b, e) ) ) ), |e| e.$0 )"""""" weld_obj.weld_code = weld_template.format(array=obj_id, type=weld_type) return weld_obj",2
1586,Python,unique elements,https://github.com/iotile/coretools/blob/2d794f5f1346b841b0dcd16c9d284e9bf2f3c6ec/iotilebuild/iotile/build/config/scons-local-3.0.1/SCons/Util.py#L1300-L1303,"def __make_unique(self): if not self.unique: self.data = uniquer_hashables(self.data) self.unique = True def __lt__(self, other):",2
1838,Python,unique elements,https://github.com/mbj4668/pyang/blob/f2a5cc3142162e5b9ee4e18d154568d939ff63dd/pyang/plugins/check_update.py#L584-L598,"def chk_unique(old, new, ctx): # do not check the unique argument string; check the parsed unique instead # i_unique is not set in groupings; ignore if not hasattr(old, 'i_unique') or not hasattr(new, 'i_unique'): return oldunique = [] for (u, l) in old.i_unique: oldunique.append((u, [s.arg for s in l])) for (u, l) in new.i_unique: # check if this unique was present before o = util.keysearch([s.arg for s in l], 1, oldunique) if o is not None: oldunique.remove(o) else: err_def_added(u, ctx)",2
651,Python,unique elements,https://github.com/joke2k/faker/blob/965824b61132e52d92d1a6ce470396dbbe01c96c/faker/providers/__init__.py#L180-L205,"def random_elements(self, elements=('a', 'b', 'c'), length=None, unique=False): fn = choices_distribution_unique if unique else choices_distribution if length is None: length = self.generator.random.randint(1, len(elements)) if unique and length > len(elements): raise ValueError( ""Sample length cannot be longer than the number of unique elements to pick from."") if isinstance(elements, dict): choices = elements.keys() probabilities = elements.values() else: if unique: # shortcut return self.generator.random.sample(elements, length) choices = elements probabilities = [1.0 for _ in range(len(choices))] return fn( list(choices), list(probabilities), self.generator.random, length=length, )",1
727,Python,unique elements,https://github.com/joke2k/faker/blob/965824b61132e52d92d1a6ce470396dbbe01c96c/faker/providers/__init__.py#L207-L223,"def random_choices(self, elements=('a', 'b', 'c'), length=None): """""" Returns a list of random, non-unique elements from a passed object. If `elements` is a dictionary, the value will be used as a weighting element. For example:: random_element({""{{variable_1}}"": 0.5, ""{{variable_2}}"": 0.2, ""{{variable_3}}"": 0.2, ""{{variable_4}}"": 0.1}) will have the following distribution: * `variable_1`: 50% probability * `variable_2`: 20% probability * `variable_3`: 20% probability * `variable_4`: 10% probability """""" return self.random_elements(elements, length, unique=False)",1
1170,Python,unique elements,https://github.com/bxlab/bx-python/blob/09cb725284803df90a468d910f2274628d8647de/scripts/bnMapper.py#L102-L117,"def union_elements(elements): """"""elements = [(chr, s, e, id), ...], this is to join elements that have a deletion in the 'to' species """""" if len(elements) < 2: return elements assert set( [e[3] for e in elements] ) == set( [elements[0][3]] ), ""more than one id"" el_id = elements[0][3] unioned_elements = [] for ch, chgrp in groupby(elements, key=itemgetter(0)): for (s, e) in elem_u( np.array([itemgetter(1, 2)(_) for _ in chgrp], dtype=np.uint) ): if (s < e): unioned_elements.append( (ch, s, e, el_id) ) assert len(unioned_elements) <= len(elements) return unioned_elements",0
1611,Python,underline text in label widget,https://github.com/dendory/menu3/blob/350414966e8f5c7737cd527369c8087f4f8f600b/menu3/menu3.py#L45-L49,"def underline(self, text): # Print an underline text if self._windows(): return text else: return self.UNDERLINE + text + self.NORMAL",3
1073,Python,underline text in label widget,https://github.com/katerina7479/pypdflite/blob/ac2501f30d6619eae9dea5644717575ca9263d0a/pypdflite/pdfobjects/pdftext.py#L165-L172,"def _underline(self): # Underline text up = self.font.underline_position ut = self.font.underline_thickness w = self.font._string_width(self.text) s = '%.2f %.2f %.2f %.2f re f' % (self.cursor.x, self.cursor.y_prime - up, w, ut) return s",2
314,Python,underline text in label widget,https://github.com/bitesofcode/projexui/blob/f18a73bec84df90b034ca69b9deea118dbedfc4d/projexui/widgets/xpushbutton.py#L72-L90,"def setShowRichText(self, state): """""" Sets whether or not to display rich text for this button. :param state | <bool> """""" self._showRichText = state text = self.text() if state: label = self.richTextLabel() label.setText(text) label.show() super(XPushButton, self).setText('') else: if self._richTextLabel: self._richTextLabel.hide() super(XPushButton, self).setText(text)",1
606,Python,underline text in label widget,https://github.com/django-leonardo/django-leonardo/blob/4b933e1792221a13b4028753d5f1d3499b0816d4/leonardo/module/web/widgets/forms.py#L55-L144,"def __init__(self, *args, **kwargs): request = kwargs.pop('request', None) model = kwargs.pop('model', None) super(WidgetForm, self).__init__(*args, **kwargs) if isinstance(model, Page): self.fields['parent'] = PageSelectField( label=_(""Parent""), help_text=_(""Parent Page"")) else: self.fields['parent'].widget = forms.widgets.HiddenInput() initial = kwargs.get('initial', None) if initial and initial.get('id', None): widget = self._meta.model.objects.get( id=initial['id']) data = widget.dimensions self.init_content_themes() elif 'instance' in kwargs: widget = kwargs['instance'] data = widget.dimensions self.init_content_themes() else: data = [] widget = None # set defaults and delete id field self.init_themes() del self.fields['id'] # get all fields for widget main_fields = self._meta.model.fields() main_fields.update({'label': 'label'}) main_fields.pop(""parent"", None) self.helper.layout = Layout( TabHolder( Tab(self._meta.model._meta.verbose_name.capitalize(), *self.get_main_fields(main_fields), css_id='field-{}'.format(slugify(self._meta.model)) ), Tab(_('Styles'), 'base_theme', 'content_theme', 'color_scheme', 'prerendered_content', Fieldset(_('Positions'), 'layout', 'align', 'vertical_align', 'parent'), *self.get_id_field(), css_id='theme-widget-settings' ), Tab(_('Effects'), 'enter_effect_style', 'enter_effect_duration', 'enter_effect_delay', 'enter_effect_offset', 'enter_effect_iteration', css_id='theme-widget-effects' ), ), HTML(render_to_string('widget/_update_preview.html', {'class_name': ""."".join([ self._meta.model._meta.app_label, self._meta.model._meta.model_name]) })) ) self.fields['label'].widget = forms.TextInput( attrs={'placeholder': self._meta.model._meta.verbose_name}) if request: _request = copy.copy(request) _request.POST = {} _request.method = 'GET' from .tables import WidgetDimensionTable dimensions = Tab(_('Dimensions'), HTML( WidgetDimensionTable(_request, widget=widget, data=data).render()), ) self.helper.layout[0].append(dimensions) # hide label if 'text' in self.fields: self.fields['text'].label = '' # finally add custom tabs self.init_custom_tabs()",1
728,Python,underline text in label widget,https://github.com/ska-sa/purr/blob/4c848768d0485d0f88b30850d0d5372221b21b66/Purr/MainWindow.py#L123-L128,"def setLabel(self, label=None): if label is None: self.label.hide() else: self.label.setText(label) self.label.show()",1
842,Python,underline text in label widget,https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/preferences/configdialog.py#L515-L534,"def create_textedit(self, text, option, default=NoDefault, tip=None, restart=False): label = QLabel(text) label.setWordWrap(True) edit = QPlainTextEdit() edit.setWordWrapMode(QTextOption.WordWrap) layout = QVBoxLayout() layout.addWidget(label) layout.addWidget(edit) layout.setContentsMargins(0, 0, 0, 0) if tip: edit.setToolTip(tip) self.textedits[edit] = (option, default) widget = QWidget(self) widget.label = label widget.textbox = edit widget.setLayout(layout) edit.restart_required = restart edit.label_text = text return widget",1
874,Python,underline text in label widget,https://github.com/nion-software/nionswift/blob/d43693eaf057b8683b9638e575000f055fede452/nion/swift/Facade.py#L594-L597,"def create_label_widget(self, text=None): label_widget = LabelWidget(self.__ui) label_widget.text = text return label_widget",1
1774,Python,underline text in label widget,https://github.com/limodou/uliweb/blob/34472f25e4bc0b954a35346672f94e84ef18b076/uliweb/form/template_layout.py#L90-L108,"def do_td_field(self, indent, value, **kwargs): field_name = kwargs.pop('name', None) field = getattr(self.form, field_name) obj = self.form.fields[field_name] if 'label' in kwargs: label = kwargs.pop('label') else: label = obj.label if label: obj.label = label label_text = obj.get_label(_class='field') else: label_text = '' display = field.data or '&nbsp;' if 'width' not in kwargs: kwargs['width'] = 200 td = begin_tag('td', **kwargs) + u_str(display) + end_tag('td') return indent * ' ' + '<th align=right width=200>%s</th>%s' % (label_text, td)",1
2041,Python,underline text in label widget,https://github.com/tzutalin/labelImg/blob/6afd15aa88f89f41254e0004ed219b3965eb2c0d/labelImg.py#L664-L674,def editLabel(self): if not self.canvas.editing(): return item = self.currentItem() if not item: return text = self.labelDialog.popUp(item.text()) if text is not None: item.setText(text) item.setBackground(generateColorByText(text)) self.setDirty(),1
772,Python,underline text in label widget,https://github.com/tzutalin/labelImg/blob/6afd15aa88f89f41254e0004ed219b3965eb2c0d/labelImg.py#L839-L876,"def newShape(self): """"""Pop-up and give focus to the label editor. position MUST be in global coordinates. """""" if not self.useDefaultLabelCheckbox.isChecked() or not self.defaultLabelTextLine.text(): if len(self.labelHist) > 0: self.labelDialog = LabelDialog( parent=self, listItem=self.labelHist) # Sync single class mode from PR#106 if self.singleClassMode.isChecked() and self.lastLabel: text = self.lastLabel else: text = self.labelDialog.popUp(text=self.prevLabelText) self.lastLabel = text else: text = self.defaultLabelTextLine.text() # Add Chris self.diffcButton.setChecked(False) if text is not None: self.prevLabelText = text generate_color = generateColorByText(text) shape = self.canvas.setLastLabel(text, generate_color, generate_color) self.addLabel(shape) if self.beginner(): # Switch to edit mode. self.canvas.setEditing(True) self.actions.create.setEnabled(True) else: self.actions.editMode.setEnabled(True) self.setDirty() if text not in self.labelHist: self.labelHist.append(text) else: # self.canvas.undoLastLine() self.canvas.resetAllLines()",0
132,Python,string to date,https://github.com/spotify/luigi/blob/c5eca1c3c3ee2a7eb612486192a0da146710a1e9/luigi/date_interval.py#L266-L267,"def to_string(self): return '-'.join([d.strftime('%Y-%m-%d') for d in (self.date_a, self.date_b)])",3
540,Python,string to date,https://github.com/eliangcs/pystock-crawler/blob/8b803c8944f36af46daf04c6767a74132e37a101/pystock_crawler/spiders/yahoo.py#L12-L16,"def parse_date(date_str): if date_str: date = datetime.strptime(date_str, '%Y%m%d') return date.year, date.month - 1, date.day return '', '', ''",3
1051,Python,string to date,https://github.com/log2timeline/plaso/blob/9c564698d2da3ffbe23607a3c54c0582ea18a6cc/plaso/parsers/trendmicroav.py#L144-L194,"def _ConvertToTimestamp(self, date, time): """"""Converts date and time strings into a timestamp. Recent versions of Office Scan write a log field with a Unix timestamp. Older versions may not write this field; their logs only provide a date and a time expressed in the local time zone. This functions handles the latter case. Args: date (str): date as an 8-character string in the YYYYMMDD format. time (str): time as a 3 or 4-character string in the [H]HMM format or a 6-character string in the HHMMSS format. Returns: dfdatetime_time_elements.TimestampElements: the parsed timestamp. Raises: ValueError: if the date and time values cannot be parsed. """""" # Check that the strings have the correct length. if len(date) != 8: raise ValueError( 'Unsupported length of date string: {0!s}'.format(repr(date))) if len(time) < 3 or len(time) > 4: raise ValueError( 'Unsupported length of time string: {0!s}'.format(repr(time))) # Extract the date. try: year = int(date[:4], 10) month = int(date[4:6], 10) day = int(date[6:8], 10) except (TypeError, ValueError): raise ValueError('Unable to parse date string: {0!s}'.format(repr(date))) # Extract the time. Note that a single-digit hour value has no leading zero. try: hour = int(time[:-2], 10) minutes = int(time[-2:], 10) except (TypeError, ValueError): raise ValueError('Unable to parse time string: {0!s}'.format(repr(date))) time_elements_tuple = (year, month, day, hour, minutes, 0) date_time = dfdatetime_time_elements.TimeElements( time_elements_tuple=time_elements_tuple) date_time.is_local_time = True # TODO: add functionality to dfdatetime to control precision. date_time._precision = dfdatetime_definitions.PRECISION_1_MINUTE # pylint: disable=protected-access return date_time",3
1230,Python,string to date,https://github.com/supercoderz/pyeurofx/blob/3f579bb6e4836dadb187df8c74a9d186ae7e39e7/eurofx/common.py#L88-L98,"def get_date(date_string): d=None try: d=datetime.datetime.strptime(date_string, ""%d %B %Y"").date() except: d=datetime.datetime.strptime(date_string, ""%Y-%m-%d"").date() if d: return d.strftime(""%Y-%m-%d"") else: return date_string",3
1605,Python,string to date,https://github.com/roclark/sportsreference/blob/ea0bae432be76450e137671d2998eb38f962dffd/sportsreference/mlb/schedule.py#L172-L179,"def datetime(self): """""" Returns a datetime object of the month, day, year, and time the game was played. """""" date_string = '%s %s' % (self._date, self._year) date_string = re.sub(r' \(\d+\)', '', date_string) return datetime.strptime(date_string, '%A, %b %d %Y')",3
1759,Python,string to date,https://github.com/EdwinvO/pyutillib/blob/6d773c31d1f27cc5256d47feb8afb5c3ae5f0db5/pyutillib/date_utils.py#L35-L84,"def datestr2date(date_str): ''' Turns a string into a datetime.date object. This will only work if the format can be ""guessed"", so the string must have one of the formats from VALID_DATE_FORMATS_TEXT. Args: date_str (str) a string that represents a date Returns: datetime.date object Raises: ValueError if the input string does not have a valid format. ''' if any(c not in '0123456789-/' for c in date_str): raise ValueError('Illegal character in date string') if '/' in date_str: try: m, d, y = date_str.split('/') except: raise ValueError('Date {} must have no or exactly 2 slashes. {}'. format(date_str, VALID_DATE_FORMATS_TEXT)) elif '-' in date_str: try: d, m, y = date_str.split('-') except: raise ValueError('Date {} must have no or exactly 2 dashes. {}'. format(date_str, VALID_DATE_FORMATS_TEXT)) elif len(date_str) == 8 or len(date_str) == 6: d = date_str[-2:] m = date_str[-4:-2] y = date_str[:-4] else: raise ValueError('Date format not recognised. {}'.format( VALID_DATE_FORMATS_TEXT)) if len(y) == 2: year = 2000 + int(y) elif len(y) == 4: year = int(y) else: raise ValueError('year must be 2 or 4 digits') for s in (m, d): if 1 <= len(s) <= 2: month, day = int(m), int(d) else: raise ValueError('m and d must be 1 or 2 digits') try: return datetime.date(year, month, day) except ValueError: raise ValueError('Invalid date {}. {}'.format(date_str, VALID_DATE_FORMATS_TEXT))",3
795,Python,string to date,https://github.com/uw-it-aca/uw-restclients-iasystem/blob/f65f169d54b0d39e2d732cba529ccd8b6cb49f8a/uw_iasystem/evaluation.py#L192-L199,"def _datetime_from_string(date_string): if date_string: date_format = ""%Y-%m-%dT%H:%M:%S"" date_string = date_string.replace(""Z"", """") date = datetime.strptime(date_string, date_format) return pytz.utc.localize(date) return """"",2
352,Python,string to date,https://github.com/robgolding/tasklib/blob/0ad882377639865283021041f19add5aeb10126a/tasklib/serializing.py#L71-L79,"def timestamp_serializer(self, date): if not date: return '' # Any serialized timestamp should be localized, we need to # convert to UTC before converting to string (DATE_FORMAT uses UTC) date = date.astimezone(pytz.utc) return date.strftime(DATE_FORMAT)",1
940,Python,string to date,https://github.com/python-rope/rope/blob/1c9f9cd5964b099a99a9111e998f0dc728860688/rope/base/change.py#L94-L109,"def __str__(self): if self.time is not None: date = datetime.datetime.fromtimestamp(self.time) if date.date() == datetime.date.today(): string_date = 'today' elif date.date() == (datetime.date.today() - datetime.timedelta(1)): string_date = 'yesterday' elif date.year == datetime.date.today().year: string_date = date.strftime('%b %d') else: string_date = date.strftime('%d %b, %Y') string_time = date.strftime('%H:%M:%S') string_time = '%s %s ' % (string_date, string_time) return self.description + ' - ' + string_time return self.description",1
198,Python,string similarity levenshtein,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/functions.py#L1634-L1643,"def levenshtein(left, right): """"""Computes the Levenshtein distance of the two given strings. >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r']) >>> df0.select(levenshtein('l', 'r').alias('d')).collect() [Row(d=3)] """""" sc = SparkContext._active_spark_context jc = sc._jvm.functions.levenshtein(_to_java_column(left), _to_java_column(right)) return Column(jc)",3
475,Python,string similarity levenshtein,https://github.com/mcs07/ChemDataExtractor/blob/349a3bea965f2073141d62043b89319222e46af1/chemdataextractor/text/__init__.py#L232-L258,"def levenshtein(s1, s2, allow_substring=False): """"""Return the Levenshtein distance between two strings. The Levenshtein distance (a.k.a ""edit difference"") is the number of characters that need to be substituted, inserted or deleted to transform s1 into s2. Setting the `allow_substring` parameter to True allows s1 to be a substring of s2, so that, for example, ""hello"" and ""hello there"" would have a distance of zero. :param string s1: The first string :param string s2: The second string :param bool allow_substring: Whether to allow s1 to be a substring of s2 :returns: Levenshtein distance. :rtype int """""" len1, len2 = len(s1), len(s2) lev = [] for i in range(len1 + 1): lev.append([0] * (len2 + 1)) for i in range(len1 + 1): lev[i][0] = i for j in range(len2 + 1): lev[0][j] = 0 if allow_substring else j for i in range(len1): for j in range(len2): lev[i + 1][j + 1] = min(lev[i][j + 1] + 1, lev[i + 1][j] + 1, lev[i][j] + (s1[i] != s2[j])) return min(lev[len1]) if allow_substring else lev[len1][len2]",3
640,Python,string similarity levenshtein,https://github.com/ChrisCummins/labm8/blob/dd10d67a757aefb180cb508f86696f99440c94f5/text.py#L137-L167,"def diff(s1, s2): """""" Return a normalised Levenshtein distance between two strings. Distance is normalised by dividing the Levenshtein distance of the two strings by the max(len(s1), len(s2)). Examples: >>> text.diff(""foo"", ""foo"") 0 >>> text.diff(""foo"", ""fooo"") 1 >>> text.diff(""foo"", """") 1 >>> text.diff(""1234"", ""1 34"") 1 Arguments: s1 (str): Argument A. s2 (str): Argument B. Returns: float: Normalised distance between the two strings. """""" return levenshtein(s1, s2) / max(len(s1), len(s2))",3
669,Python,string similarity levenshtein,https://github.com/tyiannak/pyAudioAnalysis/blob/e3da991e7247492deba50648a4c7c0f41e684af4/pyAudioAnalysis/audioVisualization.py#L32-L52,"def levenshtein(str1, s2): ''' Distance between two strings ''' N1 = len(str1) N2 = len(s2) stringRange = [range(N1 + 1)] * (N2 + 1) for i in range(N2 + 1): stringRange[i] = range(i,i + N1 + 1) for i in range(0,N2): for j in range(0,N1): if str1[j] == s2[i]: stringRange[i+1][j+1] = min(stringRange[i+1][j] + 1, stringRange[i][j+1] + 1, stringRange[i][j]) else: stringRange[i+1][j+1] = min(stringRange[i+1][j] + 1, stringRange[i][j+1] + 1, stringRange[i][j] + 1) return stringRange[N2][N1]",3
1171,Python,string similarity levenshtein,https://github.com/Games-and-Simulations/sc-docker/blob/1d7adb9b5839783655564afc4bbcd204a0055dcb/scbw/utils.py#L18-L37,"def levenshtein_dist(s1: str, s2: str) -> int: if len(s1) < len(s2): return levenshtein_dist(s2, s1) # len(s1) >= len(s2) if len(s2) == 0: return len(s1) previous_row = range(len(s2) + 1) for i, c1 in enumerate(s1): current_row = [i + 1] for j, c2 in enumerate(s2): # j+1 instead of j since previous_row and current_row are one character longer insertions = previous_row[j + 1] + 1 deletions = current_row[j] + 1 # than s2 substitutions = previous_row[j] + (c1 != c2) current_row.append(min(insertions, deletions, substitutions)) previous_row = current_row return previous_row[-1]",3
1269,Python,string similarity levenshtein,https://github.com/Shoobx/xmldiff/blob/ec7835bce9ba69ff4ce03ab6c11397183b6f8411/xmldiff/_diff_match_patch_py3.py#L1110-L1134,"def diff_levenshtein(self, diffs): """"""Compute the Levenshtein distance; the number of inserted, deleted or substituted characters. Args: diffs: Array of diff tuples. Returns: Number of changes. """""" levenshtein = 0 insertions = 0 deletions = 0 for (op, data) in diffs: if op == self.DIFF_INSERT: insertions += len(data) elif op == self.DIFF_DELETE: deletions += len(data) elif op == self.DIFF_EQUAL: # A deletion and an insertion is one substitution. levenshtein += max(insertions, deletions) insertions = 0 deletions = 0 levenshtein += max(insertions, deletions) return levenshtein",3
1543,Python,string similarity levenshtein,https://github.com/Lilykos/pyphonetics/blob/7f55cccc1135e6015520a895eb6859318a4b6111/pyphonetics/distance_metrics/levenshtein.py#L1-L29,"def levenshtein_distance(word1, word2): """""" Computes the Levenshtein distance. [Reference]: https://en.wikipedia.org/wiki/Levenshtein_distance [Article]: Levenshtein, Vladimir I. (February 1966). ""Binary codes capable of correcting deletions, insertions,and reversals"". Soviet Physics Doklady 10 (8): 707–710. [Implementation]: https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance#Python """""" if len(word1) < len(word2): return levenshtein_distance(word2, word1) if len(word2) == 0: return len(word1) previous_row = list(range(len(word2) + 1)) for i, char1 in enumerate(word1): current_row = [i + 1] for j, char2 in enumerate(word2): insertions = previous_row[j + 1] + 1 deletions = current_row[j] + 1 substitutions = previous_row[j] + (char1 != char2) current_row.append(min(insertions, deletions, substitutions)) previous_row = current_row return previous_row[-1]",3
1903,Python,string similarity levenshtein,https://github.com/J535D165/recordlinkage/blob/87a5f4af904e0834047cd07ff1c70146b1e6d693/recordlinkage/algorithms/string.py#L52-L67,"def levenshtein_similarity(s1, s2): conc = pandas.Series(list(zip(s1, s2))) def levenshtein_apply(x): try: return 1 - jellyfish.levenshtein_distance(x[0], x[1]) \ / np.max([len(x[0]), len(x[1])]) except Exception as err: if pandas.isnull(x[0]) or pandas.isnull(x[1]): return np.nan else: raise err return conc.apply(levenshtein_apply)",2
566,Python,string similarity levenshtein,https://github.com/EventTeam/beliefs/blob/c07d22b61bebeede74a72800030dde770bf64208/src/beliefs/belief_utils.py#L143-L145,"def levenshtein_distance_metric(a, b): """""" 1 - farthest apart (same number of words, all diff). 0 - same"""""" return (levenshtein_distance(a, b) / (2.0 * max(len(a), len(b), 1)))",1
688,Python,string similarity levenshtein,https://github.com/SignalN/language/blob/5c50c78f65bcc2c999b44d530e7412185248352d/language/ngrams.py#L150-L151,"def word_similarity(s1, s2, n=3): return __similarity(s1, s2, word_ngrams, n=n)",1
1267,Python,sorting multiple arrays based on another arrays sorted order,https://github.com/JoseAntFer/pyny3d/blob/fb81684935a24f7e50c975cb4383c81a63ab56df/pyny3d/utils.py#L8-L33,"def sort_numpy(array, col=0, order_back=False): """""" Sorts the columns for an entire ``ndarrray`` according to sorting one of them. :param array: Array to sort. :type array: ndarray :param col: Master column to sort. :type col: int :param order_back: If True, also returns the index to undo the new order. :type order_back: bool :returns: sorted_array or [sorted_array, order_back] :rtype: ndarray, list """""" x = array[:,col] sorted_index = np.argsort(x, kind = 'quicksort') sorted_array = array[sorted_index] if not order_back: return sorted_array else: n_points = sorted_index.shape[0] order_back = np.empty(n_points, dtype=int) order_back[sorted_index] = np.arange(n_points) return [sorted_array, order_back]",3
1832,Python,sorting multiple arrays based on another arrays sorted order,https://github.com/loverajoel/sqlalchemy-elasticquery/blob/4c99b81f59e7bb20eaeedb3adbf5126e62bbc25c/sqlalchemy_elasticquery/elastic_query.py#L159-L167,"def sort(self, sort_list): """""" Sort """""" order = [] for sort in sort_list: if sort_list[sort] == ""asc"": order.append(asc(getattr(self.model, sort, None))) elif sort_list[sort] == ""desc"": order.append(desc(getattr(self.model, sort, None))) return order",3
324,Python,sorting multiple arrays based on another arrays sorted order,https://github.com/ml31415/numpy-groupies/blob/0911e9c59b14e11319e82d0876056ad2a17e6568/numpy_groupies/aggregate_numpy.py#L181-L185,"def _sort(group_idx, a, size=None, fill_value=None, dtype=None, reverse=False): sortidx = np.lexsort((-a if reverse else a, group_idx)) # Reverse sorting back to into grouped order, but preserving groupwise sorting revidx = np.argsort(np.argsort(group_idx, kind='mergesort'), kind='mergesort') return a[sortidx][revidx]",2
1146,Python,sorting multiple arrays based on another arrays sorted order,https://github.com/shawnbot/py-organ/blob/1663b0943dd9d35a43618b4914c839f7c07d7966/organ/__init__.py#L45-L52,"def multisorter(*sorts): def _sort(aa, bb): for sort in sorts: order = sort(aa, bb) if order != 0: return order return 0 return _sort",2
1977,Python,sorting multiple arrays based on another arrays sorted order,https://github.com/google/prettytensor/blob/75daa0b11252590f548da5647addc0ea610c4c45/prettytensor/tutorial/data_utils.py#L82-L89,"def permute_data(arrays, random_state=None): """"""Permute multiple numpy arrays with the same order."""""" if any(len(a) != len(arrays[0]) for a in arrays): raise ValueError('All arrays must be the same length.') if not random_state: random_state = np.random order = random_state.permutation(len(arrays[0])) return [a[order] for a in arrays]",2
528,Python,sorting multiple arrays based on another arrays sorted order,https://github.com/PiotrDabkowski/Js2Py/blob/c0fa43f5679cf91ca8986c5747fcb07a433dc584/js2py/internals/prototypes/jsarray.py#L157-L183,"def sort( this, args ): # todo: this assumes array continous (not sparse) - fix for sparse arrays cmpfn = get_arg(args, 0) if not GetClass(this) in ('Array', 'Arguments'): return to_object(this, args.space) # do nothing arr_len = js_arr_length(this) if not arr_len: return this arr = [ (this.get(unicode(e)) if this.has_property(unicode(e)) else None) for e in xrange(arr_len) ] if not is_callable(cmpfn): cmpfn = None cmp = lambda a, b: sort_compare(a, b, cmpfn) if six.PY3: key = functools.cmp_to_key(cmp) arr.sort(key=key) else: arr.sort(cmp=cmp) for i in xrange(arr_len): if arr[i] is None: this.delete(unicode(i)) else: this.put(unicode(i), arr[i]) return this",1
1388,Python,sorting multiple arrays based on another arrays sorted order,https://github.com/radujica/baloo/blob/f6e05e35b73a75e8a300754c6bdc575e5f2d53b9/baloo/weld/weld_ops.py#L464-L491,"def _weld_sort(arrays, weld_types, ascending=True): obj_id, index_obj = create_weld_object(arrays[0]) index_obj.weld_code = 'len({})'.format(obj_id) # get indexes that will be sorted and returned index_column = weld_range(0, index_obj, 1) arrays.insert(0, index_column) weld_types.insert(0, WeldLong()) weld_obj_vec_of_struct = weld_arrays_to_vec_of_struct(arrays, weld_types) weld_obj = create_empty_weld_object() weld_obj_vec_of_struct_id = get_weld_obj_id(weld_obj, weld_obj_vec_of_struct) types = struct_of('{e}', weld_types) # TODO: update here when sorting on structs is possible ascending_sort_func = '{}'.format(', '.join(('e.${}'.format(i) for i in range(1, len(arrays))))) zero_literals = dict(enumerate([to_weld_literal(0, weld_type) for weld_type in weld_types])) descending_sort_func = '{}'.format(', '.join(('{} - e.${}'.format(zero_literals[i], i) for i in range(1, len(arrays))))) sort_func = ascending_sort_func if ascending else descending_sort_func weld_template = 'sort({struct}, |e: {types}| {sort_func})' weld_obj.weld_code = weld_template.format(struct=weld_obj_vec_of_struct_id, types=types, sort_func=sort_func) return weld_obj",1
1811,Python,sorting multiple arrays based on another arrays sorted order,https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/indexes/multi.py#L292-L350,"def from_arrays(cls, arrays, sortorder=None, names=None): """""" Convert arrays to MultiIndex. Parameters ---------- arrays : list / sequence of array-likes Each array-like gives one level's value for each data point. len(arrays) is the number of levels. sortorder : int or None Level of sortedness (must be lexicographically sorted by that level). names : list / sequence of str, optional Names for the levels in the index. Returns ------- index : MultiIndex See Also -------- MultiIndex.from_tuples : Convert list of tuples to MultiIndex. MultiIndex.from_product : Make a MultiIndex from cartesian product of iterables. MultiIndex.from_frame : Make a MultiIndex from a DataFrame. Examples -------- >>> arrays = [[1, 1, 2, 2], ['red', 'blue', 'red', 'blue']] >>> pd.MultiIndex.from_arrays(arrays, names=('number', 'color')) MultiIndex(levels=[[1, 2], ['blue', 'red']], codes=[[0, 0, 1, 1], [1, 0, 1, 0]], names=['number', 'color']) """""" error_msg = ""Input must be a list / sequence of array-likes."" if not is_list_like(arrays): raise TypeError(error_msg) elif is_iterator(arrays): arrays = list(arrays) # Check if elements of array are list-like for array in arrays: if not is_list_like(array): raise TypeError(error_msg) # Check if lengths of all arrays are equal or not, # raise ValueError, if not for i in range(1, len(arrays)): if len(arrays[i]) != len(arrays[i - 1]): raise ValueError('all arrays must be same length') from pandas.core.arrays.categorical import _factorize_from_iterables codes, levels = _factorize_from_iterables(arrays) if names is None: names = [getattr(arr, ""name"", None) for arr in arrays] return MultiIndex(levels=levels, codes=codes, sortorder=sortorder, names=names, verify_integrity=False)",1
511,Python,sorting multiple arrays based on another arrays sorted order,https://github.com/stephan-mclean/KickassTorrentsAPI/blob/4d867a090c06ce95b9ed996b48092cb5bfe28bbd/kat.py#L202-L212,"def _format_sort(self, sort, order): sorting = """" if sort: self.sort = sort sorting = ""?field="" + self.sort if order: self.order = order else: self.order = Sorting.Order.DESC sorting = sorting + ""&sorder="" + self.order return sorting",0
1016,Python,sort string list,https://github.com/qacafe/cdrouter.py/blob/aacf2c6ab0b987250f7b1892f4bba14bb2b7dbe5/cdrouter/cdrouter.py#L282-L288,"def list(self, base, filter=None, type=None, sort=None, limit=None, page=None, format=None): # pylint: disable=redefined-builtin if sort != None: if not isinstance(sort, list): sort = [sort] sort = ','.join(sort) return self.get(base, params={'filter': filter, 'type': type, 'sort': sort, 'limit': limit, 'page': page, 'format': format})",3
1897,Python,sort string list,https://github.com/OnroerendErfgoed/skosprovider_getty/blob/5aa0b5a8525d607e07b631499ff31bac7a0348b7/skosprovider_getty/providers.py#L390-L396,"def _sort(self, items, sort, language='en', reverse=False): if sort is None: sort = 'id' if sort == 'sortlabel': sort='label' items.sort(key=lambda item: item[sort], reverse=reverse) return items",3
483,Python,sort string list,https://github.com/loverajoel/sqlalchemy-elasticquery/blob/4c99b81f59e7bb20eaeedb3adbf5126e62bbc25c/sqlalchemy_elasticquery/elastic_query.py#L159-L167,"def sort(self, sort_list): """""" Sort """""" order = [] for sort in sort_list: if sort_list[sort] == ""asc"": order.append(asc(getattr(self.model, sort, None))) elif sort_list[sort] == ""desc"": order.append(desc(getattr(self.model, sort, None))) return order",2
763,Python,sort string list,https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/jx_python/containers/doc_store.py#L144-L174,"def _sort(self, short_list, sorts): """""" TAKE SHORTLIST, RETURN IT SORTED :param short_list: :param sorts: LIST OF SORTS TO PERFORM :return: """""" sort_values = self._index_columns(sorts) # RECURSIVE SORTING output = [] def _sort_more(short_list, i, sorts): if len(sorts) == 0: output.extend(short_list) sort = sorts[0] index = self._index[sort_values[i]] if sort.sort == 1: sorted_keys = sorted(index.keys()) elif sort.sort == -1: sorted_keys = reversed(sorted(index.keys())) else: sorted_keys = list(index.keys()) for k in sorted_keys: self._sort(index[k] & short_list, i + 1, sorts[1:]) _sort_more(short_list, 0, sorts) return output",2
1049,Python,sort string list,https://github.com/mirukan/lunafind/blob/77bdfe02df98a7f74d0ae795fee3b1729218995d/lunafind/order.py#L53-L79,"def sort(posts: List[Post], by: str) -> List[Post]: by_val = by.replace(""asc_"", """").replace(""desc_"", """") in_dict = (ORDER_NUM if by_val in ORDER_NUM else ORDER_DATE if by_val in ORDER_DATE else ORDER_FUNCS if by_val in ORDER_FUNCS else None) if not in_dict: raise ValueError( f""Got {by_val!r} as ordering method, must be one of: %s"" % "", "".join(set(ORDER_NUM) | set(ORDER_DATE) | set(ORDER_FUNCS)) ) if in_dict == ORDER_FUNCS: posts.sort(key=ORDER_FUNCS[by], reverse=(by != ""random"")) return posts by_full = by if by.startswith(""asc_"") or by.startswith(""desc_"") else \ f""%s_{by}"" % in_dict[by][0] def sort_key(post: Post) -> int: key = in_dict[by_val][1] key = post.info[key] if not callable(key) else key(post.info) return pend.parse(key) if in_dict == ORDER_DATE else key posts.sort(key=sort_key, reverse=by_full.startswith(""desc_"")) return posts",2
1538,Python,sort string list,https://github.com/readbeyond/aeneas/blob/9d95535ad63eef4a98530cfdff033b8c35315ee1/aeneas/idsortingalgorithm.py#L79-L109,"def sort(self, ids): """""" Sort the given list of identifiers, returning a new (sorted) list. :param list ids: the list of identifiers to be sorted :rtype: list """""" def extract_int(string): """""" Extract an integer from the given string. :param string string: the identifier string :rtype: int """""" return int(re.sub(r""[^0-9]"", """", string)) tmp = list(ids) if self.algorithm == IDSortingAlgorithm.UNSORTED: self.log(u""Sorting using UNSORTED"") elif self.algorithm == IDSortingAlgorithm.LEXICOGRAPHIC: self.log(u""Sorting using LEXICOGRAPHIC"") tmp = sorted(ids) elif self.algorithm == IDSortingAlgorithm.NUMERIC: self.log(u""Sorting using NUMERIC"") tmp = ids try: tmp = sorted(tmp, key=extract_int) except (ValueError, TypeError) as exc: self.log_exc(u""Not all id values contain a numeric part. Returning the id list unchanged."", exc, False, None) return tmp",2
415,Python,sort string list,https://github.com/aptivate/django-sortable-listview/blob/9d5fa5847f0c3e80893780c6540e5098635ace9f/sortable_listview/views.py#L127-L133,"def get_sort_string(self, sort=None): if not sort: sort = self.sort sort_string = '' if not sort == self.default_sort: sort_string = self.sort_parameter + '=' + sort return sort_string",0
1323,Python,sort string list,https://github.com/opencobra/cobrapy/blob/9d1987cdb3a395cf4125a3439c3b002ff2be2009/cobra/io/dict.py#L149-L184,"def model_to_dict(model, sort=False): """"""Convert model to a dict. Parameters ---------- model : cobra.Model The model to reformulate as a dict. sort : bool, optional Whether to sort the metabolites, reactions, and genes or maintain the order defined in the model. Returns ------- OrderedDict A dictionary with elements, 'genes', 'compartments', 'id', 'metabolites', 'notes' and 'reactions'; where 'metabolites', 'genes' and 'metabolites' are in turn lists with dictionaries holding all attributes to form the corresponding object. See Also -------- cobra.io.model_from_dict """""" obj = OrderedDict() obj[""metabolites""] = list(map(metabolite_to_dict, model.metabolites)) obj[""reactions""] = list(map(reaction_to_dict, model.reactions)) obj[""genes""] = list(map(gene_to_dict, model.genes)) obj[""id""] = model.id _update_optional(model, obj, _OPTIONAL_MODEL_ATTRIBUTES, _ORDERED_OPTIONAL_MODEL_KEYS) if sort: get_id = itemgetter(""id"") obj[""metabolites""].sort(key=get_id) obj[""reactions""].sort(key=get_id) obj[""genes""].sort(key=get_id) return obj",0
220,Python,socket recv timeout,https://github.com/anthonynguyen/pyrcon/blob/278cba95dd4d53a347d37acfce556ad375370e15/pyrcon/rcon.py#L97-L122,"def recvall(self, timeout=0.5): """""" Receive the RCON command response :param timeout: The timeout between consequent data receive :return str: The RCON command response with header stripped out """""" response = '' self.socket.setblocking(False) start = time.time() while True: if response and time.time() - start > timeout: break elif time.time() - start > timeout * 2: break try: data = self.socket.recv(4096) if data: response += data.replace(self._rconreplystring, '') start = time.time() else: time.sleep(0.1) except socket.error: pass return response.strip()",3
833,Python,socket recv timeout,https://github.com/secdev/scapy/blob/3ffe757c184017dd46464593a8f80f85abc1e79a/scapy/contrib/isotp.py#L612-L621,"def recv_with_timeout(self, timeout=1): """"""Receive a complete ISOTP message, blocking until a message is received or the specified timeout is reached. If timeout is 0, then this function doesn't block and returns the first frame in the receive buffer or None if there isn't any."""""" msg = self.ins.recv(timeout) t = time.time() if msg is None: raise Scapy_Exception(""Timeout"") return self.basecls, msg, t",3
862,Python,socket recv timeout,https://github.com/nfcpy/nfcpy/blob/6649146d1afdd5e82b2b6b1ea00aa58d50785117/examples/llcp-dta-iut.py#L156-L180,"def recv(self, recv_socket, llc): time.sleep(0.1) # delay to accept inbound connection before resolve echo_buffer = queue.Queue(self.options.co_echo_buffer) send_socket = nfc.llcp.Socket(llc, nfc.llcp.DATA_LINK_CONNECTION) if self.options.pattern_number == 0x1200: send_socket.connect(self.options.sap_lt_co_out_dest) elif self.options.pattern_number == 0x1240: send_socket.connect(""urn:nfc:sn:dta-co-echo-out"") elif self.options.pattern_number == 0x1280: send_socket.connect(llc.resolve(""urn:nfc:sn:dta-co-echo-out"")) send_thread = Thread(target=self.send, args=(send_socket, echo_buffer)) send_thread.start() log.info(""receiving from sap %d"", recv_socket.getpeername()) while recv_socket.poll(""recv""): data = recv_socket.recv() if data == None: break log.info(""rcvd %d byte"", len(data)) recv_socket.setsockopt(nfc.llcp.SO_RCVBSY, echo_buffer.full()) echo_buffer.put(data) log.info(""remote side closed connection"") try: echo_buffer.put_nowait(int(0)) except queue.Full: pass send_thread.join() recv_socket.close() log.info(""recv thread terminated"")",3
1091,Python,socket recv timeout,https://github.com/AirtestProject/Poco/blob/2c559a586adf3fd11ee81cabc446d4d3f6f2d119/poco/utils/simplerpc/transport/tcp/main.py#L34-L40,"def recv(self): try: msg_bytes = self.c.recv() except socket.timeout: # print(""socket recv timeout"") msg_bytes = b"""" return self.prot.input(msg_bytes)",3
1487,Python,socket recv timeout,https://github.com/rameshg87/pyremotevbox/blob/123dffff27da57c8faa3ac1dd4c68b1cf4558b1a/pyremotevbox/ZSI/wstools/TimeoutSocket.py#L90-L93,"def recv(self, amt, flags=0): if select.select([self.sock], [], [], self.timeout)[0]: return self.sock.recv(amt, flags) raise TimeoutError('socket recv() timeout.')",3
1712,Python,socket recv timeout,https://github.com/nfcpy/nfcpy/blob/6649146d1afdd5e82b2b6b1ea00aa58d50785117/src/nfc/snep/client.py#L51-L74,"def recv_response(socket, acceptable_length, timeout): if socket.poll(""recv"", timeout): snep_response = socket.recv() if len(snep_response) < 6: log.debug(""snep response initial fragment too short"") return None version, status, length = struct.unpack("">BBL"", snep_response[:6]) if length > acceptable_length: log.debug(""snep response exceeds acceptable length"") return None if len(snep_response) - 6 < length: # request remaining fragments socket.send(b""\x10\x00\x00\x00\x00\x00"") while len(snep_response) - 6 < length: if socket.poll(""recv"", timeout): snep_response += socket.recv() else: return None return bytearray(snep_response)",3
156,Python,socket recv timeout,https://github.com/log2timeline/plaso/blob/9c564698d2da3ffbe23607a3c54c0582ea18a6cc/plaso/engine/zeromq_queue.py#L161-L171,"def _SetSocketTimeouts(self): """"""Sets the timeouts for socket send and receive."""""" # Note that timeout must be an integer value. If timeout is a float # it appears that zmq will not enforce the timeout. timeout = int(self.timeout_seconds * 1000) receive_timeout = min( self._ZMQ_SOCKET_RECEIVE_TIMEOUT_MILLISECONDS, timeout) send_timeout = min(self._ZMQ_SOCKET_SEND_TIMEOUT_MILLISECONDS, timeout) self._zmq_socket.setsockopt(zmq.RCVTIMEO, receive_timeout) self._zmq_socket.setsockopt(zmq.SNDTIMEO, send_timeout)",2
942,Python,socket recv timeout,https://github.com/Jaymon/endpoints/blob/2f1c4ae2c69a168e69447d3d8395ada7becaa5fb/endpoints/interface/uwsgi/client.py#L265-L307,"def recv_raw(self, timeout, opcodes, **kwargs): """"""this is very internal, it will return the raw opcode and data if they match the passed in opcodes"""""" orig_timeout = self.get_timeout(timeout) timeout = orig_timeout while timeout > 0.0: start = time.time() if not self.connected: self.connect(timeout=timeout, **kwargs) with self.wstimeout(timeout, **kwargs) as timeout: logger.debug('{} waiting to receive for {} seconds'.format(self.client_id, timeout)) try: opcode, data = self.ws.recv_data() if opcode in opcodes: timeout = 0.0 break else: if opcode == websocket.ABNF.OPCODE_CLOSE: raise websocket.WebSocketConnectionClosedException() except websocket.WebSocketTimeoutException: pass except websocket.WebSocketConnectionClosedException: # bug in Websocket.recv_data(), this should be done by Websocket try: self.ws.shutdown() except AttributeError: pass #raise EOFError(""websocket closed by server and reconnection did nothing"") if timeout: stop = time.time() timeout -= (stop - start) else: break if timeout < 0.0: raise IOError(""recv timed out in {} seconds"".format(orig_timeout)) return opcode, data",2
1053,Python,socket recv timeout,https://github.com/teepark/greenhouse/blob/8fd1be4f5443ba090346b5ec82fdbeb0a060d956/greenhouse/io/sockets.py#L346-L377,"def recvfrom(self, bufsize, flags=0): """"""receive data on a socket that isn't necessarily a 1-1 connection .. note:: this method will block until data is available to be read :param bufsize: the maximum number of bytes to receive. fewer may be returned, however :type bufsize: int :param flags: flags for the receive call. consult the unix manpage for ``recv(2)`` for what flags are available :type flags: int :returns: a two-tuple of ``(data, address)`` -- the string data received and the address from which it was received """""" with self._registered('re'): while 1: if self._closed: raise socket.error(errno.EBADF, ""Bad file descriptor"") try: return self._sock.recvfrom(bufsize, flags) except socket.error, exc: if not self._blocking or exc[0] not in _BLOCKING_OP: raise sys.exc_clear() if self._readable.wait(self.gettimeout()): raise socket.timeout(""timed out"") if scheduler.state.interrupted: raise IOError(errno.EINTR, ""interrupted system call"")",2
1253,Python,socket recv timeout,https://github.com/cablehead/vanilla/blob/c9f5b86f45720a30e8840fb68b1429b919c4ca66/vanilla/message.py#L57-L58,"def recv_n(self, n, timeout=-1): return self.recver.recv_n(n, timeout=timeout)",1
758,Python,set working directory,https://github.com/GNS3/gns3-server/blob/a221678448fb5d24e977ef562f81d56aacc89ab1/gns3server/compute/dynamips/dynamips_hypervisor.py#L152-L162,"def set_working_dir(self, working_dir): """""" Sets the working directory for this hypervisor. :param working_dir: path to the working directory """""" # encase working_dir in quotes to protect spaces in the path yield from self.send('hypervisor working_dir ""{}""'.format(working_dir)) self._working_dir = working_dir log.debug(""Working directory set to {}"".format(self._working_dir))",3
870,Python,set working directory,https://github.com/Alignak-monitoring/alignak/blob/f3c145207e83159b799d3714e4241399c7740a64/alignak/daemon.py#L1243-L1259,"def change_to_workdir(self): """"""Change working directory to working attribute :return: None """""" logger.info(""Changing working directory to: %s"", self.workdir) self.check_dir(self.workdir) try: os.chdir(self.workdir) except OSError as exp: self.exit_on_error(""Error changing to working directory: %s. Error: %s. "" ""Check the existence of %s and the %s/%s account "" ""permissions on this directory."" % (self.workdir, str(exp), self.workdir, self.user, self.group), exit_code=3) self.pre_log.append((""INFO"", ""Using working directory: %s"" % os.path.abspath(self.workdir)))",3
1419,Python,set working directory,https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/ipythonconsole/plugin.py#L577-L583,"def set_working_directory(self, dirname): """"""Set current working directory. In the workingdirectory and explorer plugins. """""" if dirname: self.main.workingdirectory.chdir(dirname, refresh_explorer=True, refresh_console=False)",3
1462,Python,set working directory,https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/io/abinit/tasks.py#L4149-L4153,"def set_workdir(self, workdir, chroot=False): """"""Set the working directory of the task."""""" super().set_workdir(workdir, chroot=chroot) # Small hack: the log file of optics is actually the main output file. self.output_file = self.log_file",3
1772,Python,set working directory,https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/editor/plugin.py#L1537-L1542,"def __set_workdir(self): """"""Set current script directory as working directory"""""" fname = self.get_current_filename() if fname is not None: directory = osp.dirname(osp.abspath(fname)) self.open_dir.emit(directory)",3
124,Python,set working directory,https://github.com/biocore/burrito-fillings/blob/02ab71a46119b40793bd56a4ae00ca15f6dc3329/bfillings/mothur.py#L391-L399,"def _set_WorkingDir(self, path): """"""Sets the working directory """""" self._curr_working_dir = path try: mkdir(self.WorkingDir) except OSError: # Directory already exists pass",2
1387,Python,set working directory,https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/io/abinit/flows.py#L1718-L1752,"def register_work(self, work, deps=None, manager=None, workdir=None): """""" Register a new :class:`Work` and add it to the internal list, taking into account possible dependencies. Args: work: :class:`Work` object. deps: List of :class:`Dependency` objects specifying the dependency of this node. An empy list of deps implies that this node has no dependencies. manager: The :class:`TaskManager` responsible for the submission of the task. If manager is None, we use the `TaskManager` specified during the creation of the work. workdir: The name of the directory used for the :class:`Work`. Returns: The registered :class:`Work`. """""" if getattr(self, ""workdir"", None) is not None: # The flow has a directory, build the named of the directory of the work. work_workdir = None if workdir is None: work_workdir = os.path.join(self.workdir, ""w"" + str(len(self))) else: work_workdir = os.path.join(self.workdir, os.path.basename(workdir)) work.set_workdir(work_workdir) if manager is not None: work.set_manager(manager) self.works.append(work) if deps: deps = [Dependency(node, exts) for node, exts in deps.items()] work.add_deps(deps) return work",2
2075,Python,set working directory,https://github.com/materialsproject/pymatgen/blob/4ca558cf72f8d5f8a1f21dfdfc0181a971c186da/pymatgen/io/abinit/flows.py#L254-L266,"def set_workdir(self, workdir, chroot=False): """""" Set the working directory. Cannot be set more than once unless chroot is True """""" if not chroot and hasattr(self, ""workdir"") and self.workdir != workdir: raise ValueError(""self.workdir != workdir: %s, %s"" % (self.workdir, workdir)) # Directories with (input|output|temporary) data. self.workdir = os.path.abspath(workdir) self.indir = Directory(os.path.join(self.workdir, ""indata"")) self.outdir = Directory(os.path.join(self.workdir, ""outdata"")) self.tmpdir = Directory(os.path.join(self.workdir, ""tmpdata"")) self.wdir = Directory(self.workdir)",2
1700,Python,set working directory,https://github.com/nugget/python-insteonplm/blob/65548041f1b0729ae1ae904443dd81b0c6cbf1bf/insteonplm/tools.py#L877-L903,"def do_set_workdir(self, args): """"""Set the working directory. The working directory is used to load and save known devices to improve startup times. During startup the application loads and saves a file `insteon_plm_device_info.dat`. This file is saved in the working directory. The working directory has no default value. If the working directory is not set, the `insteon_plm_device_info.dat` file is not loaded or saved. Usage: set_workdir workdir Arguments: workdir: Required - Working directory to load and save devie list """""" params = args.split() workdir = None try: workdir = params[0] except IndexError: _LOGGING.error('Device name required.') self.do_help('set_workdir') if workdir: self.tools.workdir = workdir",1
920,Python,set working directory,https://github.com/xolox/python-vcs-repo-mgr/blob/fdad2441a3e7ba5deeeddfa1c2f5ebc00c393aed/vcs_repo_mgr/__init__.py#L1875-L1896,"def update_context(self): """""" Try to ensure that external commands are executed in the local repository. What :func:`update_context()` does depends on whether the directory given by :attr:`local` exists: - If :attr:`local` exists then the working directory of :attr:`context` will be set to :attr:`local`. This is to ensure that version control commands are run inside of the intended version control repository. - If :attr:`local` doesn't exist then the working directory of :attr:`context` is cleared. This avoids external commands from failing due to an invalid (non existing) working directory. """""" if self.context.is_directory(self.local): # Set the working directory of the execution context # to the directory containing the local repository. self.context.options['directory'] = self.local else: # Clear the execution context's working directory. self.context.options.pop('directory', None)",0
172,Python,set file attrib hidden,https://github.com/mattharrison/rst2odp/blob/4adbf29b28c8207ec882f792ded07e98b1d3e7d0/odplib/preso.py#L170-L175,"def sub_el(parent, tag, attrib=None): attrib = attrib or {} tag = get_nstag(tag) attrib = update_attrib(attrib) el = et.SubElement(parent, tag, attrib) # , nsmap=NAMESPACES) return el",2
74,Python,set file attrib hidden,https://github.com/KelSolaar/Umbra/blob/66f45f08d9d723787f1191989f8b0dda84b412ce/umbra/components/factory/script_editor/editor.py#L448-L467,"def set_file(self, file=None, is_modified=False, is_untitled=False): """""" Sets the editor file. :param File: File to set. :type File: unicode :param is_modified: File modified state. :type is_modified: bool :param is_untitled: File untitled state. :type is_untitled: bool :return: Method success. :rtype: bool """""" LOGGER.debug(""> Setting '{0}' editor file."".format(file)) self.__file = file self.__is_untitled = is_untitled self.set_modified(is_modified) self.set_title() return True",1
562,Python,set file attrib hidden,https://github.com/SuminAndrew/lxml-asserts/blob/a5e8d25177357ba52e5eb0abacdc2f4bab59d584/lxml_asserts/__init__.py#L26-L59,"def _assert_tag_and_attributes_are_equal(xml1, xml2, can_extend=False): if xml1.tag != xml2.tag: raise AssertionError(u'Tags do not match: {tag1} != {tag2}'.format( tag1=_describe_element(xml1), tag2=_describe_element(xml2) )) added_attributes = set(xml2.attrib).difference(xml1.attrib) missing_attributes = set(xml1.attrib).difference(xml2.attrib) if missing_attributes: raise AssertionError(u'Second xml misses attributes: {path}/({attributes})'.format( path=_describe_element(xml2), attributes=','.join(missing_attributes) )) if not can_extend and added_attributes: raise AssertionError(u'Second xml has additional attributes: {path}/({attributes})'.format( path=_describe_element(xml2), attributes=','.join(added_attributes) )) for attrib in xml1.attrib: if not _xml_compare_text(xml1.attrib[attrib], xml2.attrib[attrib], False): raise AssertionError(u""Attribute values are not equal: {path}/{attribute}['{v1}' != '{v2}']"".format( path=_describe_element(xml1), attribute=attrib, v1=xml1.attrib[attrib], v2=xml2.attrib[attrib] )) if not _xml_compare_text(xml1.text, xml2.text, True): raise AssertionError(u""Tags text differs: {path}['{t1}' != '{t2}']"".format( path=_describe_element(xml1), t1=xml1.text, t2=xml2.text )) if not _xml_compare_text(xml1.tail, xml2.tail, True): raise AssertionError(u""Tags tail differs: {path}['{t1}' != '{t2}']"".format( path=_describe_element(xml1), t1=xml1.tail, t2=xml2.tail ))",1
816,Python,set file attrib hidden,https://github.com/tjcsl/ion/blob/5d722b0725d572039bb0929fd5715a4070c82c72/intranet/utils/html.py#L20-L21,"def safe_html(txt): return bleach.linkify(bleach.clean(txt, tags=ALLOWED_TAGS, attributes=ALLOWED_ATTRIBUTES, styles=ALLOWED_STYLES))",1
1468,Python,set file attrib hidden,https://github.com/HearthSim/python-hsreplay/blob/ec03a18a75ae4c1e0facc583a7213a4c5b7f99ff/hsreplay/elements.py#L47-L66,"def xml(self): element = ElementTree.Element(self.tagname) for node in self.nodes: element.append(node.xml()) for attr in self.attributes: attrib = getattr(self, attr, None) if attrib is not None: if isinstance(attrib, bool): attrib = str(attrib).lower() elif isinstance(attrib, int): # Check for enums attrib = str(int(attrib)) element.attrib[attr] = attrib if self.timestamp and self.ts: element.attrib[""ts""] = self.ts.isoformat() for k, v in self._attributes.items(): element.attrib[k] = v return element",1
29,Python,set file attrib hidden,https://github.com/drestebon/papageorge/blob/a30ea59bf6b4f5d151bd3f476a0a8357d89495d4/lib/papageorge/cli.py#L573-L579,"def continuation(self, regexp, txt): txt_ = self.txt_list.body.pop().get_text() self.txt_list.body.append(ConsoleText((txt_[1][0][0], txt_[0]+regexp.groups()[0]))) pos = len(self.txt_list.body)-1 self.txt_list.set_focus(pos) return False",0
251,Python,set file attrib hidden,https://github.com/MolSSI-BSE/basis_set_exchange/blob/e79110aaeb65f392ed5032420322dee3336948f7/basis_set_exchange/cli/bsecurate_handlers.py#L92-L105,"def bsecurate_cli_handle_subcmd(args): handler_map = { 'get-reader-formats': _bsecurate_cli_get_reader_formats, 'elements-in-files': _bsecurate_cli_elements_in_files, 'component-file-refs': _bsecurate_cli_component_file_refs, 'print-component-file': _bsecurate_cli_print_component_file, 'compare-basis-sets': _bsecurate_cli_compare_basis_sets, 'compare-basis-files': _bsecurate_cli_compare_basis_files, 'make-diff': _bsecurate_cli_make_diff, 'view-graph': _bsecurate_cli_view_graph, 'make-graph-file': _bsecurate_cli_make_graph_file } return handler_map[args.subcmd](args)",0
603,Python,set file attrib hidden,https://github.com/bmuller/txairbrake/blob/38e65fe2330c6ce7fb788bad0cff0a85cceb1943/txairbrake/observers.py#L112-L125,"def _tracebackToTree(self, failure): backtrace = ET.Element('backtrace') frames = failure.stack + failure.frames for function_name, filename, line_number, localz, globalz in frames: attrib = {'file': filename, 'number': str(line_number), 'method': ""%s: %s"" % ( function_name, linecache.getline(filename, line_number).strip())} backtrace.append(ET.Element('line', attrib=attrib)) return backtrace",0
1415,Python,set file attrib hidden,https://github.com/lamestation/lamestation-sdk-tools/blob/25d1fe8dc276cf4addbb13fd29ea3cf22b09f9ca/lspaint/EventHandler.py#L97-L106,"def SetUndoRedo(self): f = FileManager().CurrentFile() self.parent.toolbar.EnableTool( wx.ID_UNDO, f.undo) self.parent.menu.Enable( wx.ID_UNDO, f.undo) # self.parent.toolbar.EnableTool( wx.ID_SAVE, f.undo) # self.parent.menu.Enable( wx.ID_SAVE, f.undo) self.parent.toolbar.EnableTool( wx.ID_REDO, f.redo) self.parent.menu.Enable( wx.ID_REDO, f.redo)",0
1645,Python,set file attrib hidden,https://github.com/googledatalab/pydatalab/blob/d9031901d5bca22fe0d5925d204e6698df9852e1/solutionbox/ml_workbench/xgboost/trainer/task.py#L155-L261,"def parse_arguments(argv): """"""Parse the command line arguments."""""" parser = DatalabParser( epilog=('Note that if using a DNN model, --hidden-layer-size1=NUM, ' '--hidden-layer-size2=NUM, ..., is also required. '), datalab_epilog=("""""" Note that if using a DNN model, hidden-layer-size1: NUM hidden-layer-size2: NUM ... is also required. """""")) # HP parameters parser.add_argument( '--epsilon', type=float, default=0.0005, metavar='R', help='tf.train.AdamOptimizer epsilon. Only used in dnn models.') parser.add_argument( '--l1-regularization', type=float, default=0.0, metavar='R', help='L1 term for linear models.') parser.add_argument( '--l2-regularization', type=float, default=0.0, metavar='R', help='L2 term for linear models.') # Model parameters parser.add_argument( '--model', required=True, choices=['linear_classification', 'linear_regression', 'dnn_classification', 'dnn_regression']) parser.add_argument( '--top-n', type=int, default=0, metavar='N', help=('For classification problems, the output graph will contain the ' 'labels and scores for the top n classes, and results will be in the form of ' '""predicted, predicted_2, ..., probability, probability_2, ..."". ' 'If --top-n=0, then all labels and scores are returned in the form of ' '""predicted, class_name1, class_name2,..."".')) # HP parameters parser.add_argument( '--learning-rate', type=float, default=0.01, metavar='R', help='optimizer learning rate.') # Training input parameters parser.add_argument( '--max-steps', type=int, metavar='N', help='Maximum number of training steps to perform. If unspecified, will ' 'honor ""max-epochs"".') parser.add_argument( '--max-epochs', type=int, default=1000, metavar='N', help='Maximum number of training data epochs on which to train. If ' 'both ""max-steps"" and ""max-epochs"" are specified, the training ' 'job will run for ""max-steps"" or ""num-epochs"", whichever occurs ' 'first. If early stopping is enabled, training may also stop ' 'earlier.') parser.add_argument( '--train-batch-size', type=int, default=64, metavar='N', help='How many training examples are used per step. If num-epochs is ' 'used, the last batch may not be full.') parser.add_argument( '--eval-batch-size', type=int, default=64, metavar='N', help='Batch size during evaluation. Larger values increase performance ' 'but also increase peak memory usgae on the master node. One pass ' 'over the full eval set is performed per evaluation run.') parser.add_argument( '--min-eval-frequency', type=int, default=1000, metavar='N', help='Minimum number of training steps between evaluations. Evaluation ' 'does not occur if no new checkpoint is available, hence, this is ' 'the minimum. If 0, the evaluation will only happen after training. ') parser.add_argument( '--early-stopping-num_evals', type=int, default=3, help='Automatic training stop after results of specified number of evals ' 'in a row show the model performance does not improve. Set to 0 to ' 'disable early stopping.') parser.add_argument( '--logging-level', choices=['error', 'warning', 'info'], help='The TF logging level. If absent, use info for cloud training ' 'and warning for local training.') args, remaining_args = parser.parse_known_args(args=argv[1:]) # All HP parambeters must be unique, so we need to support an unknown number # of --hidden-layer-size1=10 --lhidden-layer-size2=10 ... # Look at remaining_args for hidden-layer-size\d+ to get the layer info. # Get number of layers pattern = re.compile('hidden-layer-size(\d+)') num_layers = 0 for other_arg in remaining_args: match = re.search(pattern, other_arg) if match: if int(match.group(1)) <= 0: raise ValueError('layer size must be a positive integer. Was given %s' % other_arg) num_layers = max(num_layers, int(match.group(1))) # Build a new parser so we catch unknown args and missing layer_sizes. parser = argparse.ArgumentParser() for i in range(num_layers): parser.add_argument('--hidden-layer-size%s' % str(i + 1), type=int, required=True) layer_args = vars(parser.parse_args(args=remaining_args)) hidden_layer_sizes = [] for i in range(num_layers): key = 'hidden_layer_size%s' % str(i + 1) hidden_layer_sizes.append(layer_args[key]) assert len(hidden_layer_sizes) == num_layers args.hidden_layer_sizes = hidden_layer_sizes return args",0
43,Python,sending binary data over a serial connection,https://github.com/nutechsoftware/alarmdecoder/blob/b0c014089e24455228cb4402cf30ba98157578cd/alarmdecoder/devices/serial_device.py#L149-L172,"def write(self, data): """""" Writes data to the device. :param data: data to write :type data: string :raises: py:class:`~alarmdecoder.util.CommError` """""" try: # Hack to support unicode under Python 2.x if isinstance(data, str) or (sys.version_info < (3,) and isinstance(data, unicode)): data = data.encode('utf-8') self._device.write(data) except serial.SerialTimeoutException: pass except serial.SerialException as err: raise CommError('Error writing to device.', err) else: self.on_write(data=data)",3
119,Python,sending binary data over a serial connection,https://github.com/FaradayRF/faradayio/blob/6cf3af88bb4a83e5d2036e5cbdfaf8f0f01500bb/faradayio/faraday.py#L32-L56,"def send(self, msg): """"""Encodes data to slip protocol and then sends over serial port Uses the SlipLib module to convert the message data into SLIP format. The message is then sent over the serial port opened with the instance of the Faraday class used when invoking send(). Args: msg (bytes): Bytes format message to send over serial port. Returns: int: Number of bytes transmitted over the serial port. """""" # Create a sliplib Driver slipDriver = sliplib.Driver() # Package data in slip format slipData = slipDriver.send(msg) # Send data over serial port res = self._serialPort.write(slipData) # Return number of bytes transmitted over serial port return res",3
653,Python,sending binary data over a serial connection,https://github.com/Seeed-Studio/wio-cli/blob/ce83f4c2d30be7f72d1a128acd123dfc5effa563/wio/commands/cmd_setup.py#L217-L384,"def serial_send(msvr, msvr_ip, xsvr, xsvr_ip, node_sn, node_key, port): ### check is configure mode? thread = termui.waiting_echo(""Getting device information..."") thread.daemon = True thread.start() flag = False try: with serial.Serial(port, 115200, timeout=5) as ser: cmd = 'Blank?\r\n' ser.write(cmd.encode('utf-8')) if 'Node' in ser.readline(): flag = True except serial.SerialException as e: thread.stop('') thread.join() click.secho('>> ', fg='red', nl=False) click.echo(e) if e.errno == 13: click.echo(""For more information, see https://github.com/Seeed-Studio/wio-cli#serial-port-permissions"") return None thread.stop('') thread.join() if flag: click.secho('> ', fg='green', nl=False) click.secho(""Found Wio."", fg='green', bold=True) click.echo() else: click.secho('> ', fg='green', nl=False) click.secho(""No nearby Wio detected."", fg='white', bold=True) if click.confirm(click.style('? ', fg='green') + click.style(""Would you like to wait and monitor for Wio entering configure mode"", bold=True), default=True): thread = termui.waiting_echo(""Waiting for a wild Wio to appear... (press ctrl + C to exit)"") thread.daemon = True thread.start() flag = False while 1: with serial.Serial(port, 115200, timeout=5) as ser: cmd = 'Blank?\r\n' ser.write(cmd.encode('utf-8')) if 'Node' in ser.readline(): flag = True break thread.stop('') thread.join() click.secho('> ', fg='green', nl=False) click.secho(""Found Wio."", fg='green', bold=True) click.echo() else: click.secho('> ', fg='green', nl=False) click.secho(""\nQuit wio setup!"", bg='white', bold=True) while 1: if not click.confirm(click.style('? ', fg='green') + click.style(""Would you like to manually enter your Wi-Fi network configuration?"", bold=True), default=False): thread = termui.waiting_echo(""Asking the Wio to scan for nearby Wi-Fi networks..."") thread.daemon = True thread.start() flag = False with serial.Serial(port, 115200, timeout=3) as ser: cmd = 'SCAN\r\n' ser.write(cmd.encode('utf-8')) ssid_list = [] while True: ssid = ser.readline() if ssid == '\r\n': flag = True break ssid = ssid.strip('\r\n') ssid_list.append(ssid) if flag: thread.stop('') thread.join() else: thread.stop(""\rsearch failure...\n"") return None while 1: for x in range(len(ssid_list)): click.echo(""%s.) %s"" %(x, ssid_list[x])) click.secho('? ', fg='green', nl=False) value = click.prompt( click.style('Please select the network to which your Wio should connect', bold=True), type=int) if value >= 0 and value < len(ssid_list): ssid = ssid_list[value] break else: click.echo(click.style('>> ', fg='red') + ""invalid input, range 0 to %s"" %(len(ssid_list)-1)) ap = ssid else: ap = click.prompt(click.style('> ', fg='green') + click.style('Please enter the SSID of your Wi-Fi network', bold=True), type=str) ap_pwd = click.prompt(click.style('> ', fg='green') + click.style('Please enter your Wi-Fi network password (leave blank for none)', bold=True), default='', show_default=False) d_name = click.prompt(click.style('> ', fg='green') + click.style('Please enter the name of a device will be created', bold=True), type=str) click.echo(click.style('> ', fg='green') + ""Here's what we're going to send to the Wio:"") click.echo() click.echo(click.style('> ', fg='green') + ""Wi-Fi network: "" + click.style(ap, fg='green', bold=True)) ap_pwd_p = ap_pwd if ap_pwd_p == '': ap_pwd_p = 'None' click.echo(click.style('> ', fg='green') + ""Password: "" + click.style(ap_pwd_p, fg='green', bold=True)) click.echo(click.style('> ', fg='green') + ""Device name: "" + click.style(d_name, fg='green', bold=True)) click.echo() if click.confirm(click.style('? ', fg='green') + ""Would you like to continue with the information shown above?"", default=True): break click.echo() #waiting ui thread = termui.waiting_echo(""Sending Wi-Fi information to device..."") thread.daemon = True thread.start() # send serial command ## get version version = 1.1 with serial.Serial(port, 115200, timeout=10) as ser: cmd = 'VERSION\r\n' ser.write(cmd.encode('utf-8')) res = ser.readline() try: version = float(re.match(r""([0-9]+.[0-9]+)"", res).group(0)) except Exception as e: version = 1.1 send_flag = False while 1: with serial.Serial(port, 115200, timeout=10) as ser: if version <= 1.1: cmd = ""APCFG: %s\t%s\t%s\t%s\t%s\t%s\t\r\n"" %(ap, ap_pwd, node_key, node_sn, xsvr_ip, msvr_ip) elif version >= 1.2: cmd = ""APCFG: %s\t%s\t%s\t%s\t%s\t%s\t\r\n"" %(ap, ap_pwd, node_key, node_sn, xsvr, msvr) else: cmd = ""APCFG: %s\t%s\t%s\t%s\t%s\t%s\t\r\n"" %(ap, ap_pwd, node_key, node_sn, xsvr, msvr) # click.echo(cmd) ser.write(cmd.encode('utf-8')) if ""ok"" in ser.readline(): click.echo(click.style('\r> ', fg='green') + ""Send Wi-Fi information to device success."") thread.stop('') thread.join() send_flag = True if send_flag: break if send_flag: return {'name': d_name} else: return None",3
90,Python,sending binary data over a serial connection,https://github.com/securestate/termineter/blob/d657d25d97c7739e650b951c396404e857e56625/lib/c1218/connection.py#L210-L222,"def read(self, size): """""" Read raw data from the serial connection. This function is not meant to be called directly. :param int size: The number of bytes to read from the serial connection. """""" data = self.serial_h.read(size) self.logger.debug('read data, length: ' + str(len(data)) + ' data: ' + binascii.b2a_hex(data).decode('utf-8')) self.serial_h.write(ACK) if sys.version_info[0] == 2: data = bytearray(data) return data",2
387,Python,sending binary data over a serial connection,https://github.com/securestate/termineter/blob/d657d25d97c7739e650b951c396404e857e56625/lib/termineter/core.py#L363-L374,"def serial_connect(self): """""" Connect to the serial device. """""" self.serial_get() try: self.serial_connection.start() except c1218.errors.C1218IOError as error: self.logger.error('serial connection has been opened but the meter is unresponsive') raise error self._serial_connected = True return True",2
752,Python,sending binary data over a serial connection,https://github.com/google/mobly/blob/38ba2cf7d29a20e6a2fca1718eecb337df38db26/mobly/controllers/android_device_lib/jsonrpc_shell_base.py#L44-L69,"def load_device(self, serial=None): """"""Creates an AndroidDevice for the given serial number. If no serial is given, it will read from the ANDROID_SERIAL environmental variable. If the environmental variable is not set, then it will read from 'adb devices' if there is only one. """""" serials = android_device.list_adb_devices() if not serials: raise Error('No adb device found!') # No serial provided, try to pick up the device automatically. if not serial: env_serial = os.environ.get('ANDROID_SERIAL', None) if env_serial is not None: serial = env_serial elif len(serials) == 1: serial = serials[0] else: raise Error( 'Expected one phone, but %d found. Use the -s flag or ' 'specify ANDROID_SERIAL.' % len(serials)) if serial not in serials: raise Error('Device ""%s"" is not found by adb.' % serial) ads = android_device.get_instances([serial]) assert len(ads) == 1 self._ad = ads[0]",2
804,Python,sending binary data over a serial connection,https://github.com/clach04/x10_any/blob/5b90a543b127ab9e6112fd547929b5ef4b8f0cbc/x10_any/cm17a.py#L106-L118,"def _sendBinaryData(port, data): """"""Send a string of binary data to the FireCracker with proper timing. See the diagram in the spec referenced above for timing information. The module level variables leadInOutDelay and bitDelay represent how long each type of delay should be in seconds. They may require tweaking on some setups. """""" _reset(port) time.sleep(leadInOutDelay) for digit in data: _sendBit(port, digit) time.sleep(leadInOutDelay)",2
64,Python,sending binary data over a serial connection,https://github.com/ransford/sllurp/blob/d744b7e17d7ba64a24d9a31bde6cba65d91ad9b1/sllurp/epc/sgtin_96.py#L27-L71,"def parse_sgtin_96(sgtin_96): '''Given a SGTIN-96 hex string, parse each segment. Returns a dictionary of the segments.''' if not sgtin_96: raise Exception('Pass in a value.') if not sgtin_96.startswith(""30""): # not a sgtin, not handled raise Exception('Not SGTIN-96.') binary = ""{0:020b}"".format(int(sgtin_96, 16)).zfill(96) header = int(binary[:8], 2) tag_filter = int(binary[8:11], 2) partition = binary[11:14] partition_value = int(partition, 2) m, l, n, k = SGTIN_96_PARTITION_MAP[partition_value] company_start = 8 + 3 + 3 company_end = company_start + m company_data = int(binary[company_start:company_end], 2) if company_data > pow(10, l): # can't be too large raise Exception('Company value is too large') company_prefix = str(company_data).zfill(l) item_start = company_end item_end = item_start + n item_data = binary[item_start:item_end] item_number = int(item_data, 2) item_reference = str(item_number).zfill(k) serial = int(binary[-38:], 2) return { ""header"": header, ""filter"": tag_filter, ""partition"": partition, ""company_prefix"": company_prefix, ""item_reference"": item_reference, ""serial"": serial }",1
463,Python,sending binary data over a serial connection,https://github.com/thomasdelaet/python-velbus/blob/af2f8af43f1a24bf854eff9f3126fd7b5c41b3dd/velbus/messages/relay_status.py#L120-L130,"def data_to_binary(self): """""" :return: bytes """""" return bytes([ COMMAND_CODE, self.channels_to_byte([self.channel]), self.disable_inhibit_forced, self.status, self.led_status ]) + struct.pack('>L', self.delay_time)[-3:]",1
887,Python,sending binary data over a serial connection,https://github.com/thomasdelaet/python-velbus/blob/af2f8af43f1a24bf854eff9f3126fd7b5c41b3dd/velbus/messages/write_module_address_and_serial_number.py#L47-L54,"def data_to_binary(self): """""" :return: bytes """""" return chr(COMMAND_CODE) + chr(self.module_type) + \ struct.pack('>L', self.current_serial)[2:] + \ chr(self.module_address) + \ struct.pack('>L', self.new_serial)[2:]",1
49,Python,scatter plot,https://github.com/flowersteam/explauto/blob/cf0f81ecb9f6412f7276a95bd27359000e1e26b6/explauto/experiment/log.py#L67-L93,"def scatter_plot(self, ax, topic_dims, t=None, ms_limits=True, **kwargs_plot): """""" 2D or 3D scatter plot. :param axes ax: matplotlib axes (use Axes3D if 3D data) :param tuple topic_dims: list of (topic, dims) tuples, where topic is a string and dims is a list of dimensions to be plotted for that topic. :param int t: time indexes to be plotted :param dict kwargs_plot: argument to be passed to matplotlib's plot function, e.g. the style of the plotted points 'or' :param bool ms_limits: if set to True, automatically set axes boundaries to the sensorimotor boundaries (default: True) """""" plot_specs = {'marker': 'o', 'linestyle': 'None'} plot_specs.update(kwargs_plot) # t_bound = float('inf') # if t is None: # for topic, _ in topic_dims: # t_bound = min(t_bound, self.counts[topic]) # t = range(t_bound) # data = self.pack(topic_dims, t) data = self.data_t(topic_dims, t) ax.plot(*(data.T), **plot_specs) if ms_limits: ax.axis(self.axes_limits(topic_dims))",3
170,Python,scatter plot,https://github.com/zkbt/the-friendly-stars/blob/50d3f979e79e63c66629065c75595696dc79802e/thefriendlystars/constellations/constellation.py#L281-L314,"def plot(self, sizescale=10, color=None, alpha=0.5, label=None, edgecolor='none', **kw): ''' Plot the ra and dec of the coordinates, at a given epoch, scaled by their magnitude. (This does *not* create a new empty figure.) Parameters ---------- sizescale : (optional) float The marker size for scatter for a star at the magnitudelimit. color : (optional) any valid color The color to plot (but there is a default for this catalog.) **kw : dict Additional keywords will be passed on to plt.scatter. Returns ------- plotted : outputs from the plots ''' # calculate the sizes of the stars (logarithmic with brightness?) size = np.maximum(sizescale*(1 + self.magnitudelimit - self.magnitude), 1) # make a scatter plot of the RA + Dec scatter = plt.scatter(self.ra, self.dec, s=size, color=color or self.color, label=label or '{} ({:.1f})'.format(self.name, self.epoch), alpha=alpha, edgecolor=edgecolor, **kw) return scatter",3
665,Python,scatter plot,https://github.com/newville/wxmplot/blob/8e0dc037453e5cdf18c968dc5a3d29efd761edee/wxmplot/plotframe.py#L45-L47,"def scatterplot(self, x, y, **kw): """"""plot after clearing current plot """""" self.panel.scatterplot(x, y, **kw)",3
915,Python,scatter plot,https://github.com/bloomberg/bqplot/blob/8eb8b163abe9ee6306f6918067e2f36c1caef2ef/bqplot/pyplot.py#L816-L837,"def scatter(x, y, **kwargs): """"""Draw a scatter in the current context figure. Parameters ---------- x: numpy.ndarray, 1d The x-coordinates of the data points. y: numpy.ndarray, 1d The y-coordinates of the data points. options: dict (default: {}) Options for the scales to be created. If a scale labeled 'x' is required for that mark, options['x'] contains optional keyword arguments for the constructor of the corresponding scale type. axes_options: dict (default: {}) Options for the axes to be created. If an axis labeled 'x' is required for that mark, axes_options['x'] contains optional keyword arguments for the constructor of the corresponding axis type. """""" kwargs['x'] = x kwargs['y'] = y return _draw_mark(Scatter, **kwargs)",3
1118,Python,scatter plot,https://github.com/totalgood/nlpia/blob/efa01126275e9cd3c3a5151a644f1c798a9ec53f/src/nlpia/book/examples/ch05_sms_spam_linear_regression.py#L161-L173,"def sentiment_scatter(sms=sms): plt.figure(figsize=(10, 7.5)) ax = plt.subplot(1, 1, 1) ax = sms.plot.scatter(x='topic4', y='line', ax=ax, color='g', marker='+', alpha=.6) ax = sms.plot.scatter(x='topic4', y='sgd', ax=ax, color='r', marker='x', alpha=.4) ax = sms.plot.scatter(x='topic4', y='vader', ax=ax, color='k', marker='.', alpha=.3) ax = sms.plot.scatter(x='topic4', y='sgd', ax=ax, color='c', marker='s', alpha=.6) ax = sms.plot.scatter(x='topic4', y='pca_lda_spaminess', ax=ax, color='b', marker='o', alpha=.6) plt.ylabel('Sentiment') plt.xlabel('Topic 4') plt.legend(['LinearRegressor', 'SGDRegressor', 'Vader', 'OneNeuronRegresor', 'PCA->LDA->spaminess']) plt.tight_layout() plt.show()",3
1229,Python,scatter plot,https://github.com/maartenbreddels/ipyvolume/blob/e68b72852b61276f8e6793bc8811f5b2432a155f/ipyvolume/widgets.py#L434-L441,"def scatter(x, y, z, color=(1, 0, 0), s=0.01): global _last_figure fig = _last_figure if fig is None: fig = volshow(None) fig.scatter = Scatter(x=x, y=y, z=z, color=color, size=s) fig.volume.scatter = fig.scatter return fig",3
1360,Python,scatter plot,https://github.com/SheffieldML/GPy/blob/54c32d79d289d622fb18b898aee65a2a431d90cf/GPy/plotting/matplot_dep/plot_definitions.py#L99-L102,"def scatter(self, ax, X, Y, Z=None, color=Tango.colorsHex['mediumBlue'], label=None, marker='o', **kwargs): if Z is not None: return ax.scatter(X, Y, c=color, zs=Z, label=label, marker=marker, **kwargs) return ax.scatter(X, Y, c=color, label=label, marker=marker, **kwargs)",3
1367,Python,scatter plot,https://github.com/theislab/scanpy/blob/9e4e5ee02e04cf618872d9b098e24f0542e8b227/scanpy/plotting/_anndata.py#L30-L179,"def scatter( adata, x=None, y=None, color=None, use_raw=None, layers='X', sort_order=True, alpha=None, basis=None, groups=None, components=None, projection='2d', legend_loc='right margin', legend_fontsize=None, legend_fontweight=None, color_map=None, palette=None, frameon=None, right_margin=None, left_margin=None, size=None, title=None, show=None, save=None, ax=None): """"""\ Scatter plot along observations or variables axes. Color the plot using annotations of observations (`.obs`), variables (`.var`) or expression of genes (`.var_names`). Parameters ---------- adata : :class:`~anndata.AnnData` Annotated data matrix. x : `str` or `None` x coordinate. y : `str` or `None` y coordinate. color : string or list of strings, optional (default: `None`) Keys for annotations of observations/cells or variables/genes, e.g., `'ann1'` or `['ann1', 'ann2']`. use_raw : `bool`, optional (default: `None`) Use `raw` attribute of `adata` if present. layers : `str` or tuple of strings, optional (default: `X`) Use the `layers` attribute of `adata` if present: specify the layer for `x`, `y` and `color`. If `layers` is a string, then it is expanded to `(layers, layers, layers)`. basis : {{'pca', 'tsne', 'umap', 'diffmap', 'draw_graph_fr', etc.}} String that denotes a plotting tool that computed coordinates. {scatter_temp} {show_save_ax} Returns ------- If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it. """""" if basis is not None: axs = _scatter_obs( adata=adata, x=x, y=y, color=color, use_raw=use_raw, layers=layers, sort_order=sort_order, alpha=alpha, basis=basis, groups=groups, components=components, projection=projection, legend_loc=legend_loc, legend_fontsize=legend_fontsize, legend_fontweight=legend_fontweight, color_map=color_map, palette=palette, frameon=frameon, right_margin=right_margin, left_margin=left_margin, size=size, title=title, show=show, save=save, ax=ax) elif x is not None and y is not None: if ((x in adata.obs.keys() or x in adata.var.index) and (y in adata.obs.keys() or y in adata.var.index) and (color is None or color in adata.obs.keys() or color in adata.var.index)): axs = _scatter_obs( adata=adata, x=x, y=y, color=color, use_raw=use_raw, layers=layers, sort_order=sort_order, alpha=alpha, basis=basis, groups=groups, components=components, projection=projection, legend_loc=legend_loc, legend_fontsize=legend_fontsize, legend_fontweight=legend_fontweight, color_map=color_map, palette=palette, frameon=frameon, right_margin=right_margin, left_margin=left_margin, size=size, title=title, show=show, save=save, ax=ax) elif ((x in adata.var.keys() or x in adata.obs.index) and (y in adata.var.keys() or y in adata.obs.index) and (color is None or color in adata.var.keys() or color in adata.obs.index)): axs = _scatter_var( adata=adata, x=x, y=y, color=color, use_raw=use_raw, layers=layers, sort_order=sort_order, alpha=alpha, basis=basis, groups=groups, components=components, projection=projection, legend_loc=legend_loc, legend_fontsize=legend_fontsize, legend_fontweight=legend_fontweight, color_map=color_map, palette=palette, frameon=frameon, right_margin=right_margin, left_margin=left_margin, size=size, title=title, show=show, save=save, ax=ax) else: raise ValueError( '`x`, `y`, and potential `color` inputs must all come from either `.obs` or `.var`') else: raise ValueError('Either provide a `basis` or `x` and `y`.') return axs",3
1232,Python,scatter plot,https://github.com/scivision/lowtran/blob/9954d859e53437436103f9ab54a7e2602ecaa1b7/lowtran/plots.py#L12-L49,"def plotscatter(irrad: xarray.Dataset, c1: Dict[str, Any], log: bool = False): fg = figure() axs = fg.subplots(2, 1, sharex=True) transtxt = 'Transmittance' ax = axs[0] ax.plot(irrad.wavelength_nm, irrad['transmission'].squeeze()) ax.set_title(transtxt) ax.set_ylabel('Transmission (unitless)') ax.grid(True) ax.legend(irrad.angle_deg.values) ax = axs[1] if plotNp: Np = (irrad['pathscatter']*10000) * (irrad.wavelength_nm*1e9)/(h*c) ax.plot(irrad.wavelength_nm, Np) ax.set_ylabel('Photons [s$^{-1}$ '+UNITS) else: ax.plot(irrad.wavelength_nm, irrad['pathscatter'].squeeze()) ax.set_ylabel('Radiance [W '+UNITS) ax.set_xlabel('wavelength [nm]') ax.set_title('Single-scatter Path Radiance') ax.invert_xaxis() ax.autoscale(True, axis='x', tight=True) ax.grid(True) if log: ax.set_yscale('log') # ax.set_ylim(1e-8,1) try: fg.suptitle(f'Obs. to Space: zenith angle: {c1[""angle""]} deg., ') # {datetime.utcfromtimestamp(irrad.time.item()/1e9)} except (AttributeError, TypeError): pass",2
335,Python,scatter plot,https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/structural/cnvkit.py#L633-L646,"def _add_plots_to_output(out, data): """"""Add CNVkit plots summarizing called copy number values. """""" out[""plot""] = {} diagram_plot = _add_diagram_plot(out, data) if diagram_plot: out[""plot""][""diagram""] = diagram_plot scatter = _add_scatter_plot(out, data) if scatter: out[""plot""][""scatter""] = scatter scatter_global = _add_global_scatter_plot(out, data) if scatter_global: out[""plot""][""scatter_global""] = scatter_global return out",1
2050,Python,save list to file,https://github.com/spacetelescope/stsci.tools/blob/9a022503ad24ca54ce83331482dfa3ff6de9f403/lib/stsci/tools/cfgpars.py#L752-L790,"def saveParList(self, *args, **kw): """"""Write parameter data to filename (string or filehandle)"""""" if 'filename' in kw: filename = kw['filename'] if not filename: filename = self.getFilename() if not filename: raise ValueError(""No filename specified to save parameters"") if hasattr(filename,'write'): fh = filename absFileName = os.path.abspath(fh.name) else: absFileName = os.path.expanduser(filename) absDir = os.path.dirname(absFileName) if len(absDir) and not os.path.isdir(absDir): os.makedirs(absDir) fh = open(absFileName,'w') numpars = len(self.__paramList) if self._forUseWithEpar: numpars -= 1 if not self.final_comment: self.final_comment = [''] # force \n at EOF # Empty the ConfigObj version of section.defaults since that is based # on an assumption incorrect for us, and override with our own list. # THIS IS A BIT OF MONKEY-PATCHING! WATCH FUTURE VERSION CHANGES! # See Trac ticket #762. while len(self.defaults): self.defaults.pop(-1) # empty it, keeping ref for key in self._neverWrite: self.defaults.append(key) # Note also that we are only overwriting the top/main section's # ""defaults"" list, but EVERY [sub-]section has such an attribute... # Now write to file, delegating work to ConfigObj (note that ConfigObj # write() skips any items listed by name in the self.defaults list) self.write(fh) fh.close() retval = str(numpars) + "" parameters written to "" + absFileName self.filename = absFileName # reset our own ConfigObj filename attr self.debug('Keys not written: '+str(self.defaults)) return retval",3
155,Python,save list to file,https://github.com/alejandroautalan/pygubu/blob/41c8fb37ef973736ec5d68cbe1cd4ecb78712e40/pygubudesigner/main.py#L491-L495,"def do_save(self, fname): self.save_file(fname) self.currentfile = fname self.is_changed = False logger.info(_('Project saved to {0}').format(fname))",2
588,Python,save list to file,https://github.com/ioam/lancet/blob/1fbbf88fa0e8974ff9ed462e3cb11722ddebdd6e/lancet/filetypes.py#L264-L267,"def save(self, filename, metadata={}, **data): super(NumpyFile, self).save(filename, metadata, **data) savefn = numpy.savez_compressed if self.compress else numpy.savez savefn(self._savepath(filename), metadata=metadata, **data)",2
775,Python,save list to file,https://github.com/StanfordBioinformatics/loom/blob/db2031a1a87124fee1aeb7414a668c03d774a698/worker/loomengine_worker/outputs.py#L71-L83,"def save(self): filename_list = self.output['source']['filenames'] if not isinstance(filename_list, list): filename_list = filename_list.split(' ') contents_list = [] for filename in filename_list: file_path = os.path.join( self.working_dir, filename) contents_list.append(self._read_file(file_path)) self.output.update({'data': {'contents': contents_list}}) self.connection.update_task_attempt_output( self.output['uuid'], self.output)",2
263,Python,save list to file,https://github.com/StanfordBioinformatics/loom/blob/db2031a1a87124fee1aeb7414a668c03d774a698/worker/loomengine_worker/outputs.py#L28-L35,"def save(self): filename_list = self.output['source']['filenames'] file_path_list = [ os.path.join( self.working_dir, filename) for filename in filename_list] self.import_manager.import_result_file_list( self.output, file_path_list, retry=True)",1
356,Python,save list to file,https://github.com/coderholic/pyradio/blob/c5219d350bccbccd49dbd627c1f886a952ea1963/pyradio/config.py#L258-L306,"def save_playlist_file(self, stationFile=''): """""" Save a playlist Create a txt file and write stations in it. Then rename it to final target return 0: All ok -1: Error writing file -2: Error renaming file """""" if self._playlist_format_changed(): self.dirty_playlist = True self.new_format = not self.new_format if stationFile: st_file = stationFile else: st_file = self.stations_file if not self.dirty_playlist: if logger.isEnabledFor(logging.DEBUG): logger.debug('Playlist not modified...') return 0 st_new_file = st_file.replace('.csv', '.txt') tmp_stations = self.stations[:] tmp_stations.reverse() if self.new_format: tmp_stations.append([ '# Find lots more stations at http://www.iheart.com' , '', '' ]) else: tmp_stations.append([ '# Find lots more stations at http://www.iheart.com' , '' ]) tmp_stations.reverse() try: with open(st_new_file, 'w') as cfgfile: writter = csv.writer(cfgfile) for a_station in tmp_stations: writter.writerow(self._format_playlist_row(a_station)) except: if logger.isEnabledFor(logging.DEBUG): logger.debug('Cannot open playlist file for writing,,,') return -1 try: move(st_new_file, st_file) except: if logger.isEnabledFor(logging.DEBUG): logger.debug('Cannot rename playlist file...') return -2 self.dirty_playlist = False return 0",1
621,Python,save list to file,https://github.com/pndurette/gTTS/blob/b01ac4eb22d40c6241202e202d0418ccf4f98460/gtts/tts.py#L238-L250,"def save(self, savefile): """"""Do the TTS API request and write result to file. Args: savefile (string): The path and file name to save the ``mp3`` to. Raises: :class:`gTTSError`: When there's an error with the API request. """""" with open(str(savefile), 'wb') as f: self.write_to_fp(f) log.debug(""Saved to %s"", savefile)",1
1259,Python,save list to file,https://github.com/tzutalin/labelImg/blob/6afd15aa88f89f41254e0004ed219b3965eb2c0d/labelImg.py#L1295-L1297,"def saveFileAs(self, _value=False): assert not self.image.isNull(), ""cannot save empty image"" self._saveFile(self.saveFileDialog())",1
1394,Python,save list to file,https://github.com/QualiSystems/vCenterShell/blob/e2e24cd938a92a68f4a8e6a860810d3ef72aae6d/package/cloudshell/cp/vcenter/commands/save_sandbox.py#L38-L91,"def save_app(self, si, logger, vcenter_data_model, reservation_id, save_app_actions, cancellation_context): """""" Cretaes an artifact of an app, that can later be restored :param vcenter_data_model: VMwarevCenterResourceModel :param vim.ServiceInstance si: py_vmomi service instance :type si: vim.ServiceInstance :param logger: Logger :type logger: cloudshell.core.logger.qs_logger.get_qs_logger :param list[SaveApp] save_app_actions: :param cancellation_context: """""" results = [] logger.info('Save Sandbox command starting on ' + vcenter_data_model.default_datacenter) if not save_app_actions: raise Exception('Failed to save app, missing data in request.') actions_grouped_by_save_types = groupby(save_app_actions, lambda x: x.actionParams.saveDeploymentModel) # artifactSaver or artifactHandler are different ways to save artifacts. For example, currently # we clone a vm, thenk take a snapshot. restore will be to deploy from linked snapshot # a future artifact handler we might develop is save vm to OVF file and restore from file. artifactSaversToActions = {ArtifactHandler.factory(k, self.pyvmomi_service, vcenter_data_model, si, logger, self.deployer, reservation_id, self.resource_model_parser, self.snapshot_saver, self.task_waiter, self.folder_manager, self.port_group_configurer, self.cs) : list(g) for k, g in actions_grouped_by_save_types} self.validate_requested_save_types_supported(artifactSaversToActions, logger, results) error_results = [r for r in results if not r.success] if not error_results: logger.info('Handling Save App requests') results = self._execute_save_actions_using_pool(artifactSaversToActions, cancellation_context, logger, results) logger.info('Completed Save Sandbox command') else: logger.error('Some save app requests were not valid, Save Sandbox command failed.') return results",1
1574,Python,save list to file,https://github.com/acutesoftware/AIKIF/blob/fcf1582dc5f884b9a4fa7c6e20e9de9d94d21d03/aikif/lib/cls_filelist.py#L192-L228,"def save_filelist(self, opFile, opFormat, delim=',', qu='""'): """""" uses a List of files and collects meta data on them and saves to an text file as a list or with metadata depending on opFormat. """""" op_folder = os.path.dirname(opFile) if op_folder is not None: # short filename passed if not os.path.exists(op_folder): os.makedirs(op_folder) with open(opFile,'w') as fout: fout.write(""fullFilename"" + delim) for colHeading in opFormat: fout.write(colHeading + delim) fout.write('\n') for f in self.filelist: line = qu + f + qu + delim try: for fld in opFormat: if fld == ""name"": line = line + qu + os.path.basename(f) + qu + delim if fld == ""date"": line = line + qu + self.GetDateAsString(f) + qu + delim if fld == ""size"": line = line + qu + str(os.path.getsize(f)) + qu + delim if fld == ""path"": line = line + qu + os.path.dirname(f) + qu + delim except IOError: line += '\n' # no metadata try: fout.write (str(line.encode('ascii', 'ignore').decode('utf-8'))) fout.write ('\n') except IOError: #print(""Cant print line - cls_filelist line 304"") pass",1
416,Python,replace in file,https://github.com/HeliumEdu/heliumcli/blob/f6c6cc0ed3c335f956afa77eab0228ae23bb5d47/heliumcli/actions/init.py#L86-L97,"def _replace_in_file(self, dir_name, filename, args): path = os.path.join(dir_name, filename) with open(path, ""r"") as f: s = f.read() s = s.replace(""{%PROJECT_ID%}"", args.id) s = s.replace(""{%PROJECT_ID_UPPER%}"", self._upper_slug) s = s.replace(""{%PROJECT_ID_LOWER%}"", self._lower_slug) s = s.replace(""{%PROJECT_NAME%}"", args.name) s = s.replace(""{%PROJECT_GITHUB_USER%}"", args.github_user) with open(path, ""w"") as f: f.write(s)",3
557,Python,replace in file,https://github.com/chaoss/grimoirelab-manuscripts/blob/94a3ad4f11bfbcd6c5190e01cb5d3e47a5187cd9/manuscripts/report.py#L812-L824,"def replace_text(filepath, to_replace, replacement): """""" Replaces a string in a given file with another string :param file: the file in which the string has to be replaced :param to_replace: the string to be replaced in the file :param replacement: the string which replaces 'to_replace' in the file """""" with open(filepath) as file: s = file.read() s = s.replace(to_replace, replacement) with open(filepath, 'w') as file: file.write(s)",3
677,Python,replace in file,https://github.com/riptano/ccm/blob/275699f79d102b5039b79cc17fa6305dccf18412/ccmlib/common.py#L208-L219,"def replaces_in_file(file, replacement_list): rs = [(re.compile(regexp), repl) for (regexp, repl) in replacement_list] file_tmp = file + ""."" + str(os.getpid()) + "".tmp"" with open(file, 'r') as f: with open(file_tmp, 'w') as f_tmp: for line in f: for r, replace in rs: match = r.search(line) if match: line = replace + ""\n"" f_tmp.write(line) shutil.move(file_tmp, file)",3
267,Python,replace in file,https://github.com/NoviceLive/intellicoder/blob/6cac5ebfce65c370dbebe47756a1789b120ef982/intellicoder/transformers.py#L59-L66,"def replace_source(self, body, name): logging.debug(_('Processing function body: %s'), name) replaced = re.sub( self.FUNC_NAME_RE, self._func_replacer, body) replaced = re.sub( self.STR_LITERAL_RE, self._string_replacer, replaced) return self._build_strings() + replaced return replaced",2
309,Python,replace in file,https://github.com/jmbeach/KEP.py/blob/68cda64ab649640a486534867c81274c41e39446/src/keppy/tag.py#L31-L33,"def name_replace(self, to_replace, replacement): """"""Replaces part of tag name with new value"""""" self.name = self.name.replace(to_replace, replacement)",2
1410,Python,replace in file,https://github.com/TheRobotCarlson/DocxMerge/blob/df8d2add5ff4f33e31d36a6bd3d4495140334538/DocxMerge/DocxMerge.py#L274-L293,"def replace_doc_text(file, replacements): document = Document(file) paragraphs = document.paragraphs changes = False for paragraph in paragraphs: text = paragraph.text for original, replace in replacements.items(): if original in replace and replace in text: continue if original in text: changes = True text = text.replace(original, replace) paragraph.text = text if changes: print(""changing {}"".format(file)) document.save(file)",2
169,Python,replace in file,https://github.com/char16t/wa/blob/ee28bf47665ea57f3a03a08dfc0a5daaa33d8121/wa/api.py#L152-L174,"def replace_f(self, path, arg_name=None): """"""Replace files"""""" root, file = os.path.split(path) pattern = re.compile(r'(\<\<\<)([A-Za-z_]+)(\>\>\>)') file_path = path fh, abs_path = mkstemp() with open(abs_path, 'w') as new_file: with open(file_path) as old_file: for line in old_file: for (o, var_name, c) in re.findall(pattern, line): line = self.handle_args(line, var_name, arg_name) new_file.write(line) os.close(fh) # Remove original file os.remove(file_path) # Move new file shutil.move(abs_path, file_path) pattern = re.compile(r'(\[\[)([A-Za-z_]+)(\]\])') for (o, var_name, c) in re.findall(pattern, file): file = self.handle_args(file, var_name, isfilename=True) os.rename(path, os.path.join(root, file))",1
239,Python,replace in file,https://github.com/Deathnerd/pyterp/blob/baf2957263685f03873f368226f5752da4e51f08/pyterp/__init__.py#L53-L65,"def _load_file(self, filename): try: # Try to load the file read_file = os.path.realpath(os.path.join(os.curdir, filename)) with open(read_file, 'rb') as file: temp_program = """" for line in file: temp_program += line.strip().replace("" "", """").replace(""\n"", """").replace(""\r"", """").replace(""\t"", """") except IOError: # Catch the file load error and exit gracefully print ""Cannot open file {}"".format(filename) sys.exit(1) return self._parse_program(temp_program)",1
1048,Python,replace in file,https://github.com/JesseAldridge/clipmon/blob/5e16a31cf38f64db2d9b7a490968f922aca57512/clipmon.py#L19-L38,"def test_replacements(clip_str, path_exists): replaced_str = clip_str for find_regex, replace_str in conf.find_replace_map: replaced_str = re.sub(find_regex, replace_str, replaced_str) match = re.search( # file extension # path | # | | r'((?:~|/)[^@^:^\\^\(]+\.[a-z]{2,3}).*(?:line.*?|\()([0-9]+)', replaced_str) if match and path_exists(os.path.expanduser(match.group(1))): return ':'.join([match.group(1), match.group(2)]) match = re.search( # file extension # path | # | | r'((?:~|/)[^@^:^\\^\(]+\.[a-z]{2,3}):([0-9]+)', replaced_str) if match and path_exists(os.path.expanduser(match.group(1))): return ':'.join([match.group(1), match.group(2)])",1
1194,Python,replace in file,https://github.com/pantsbuild/pants/blob/b72e650da0df685824ffdcc71988b8c282d0962d/src/python/pants/engine/native.py#L183-L194,"def _replace_file(path, content): """"""Writes a file if it doesn't already exist with the same content. This is useful because cargo uses timestamps to decide whether to compile things."""""" if os.path.exists(path): with open(path, 'r') as f: if content == f.read(): print(""Not overwriting {} because it is unchanged"".format(path), file=sys.stderr) return with open(path, 'w') as f: f.write(content)",0
186,Python,regex case insensitive,https://github.com/wolfhong/formic/blob/0d81eb88dcbb6fa705194fc6ccf2993f4abbaa76/formic/formic.py#L182-L187,"def match(self, string): """"""Returns True if the argument matches the constant."""""" if self.casesensitive: return self.pattern == os.path.normcase(string) else: return self.pattern.lower() == os.path.normcase(string).lower()",3
362,Python,regex case insensitive,https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/utils/stringmatching.py#L18-L47,"def get_search_regex(query, ignore_case=True): """"""Returns a compiled regex pattern to search for query letters in order. Parameters ---------- query : str String to search in another string (in order of character occurrence). ignore_case : True Optional value perform a case insensitive search (True by default). Returns ------- pattern : SRE_Pattern Notes ----- This function adds '.*' between the query characters and compiles the resulting regular expression. """""" regex_text = [char for char in query if char != ' '] regex_text = '.*'.join(regex_text) regex = r'({0})'.format(regex_text) if ignore_case: pattern = re.compile(regex, re.IGNORECASE) else: pattern = re.compile(regex) return pattern",3
1092,Python,regex case insensitive,https://github.com/petermelias/valhalla/blob/b08d7a4a94fd8d85a6f5ea86200ec60ef4525e3d/valhalla/filters/strings.py#L90-L99,"def regex(regex, case=False, _value=None, *args, **kwargs): if kwargs.get('case'): regex = re.compile(regex) else: regex = re.compile(regex, re.IGNORECASE) if not regex.match(_value): raise ValidationError('The _value must match the regex %s' % regex) return _value",3
1112,Python,regex case insensitive,https://github.com/cltk/cltk/blob/ed9c025b7ec43c949481173251b70e05e4dffd27/cltk/ir/query.py#L24-L37,"def _regex_span(_regex, _str, case_insensitive=True): """"""Return all matches in an input string. :rtype : regex.match.span :param _regex: A regular expression pattern. :param _str: Text on which to run the pattern. """""" if case_insensitive: flags = regex.IGNORECASE | regex.FULLCASE | regex.VERSION1 else: flags = regex.VERSION1 comp = regex.compile(_regex, flags=flags) matches = comp.finditer(_str) for match in matches: yield match",3
547,Python,regex case insensitive,https://github.com/DBuildService/dockerfile-parse/blob/3d7b514d8b8eded1b33529cf0f6a0770a573aee0/dockerfile_parse/parser.py#L708-L722,"def image_from(from_value): """""" :param from_value: string like ""image:tag"" or ""image:tag AS name"" :return: tuple of the image and stage name, e.g. (""image:tag"", None) """""" regex = re.compile(r""""""(?xi) # readable, case-insensitive regex \s* # ignore leading whitespace (?P<image> \S+ ) # image and optional tag (?: # optional ""AS name"" clause for stage \s+ AS \s+ (?P<name> \S+ ) )? """""") match = re.match(regex, from_value) return match.group('image', 'name') if match else (None, None)",2
923,Python,regex case insensitive,https://github.com/openstates/billy/blob/5fc795347f12a949e410a8cfad0c911ea6bced67/billy/web/api/handlers.py#L27-L48,"def _build_mongo_filter(request, keys, icase=True): _filter = {} keys = set(keys) - set(['fields']) for key in keys: value = request.GET.get(key) if value: if key in _lower_fields: _filter[key] = value.lower() elif key.endswith('__in'): values = value.split('|') _filter[key[:-4]] = values elif key == 'bill_id': _filter[key] = fix_bill_id(value.upper()) else: # We use regex queries to get case insensitive search - this # means they won't use any indexes for now. Real case # insensitive queries are coming eventually: # http://jira.mongodb.org/browse/SERVER-90 _filter[key] = re.compile('^%s$' % value, re.IGNORECASE) return _filter",2
458,Python,regex case insensitive,https://github.com/lotrekagency/djlotrek/blob/10a304103768c3d59bdb8859eba86ef3327c9598/djlotrek/templatetags/djlotrek_filters.py#L50-L53,"def regex_match(value, regex): pattern = re.compile(regex) if pattern.match(value): return True",1
855,Python,regex case insensitive,https://github.com/refnode/liquid/blob/8b2b5efc635b0dbfe610db9036fdb4ae3e3d5439/src/liquid/strscan.py#L516-L535,"def get_regex(regex): """""" Ensure we have a compiled regular expression object. >>> import re >>> get_regex('string') # doctest: +ELLIPSIS <_sre.SRE_Pattern object at 0x...> >>> pattern = re.compile(r'string') >>> get_regex(pattern) is pattern True >>> get_regex(3) # doctest: +ELLIPSIS Traceback (most recent call last): ... TypeError: Invalid regex type: 3 """""" if isinstance(regex, basestring): return re.compile(regex) elif not isinstance(regex, re._pattern_type): raise TypeError(""Invalid regex type: %r"" % (regex,)) return regex",1
1440,Python,regex case insensitive,https://github.com/nefarioustim/parker/blob/ccc1de1ac6bfb5e0a8cfa4fdebb2f38f2ee027d6/parker/parsedpage.py#L65-L75,"def _filter_by_regex(self, regex, text, group=1): """"""Filter @text by @regex."""""" match = re.search( regex, text, re.MULTILINE ) return match.group(group).strip() if ( match and match.groups() ) else text",1
993,Python,regex case insensitive,https://github.com/goshuirc/irc/blob/d6a5e3e04d337566c009b087f108cd76f9e122cc/girc/imapping.py#L194-L197,def lower(self): new_string = IString(self._irc_lower(self)) new_string.set_std(self._std) return new_string,0
476,Python,readonly array,https://github.com/nyaruka/smartmin/blob/488a676a4960555e4d216a7b95d6e01a4ad4efd8/smartmin/views.py#L914-L924,"def derive_readonly(self): """""" Figures out what fields should be readonly. We iterate our field_config to find all that have a readonly of true """""" readonly = list(self.readonly) for key, value in self.field_config.items(): if 'readonly' in value and value['readonly']: readonly.append(key) return readonly",3
1256,Python,readonly array,https://github.com/mattja/sdeint/blob/7cf807cdf97b3bb39d29e1c2dc834b519499b601/sdeint/_broadcast.py#L70-L108,"def broadcast_to(array, shape, subok=False): """"""Broadcast an array to a new shape. Parameters ---------- array : array_like The array to broadcast. shape : tuple The shape of the desired array. subok : bool, optional If True, then sub-classes will be passed-through, otherwise the returned array will be forced to be a base-class array (default). Returns ------- broadcast : array A readonly view on the original array with the given shape. It is typically not contiguous. Furthermore, more than one element of a broadcasted array may refer to a single memory location. Raises ------ ValueError If the array is not compatible with the new shape according to NumPy's broadcasting rules. Notes ----- .. versionadded:: 1.10.0 Examples -------- >>> x = np.array([1, 2, 3]) >>> np.broadcast_to(x, (3, 3)) array([[1, 2, 3], [1, 2, 3], [1, 2, 3]]) """""" return _broadcast_to(array, shape, subok=subok, readonly=True)",3
749,Python,readonly array,https://github.com/vaexio/vaex/blob/a45b672f8287afca2ada8e36b74b604b9b28dd85/packages/vaex-core/vaex/dataframe.py#L4548-L4555,"def _readonly(self, inplace=False): # make arrays read only if possib;e df = self if inplace else self.copy() for key, ar in self.columns.items(): if isinstance(ar, np.ndarray): df.columns[key] = ar = ar.view() # make new object so we don't modify others ar.flags['WRITEABLE'] = False return df",2
1583,Python,readonly array,https://github.com/mattja/sdeint/blob/7cf807cdf97b3bb39d29e1c2dc834b519499b601/sdeint/_broadcast.py#L53-L67,"def _broadcast_to(array, shape, subok, readonly): shape = tuple(shape) if np.iterable(shape) else (shape,) array = np.array(array, copy=False, subok=subok) if not shape and array.shape: raise ValueError('cannot broadcast a non-scalar to a scalar array') if any(size < 0 for size in shape): raise ValueError('all elements of broadcast shape must be non-' 'negative') broadcast = np.nditer( (array,), flags=['multi_index', 'refs_ok', 'zerosize_ok'], op_flags=['readonly'], itershape=shape, order='C').itviews[0] result = _maybe_view_as_subclass(array, broadcast) if not readonly and array.flags.writeable: result.flags.writeable = True return result",2
1569,Python,readonly array,https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/io/sas/sas_xport.py#L427-L465,"def read(self, nrows=None): if nrows is None: nrows = self.nobs read_lines = min(nrows, self.nobs - self._lines_read) read_len = read_lines * self.record_length if read_len <= 0: self.close() raise StopIteration raw = self.filepath_or_buffer.read(read_len) data = np.frombuffer(raw, dtype=self._dtype, count=read_lines) df = pd.DataFrame(index=range(read_lines)) for j, x in enumerate(self.columns): vec = data['s%d' % j] ntype = self.fields[j]['ntype'] if ntype == ""numeric"": vec = _handle_truncated_float_vec( vec, self.fields[j]['field_length']) miss = self._missing_double(vec) v = _parse_float_vec(vec) v[miss] = np.nan elif self.fields[j]['ntype'] == 'char': v = [y.rstrip() for y in vec] if self._encoding is not None: v = [y.decode(self._encoding) for y in v] df[x] = v if self._index is None: df.index = range(self._lines_read, self._lines_read + read_lines) else: df = df.set_index(self._index) self._lines_read += read_lines return df",1
226,Python,readonly array,https://github.com/nerdvegas/rez/blob/1d3b846d53b5b5404edfe8ddb9083f9ceec8c5e7/src/rez/vendor/amqp/serialization.py#L210-L217,"def read_array(self): array_length = unpack('>I', self.input.read(4))[0] array_data = AMQPReader(self.input.read(array_length)) result = [] while array_data.input.tell() < array_length: val = array_data.read_item() result.append(val) return result",0
358,Python,readonly array,https://github.com/zeroSteiner/AdvancedHTTPServer/blob/8c53cf7e1ddbf7ae9f573c82c5fe5f6992db7b5a/advancedhttpserver.py#L1920-L1927,"def _serve_ready(self): read_check = [self.__wakeup_fd] for sub_server in self.sub_servers: read_check.extend(sub_server.read_checkable_fds) all_read_ready, _, _ = select.select(read_check, [], []) for read_ready in all_read_ready: if isinstance(read_ready, (_RequestEmbryo, http.server.HTTPServer)): read_ready.serve_ready()",0
436,Python,readonly array,https://github.com/piglei/uwsgi-sloth/blob/2834ac5ed17d89ca5f19151c649ac610f6f37bd1/uwsgi_sloth/tailer.py#L45-L51,"def read(self, read_size=None): if read_size: read_str = self.file.read(read_size) else: read_str = self.file.read() return len(read_str), read_str",0
830,Python,readonly array,https://github.com/SmokinCaterpillar/pypet/blob/97ad3e80d46dbdea02deeb98ea41f05a19565826/pypet/storageservice.py#L5026-L5042,"def _svrc_read_array(array): EMPTY_ARRAY_FIX_PT_2 = 'PTCOMPAT__empty__dtype' empty_array_val = 0 none_type = 0 res = array.read() try: if (res.shape == (1,) and res[0] == empty_array_val and EMPTY_ARRAY_FIX_PT_2 in array._v_attrs): # If the array was stored with pytables 2 we end up here dtype = array._v_attrs[EMPTY_ARRAY_FIX_PT_2] if dtype == none_type: res = np.array([]) else: res = np.array([], dtype=np.dtype(dtype)) except (AttributeError, TypeError): pass # has no size or getitem, we don't need to worry return res",0
948,Python,readonly array,https://github.com/ThreatConnect-Inc/tcex/blob/dd4d7a1ef723af1561687120191886b9a2fd4b47/tcex/tcex_playbook.py#L434-L445,"def read_array(self, key, embedded=True): """"""Alias for read method that will read any type (e.g., String, KeyValue) and always return array. Args: key (string): The variable to read from the DB. embedded (boolean): Resolve embedded variables. Returns: (any): Results retrieved from DB """""" return self.read(key, True, embedded)",0
1881,Python,reading element from html  td,https://github.com/atlassian-api/atlassian-python-api/blob/540d269905c3e7547b666fe30c647b2d512cf358/atlassian/utils.py#L80-L103,"def html_row_with_ordered_headers(data, headers): """""" >>> headers = ['administrators', 'key', 'leader', 'project'] >>> data = {'key': 'DEMO', 'project': 'Demonstration', 'leader': 'leader@example.com', 'administrators': ['admin1@example.com', 'admin2@example.com']} >>> html_row_with_ordered_headers(data, headers) '\\n\\t<tr><td><ul><li><a href=""mailto:admin1@example.com"">admin1@example.com</a></li><li><a href=""mailto:admin2@example.com"">admin2@example.com</a></li></ul></td><td>DEMO</td><td>leader@example.com</td><td>Demonstration</td></tr>' >>> headers = ['key', 'project', 'leader', 'administrators'] >>> html_row_with_ordered_headers(data, headers) '\\n\\t<tr><td>DEMO</td><td>Demonstration</td><td>leader@example.com</td><td><ul><li><a href=""mailto:admin1@example.com"">admin1@example.com</a></li><li><a href=""mailto:admin2@example.com"">admin2@example.com</a></li></ul></td></tr>' """""" html = '\n\t<tr>' for header in headers: element = data[header] if isinstance(element, list): element = html_list(element) if is_email(element): element = html_email(element) html += '<td>{}</td>'.format(element) return html + '</tr>'",3
148,Python,reading element from html  td,https://github.com/turicas/rows/blob/c74da41ae9ed091356b803a64f8a30c641c5fc45/rows/plugins/plugin_html.py#L177-L183,"def tag_to_dict(html): """"""Extract tag's attributes into a `dict`."""""" element = document_fromstring(html).xpath(""//html/body/child::*"")[0] attributes = dict(element.attrib) attributes[""text""] = element.text_content() return attributes",2
691,Python,reading element from html  td,https://github.com/danidee10/Staticfy/blob/ebc555b00377394b0f714e4a173d37833fec90cb/staticfy/staticfy.py#L71-L89,"def get_elements(html_file, tags): """""" Extract all the elements we're interested in. Returns a list of tuples with the attribute as first item and the list of elements as the second item. """""" with open(html_file) as f: document = BeautifulSoup(f, 'html.parser') def condition(tag, attr): # Don't include external links return lambda x: x.name == tag \ and not x.get(attr, 'http').startswith(('http', '//')) all_tags = [(attr, document.find_all(condition(tag, attr))) for tag, attr in tags] return all_tags",2
854,Python,reading element from html  td,https://github.com/toomore/goristock/blob/e61f57f11a626cfbc4afbf66337fd9d1c51e3e71/grs/mobileapi.py#L38-L72,"def output(self): #re = ""{%(time)s} %(name)s %(stock_no)s %(c)s %(range)+.2f(%(pp)+.2f%%) %(value)s"" % { ''' re = """"""<table> <tr><td>%(name)s</td><td>%(c)s</td><td>%(range)+.2f(%(pp)+.2f%%)</td></tr> <tr><td>%(stock_no)s</td><td>%(value)s</td><td>%(time)s</td></tr></table>"""""" % { ''' if covstr(self.g['range']) > 0: css = ""red"" elif covstr(self.g['range']) < 0: css = ""green"" else: css = ""gray"" re = { 'name': self.g['name'], 'stock_no': self.g['no'], 'time': self.g['time'], 'open': self.g['open'], 'h': self.g['h'], 'l': self.g['l'], 'c': self.g['c'], 'max': self.g['max'], 'min': self.g['min'], 'range': covstr(self.g['range']), 'ranges': self.g['ranges'], 'value': self.g['value'], 'pvalue': self.g['pvalue'], 'pp': covstr(self.g['pp']), 'top5buy': self.g['top5buy'], 'top5sell': self.g['top5sell'], 'crosspic': self.g['crosspic'], 'css': css } return re",2
1192,Python,reading element from html  td,https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/extractors/readability/htmls.py#L106-L116,"def get_body(doc): [ elem.drop_tree() for elem in doc.xpath('.//script | .//link | .//style') ] raw_html = tostring(doc.body or doc).decode(""utf-8"") print(raw_html) cleaned = clean_attributes(raw_html) try: #BeautifulSoup(cleaned) #FIXME do we really need to try loading it? return cleaned except Exception: #FIXME find the equivalent lxml error #logging.error(""cleansing broke html content: %s\n---------\n%s"" % (raw_html, cleaned)) return raw_html",2
1210,Python,reading element from html  td,https://github.com/alvinwan/md2py/blob/7f28a008367d7d9b5b3c8bbf7baf17985271489f/md2py/md2py.py#L140-L150,"def fromHTML(html, *args, **kwargs): """""" Creates abstraction using HTML :param str html: HTML :return: TreeOfContents object """""" source = BeautifulSoup(html, 'html.parser', *args, **kwargs) return TOC('[document]', source=source, descendants=source.children)",2
1439,Python,reading element from html  td,https://github.com/posativ/isso/blob/78997f491044b7d694ac7170edc32030544095b7/isso/utils/parse.py#L21-L80,"def thread(data, default=u""Untitled."", id=None): """""" Extract <h1> title from web page. The title is *probably* the text node, which is the nearest H1 node in context to an element with the `isso-thread` id. """""" html = html5lib.parse(data, treebuilder=""dom"") assert html.lastChild.nodeName == ""html"" html = html.lastChild # aka getElementById, but limited to div and section tags el = list(filter(lambda i: i.attributes[""id""].value == ""isso-thread"", filter(lambda i: ""id"" in i.attributes, chain(*map(html.getElementsByTagName, (""div"", ""section"")))))) if not el: return id, default el = el[0] visited = [] def recurse(node): for child in node.childNodes: if child.nodeType != child.ELEMENT_NODE: continue if child.nodeName.upper() == ""H1"": return child if child not in visited: return recurse(child) def gettext(rv): for child in rv.childNodes: if child.nodeType == child.TEXT_NODE: yield child.nodeValue if child.nodeType == child.ELEMENT_NODE: for item in gettext(child): yield item try: id = unquote(el.attributes[""data-isso-id""].value) except (KeyError, AttributeError): pass try: return id, unquote(el.attributes[""data-title""].value) except (KeyError, AttributeError): pass while el is not None: # el.parentNode is None in the very end visited.append(el) rv = recurse(el) if rv: return id, ''.join(gettext(rv)).strip() el = el.parentNode return id, default",2
505,Python,reading element from html  td,https://github.com/lsst-epo/vela/blob/8e17ebec509be5c3cc2063f4645dfe9e26b49c18/astropixie-widgets/astropixie_widgets/visual.py#L39-L53,"def _telescope_pointing_widget(cluster_name): html = '<table><thead><tr>' html += '<td><b>Telescope pointing</b></td>' html += '<td><b>Cluster Name</b></td>' html += '<td><b>Image number</b></td>' html += '<td><b>Right ascension</b></td>' html += '<td><b>Declination</b></td>' html += '</tr></thead><tbody><tr>' html += '<td><img src=""http://assets.lsst.rocks/data/sphere.png""></td>' html += '<td>%s</td>' % cluster_name html += '<td>20221274993</td>' html += '<td>05h 32m 37s</td>' html += '<td>+00h 11m 18s</td>' html += '</tr></tbody></table>' return Div(text=html, width=600, height=175)",1
1788,Python,reading element from html  td,https://github.com/horejsek/python-webdriverwrapper/blob/a492f79ab60ed83d860dd817b6a0961500d7e3f5/webdriverwrapper/forms.py#L133-L138,"def fill_input_radio(self, value, skip_reset=False): elm = self.form_elm.get_elm(xpath='//input[@type=""radio""][@name=""%s""][@value=""%s""]' % ( self.elm_name, self.convert_value(value), )) self._click_on_elm_or_his_ancestor(elm)",1
397,Python,reading element from html  td,https://github.com/katerina7479/pypdflite/blob/ac2501f30d6619eae9dea5644717575ca9263d0a/pypdflite/pdfobjects/pdfhtml.py#L407-L410,def _parsehtml(self): parser = PDFHTMLParser() parser.feed(self.htmltext) self.commandlist = parser.get_commandlist(),0
1445,Python,read text file line by line,https://github.com/adrienverge/yamllint/blob/fec2c2fba736cabf6bee6b5eeb905cab0dc820f6/yamllint/rules/new_line_at_end_of_file.py#L34-L37,"def check(conf, line): if line.end == len(line.buffer) and line.end > line.start: yield LintProblem(line.line_no, line.end - line.start + 1, 'no new line character at the end of file')",3
1539,Python,read text file line by line,https://github.com/chrisjsewell/jsonextended/blob/c3a7a880cc09789b3c61204265dcbb127be76c8a/jsonextended/mockpath.py#L44-L54,def readline(self): if self._current_line >= len(self._linelist): line = '' else: line = self._linelist[self._current_line] + '\n' self._current_line += 1 self._current_indx += len( '\n'.join(self._linelist[0:self._current_line])) if self._encoding is not None: line = line.encode(self._encoding) return line,3
145,Python,read text file line by line,https://github.com/molmod/molmod/blob/a7b5b4364ed514ad4c465856c05b5eda1cb561e0/molmod/io/cp2k.py#L204-L213,"def readline(self, f): """"""A helper method that only reads uncommented lines"""""" while True: line = f.readline() if len(line) == 0: raise EOFError line = line[:line.find('#')] line = line.strip() if len(line) > 0: return line",2
166,Python,read text file line by line,https://github.com/limodou/uliweb/blob/34472f25e4bc0b954a35346672f94e84ef18b076/uliweb/core/template.py#L489-L524,"def reindent(text, filename): new_lines=[] k=0 c=0 for n, raw_line in enumerate(text.splitlines()): line=raw_line.strip() if not line or line[0]=='#': new_lines.append(line) continue line3 = line[:3] line4 = line[:4] line5 = line[:5] line6 = line[:6] line7 = line[:7] if line3=='if ' or line4 in ('def ', 'for ', 'try:') or\ line6=='while ' or line6=='class ' or line5=='with ': new_lines.append(' '*k+line) k += 1 continue elif line5=='elif ' or line5=='else:' or \ line7=='except:' or line7=='except ' or \ line7=='finally:': c = k-1 if c<0: # print (_format_code(text)) raise ParseError(""Extra pass founded on line %s:%d"" % (filename, n)) new_lines.append(' '*c+line) continue else: new_lines.append(' '*k+line) if line=='pass' or line5=='pass ': k-=1 if k<0: k = 0 text='\n'.join(new_lines) return text",2
1143,Python,read text file line by line,https://github.com/tamasgal/km3pipe/blob/7a9b59ac899a28775b5bdc5d391d9a5340d08040/km3pipe/hardware.py#L106-L119,"def _readline(self, ignore_comments=True): """"""The next line of the DETX file, optionally ignores comments"""""" while True: line = self._det_file.readline() if line == '': return line # To conform the EOF behaviour of .readline() line = line.strip() if line == '': continue # white-space-only line if line.startswith('#'): if not ignore_comments: return line else: return line",2
1277,Python,read text file line by line,https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/widgets/mixins.py#L499-L507,"def get_text_line(self, line_nb): """"""Return text line at line number *line_nb*"""""" # Taking into account the case when a file ends in an empty line, # since splitlines doesn't return that line as the last element # TODO: Make this function more efficient try: return to_text_string(self.toPlainText()).splitlines()[line_nb] except IndexError: return self.get_line_separator()",2
1590,Python,read text file line by line,https://github.com/bxlab/bx-python/blob/09cb725284803df90a468d910f2274628d8647de/lib/bx/align/lav.py#L287-L296,"def fetch_line(self,strip=True,requireLine=True,report=""""): if (strip == None): line = self.file.readline() elif (strip == True): line = self.file.readline().strip() else: line = self.file.readline().strip().strip(strip) self.lineNumber += 1 if (requireLine): assert (line), \ ""unexpected blank line or end of file%s (line %d)"" \ % (report,self.lineNumber) return line",2
1635,Python,read text file line by line,https://github.com/MIT-LCP/wfdb-python/blob/cc8c9e9e44f10af961b7a9d8ae03708b31ac8a8c/wfdb/io/_header.py#L675-L730,"def _read_header_lines(base_record_name, dir_name, pb_dir): """""" Read the lines in a local or remote header file. Parameters ---------- base_record_name : str The base name of the WFDB record to be read, without any file extensions. dir_name : str The local directory location of the header file. This parameter is ignored if `pb_dir` is set. pb_dir : str Option used to stream data from Physiobank. The Physiobank database directory from which to find the required record files. eg. For record '100' in 'http://physionet.org/physiobank/database/mitdb' pb_dir='mitdb'. Returns ------- header_lines : list List of strings corresponding to the header lines. comment_lines : list List of strings corresponding to the comment lines. """""" file_name = base_record_name + '.hea' # Read local file if pb_dir is None: with open(os.path.join(dir_name, file_name), 'r') as fp: # Record line followed by signal/segment lines if any header_lines = [] # Comment lines comment_lines = [] for line in fp: line = line.strip() # Comment line if line.startswith('#'): comment_lines.append(line) # Non-empty non-comment line = header line. elif line: # Look for a comment in the line ci = line.find('#') if ci > 0: header_lines.append(line[:ci]) # comment on same line as header line comment_lines.append(line[ci:]) else: header_lines.append(line) # Read online header file else: header_lines, comment_lines = download._stream_header(file_name, pb_dir) return header_lines, comment_lines",2
798,Python,read text file line by line,https://github.com/ml4ai/delphi/blob/6d03d8aafeab99610387c51b89c99738ff2abbe3/delphi/translators/for2py/format.py#L99-L138,"def read_line(self, line): """""" Match a line of input according to the format specified and return a tuple of the resulting values """""" if not self._read_line_init: self.init_read_line() match = self._re.match(line) assert match is not None, f""Format mismatch (line = {line})"" matched_values = [] for i in range(self._re.groups): cvt_re = self._match_exps[i] cvt_div = self._divisors[i] cvt_fn = self._in_cvt_fns[i] match_str = match.group(i + 1) match0 = re.match(cvt_re, match_str) if match0 is not None: if cvt_fn == ""float"": if ""."" in match_str: val = float(match_str) else: val = int(match_str) / cvt_div elif cvt_fn == ""int"": val = int(match_str) else: sys.stderr.write( f""Unrecognized conversion function: {cvt_fn}\n"" ) else: sys.stderr.write( f""Format conversion failed: {match_str}\n"" ) matched_values.append(val) return tuple(matched_values)",1
962,Python,read text file line by line,https://github.com/niccokunzmann/ledtable/blob/a94d276f8a06e0f7f05f5cc704018c899e56bd9f/python/ledtable/SerialLEDTable.py#L54-L63,"def default_line(self, line): if line.endswith(b""\n""): line = line[:-1] if line.endswith(b""\r""): line = line[:-1] try: line = line.decode(""ASCII"") except UnicodeDecodeError: pass print(line)",1
856,Python,read properties file,https://github.com/rcook/upload-haddocks/blob/a33826be1873da68ba073a42ec828c8ec150d576/setup.py#L16-L26,"def _read_properties(): init_path = os.path.abspath(os.path.join(""uploadhaddocks"", ""__init__.py"")) regex = re.compile(""^\\s*__(?P<key>.*)__\\s*=\\s*\""(?P<value>.*)\""\\s*$"") with open(init_path, ""rt"") as f: props = {} for line in f.readlines(): m = regex.match(line) if m is not None: props[m.group(""key"")] = m.group(""value"") return props",3
957,Python,read properties file,https://github.com/crs4/pydoop/blob/f375be2a06f9c67eaae3ce6f605195dbca143b2b/pydoop/__init__.py#L183-L193,"def read_properties(fname): parser = configparser.SafeConfigParser() parser.optionxform = str # preserve key case try: with open(fname) as f: parser_read(parser, AddSectionWrapper(f)) except IOError as e: if e.errno != errno.ENOENT: raise return None # compile time, prop file is not there return dict(parser.items(AddSectionWrapper.SEC_NAME))",3
2070,Python,read properties file,https://github.com/markreidvfx/pyaaf2/blob/37de8c10d3c3495cc00c705eb6c5048bc4a7e51f/tools/model_dump.py#L99-L127,"def read_properties(entry): stream = entry.get('properties') if stream is None: raise Exception(""can not find properties"") s = stream.open() # read the whole stream f = BytesIO(s.read()) byte_order = read_u8(f) if byte_order != 0x4c: raise NotImplementedError(""be byteorder"") version = read_u8(f) entry_count = read_u16le(f) props = [] for i in range(entry_count): pid = read_u16le(f) format = read_u16le(f) byte_size = read_u16le(f) props.append([pid, format, byte_size]) property_entries = {} for pid, format, byte_size in props: data = f.read(byte_size) property_entries[pid] = data return property_entries",3
627,Python,read properties file,https://github.com/markreidvfx/pyaaf2/blob/37de8c10d3c3495cc00c705eb6c5048bc4a7e51f/aaf2/core.py#L73-L113,"def read_properties(self): stream = self.dir.get('properties') if stream is None: return s = stream.open() # read the whole stream f = BytesIO(s.read()) (byte_order, version, entry_count) = P_HEADER_STRUCT.unpack(f.read(4)) if byte_order != 0x4c: raise NotImplementedError(""be byteorder"") props = array.array(str('H')) if hasattr(props, 'frombytes'): props.frombytes(f.read(6 * entry_count)) else: props.fromstring(f.read(6 * entry_count)) if sys.byteorder == 'big': props.byteswap() for i in range(entry_count): index = i * 3 pid = props[index + 0] format = props[index + 1] byte_size = props[index + 2] data = f.read(byte_size) p = property_formats[format](self, pid, format, version) p.data = data self.property_entries[pid] = p for p in self.property_entries.values(): p.decode() if isinstance(p, (properties.StrongRefSetProperty, properties.StrongRefVectorProperty, properties.WeakRefArrayProperty)): p.read_index()",2
1581,Python,read properties file,https://github.com/GreatFruitOmsk/tailhead/blob/a3b1324a39935f8ffcfda59328a9a458672889d9/tailhead/__init__.py#L72-L79,"def read(self, read_size=-1): """""" Read given number of bytes from file. :param read_size: Number of bytes to read. -1 to read all. :return: Number of bytes read and data that was read. """""" read_str = self.file.read(read_size) return len(read_str), read_str",2
1794,Python,read properties file,https://github.com/nion-software/nionswift/blob/d43693eaf057b8683b9638e575000f055fede452/nion/swift/model/FileStorageSystem.py#L97-L106,"def __read_properties(self): properties = dict() if self.__filepath and os.path.exists(self.__filepath): try: with open(self.__filepath, ""r"") as fp: properties = json.load(fp) except Exception: os.replace(self.__filepath, self.__filepath + "".bak"") # migrations go here return properties",2
227,Python,read properties file,https://github.com/piglei/uwsgi-sloth/blob/2834ac5ed17d89ca5f19151c649ac610f6f37bd1/uwsgi_sloth/tailer.py#L45-L51,"def read(self, read_size=None): if read_size: read_str = self.file.read(read_size) else: read_str = self.file.read() return len(read_str), read_str",1
720,Python,read properties file,https://github.com/nion-software/nionswift/blob/d43693eaf057b8683b9638e575000f055fede452/nion/swift/model/NDataHandler.py#L469-L481,"def read_properties(self): """""" Read properties from the ndata file reference :param reference: the reference from which to read :return: a tuple of the item_uuid and a dict of the properties """""" with self.__lock: absolute_file_path = self.__file_path with open(absolute_file_path, ""rb"") as fp: local_files, dir_files, eocd = parse_zip(fp) properties = read_json(fp, local_files, dir_files, b""metadata.json"") return properties",1
840,Python,read properties file,https://github.com/readbeyond/aeneas/blob/9d95535ad63eef4a98530cfdff033b8c35315ee1/aeneas/audiofile.py#L330-L376,"def read_properties(self): """""" Populate this object by reading the audio properties of the file at the given path. Currently this function uses :class:`~aeneas.ffprobewrapper.FFPROBEWrapper` to get the audio file properties. :raises: :class:`~aeneas.audiofile.AudioFileProbeError`: if the path to the ``ffprobe`` executable cannot be called :raises: :class:`~aeneas.audiofile.AudioFileUnsupportedFormatError`: if the audio file has a format not supported :raises: OSError: if the audio file cannot be read """""" self.log(u""Reading properties..."") # check the file can be read if not gf.file_can_be_read(self.file_path): self.log_exc(u""File '%s' cannot be read"" % (self.file_path), None, True, OSError) # get the file size self.log([u""Getting file size for '%s'"", self.file_path]) self.file_size = gf.file_size(self.file_path) self.log([u""File size for '%s' is '%d'"", self.file_path, self.file_size]) # get the audio properties using FFPROBEWrapper try: self.log(u""Reading properties with FFPROBEWrapper..."") properties = FFPROBEWrapper( rconf=self.rconf, logger=self.logger ).read_properties(self.file_path) self.log(u""Reading properties with FFPROBEWrapper... done"") except FFPROBEPathError: self.log_exc(u""Unable to call ffprobe executable"", None, True, AudioFileProbeError) except (FFPROBEUnsupportedFormatError, FFPROBEParsingError): self.log_exc(u""Audio file format not supported by ffprobe"", None, True, AudioFileUnsupportedFormatError) # save relevant properties in results inside the audiofile object self.audio_length = TimeValue(properties[FFPROBEWrapper.STDOUT_DURATION]) self.audio_format = properties[FFPROBEWrapper.STDOUT_CODEC_NAME] self.audio_sample_rate = gf.safe_int(properties[FFPROBEWrapper.STDOUT_SAMPLE_RATE]) self.audio_channels = gf.safe_int(properties[FFPROBEWrapper.STDOUT_CHANNELS]) self.log([u""Stored audio_length: '%s'"", self.audio_length]) self.log([u""Stored audio_format: '%s'"", self.audio_format]) self.log([u""Stored audio_sample_rate: '%s'"", self.audio_sample_rate]) self.log([u""Stored audio_channels: '%s'"", self.audio_channels]) self.log(u""Reading properties... done"")",1
1097,Python,read properties file,https://github.com/openstax/cnx-epub/blob/f648a309eff551b0a68a115a98ddf7858149a2ea/cnxepub/epub.py#L349-L376,"def from_file(cls, file): """"""Create the object from a *file* or *file-like object*."""""" opf_xml = etree.parse(file) # Check if ``file`` is file-like. if hasattr(file, 'read'): name = os.path.basename(file.name) root = os.path.abspath(os.path.dirname(file.name)) else: # ...a filepath name = os.path.basename(file) root = os.path.abspath(os.path.dirname(file)) parser = OPFParser(opf_xml) # Roll through the item entries manifest = opf_xml.xpath('/opf:package/opf:manifest/opf:item', namespaces=EPUB_OPF_NAMESPACES) pkg_items = [] for item in manifest: absolute_filepath = os.path.join(root, item.get('href')) properties = item.get('properties', '').split() is_navigation = 'nav' in properties media_type = item.get('media-type') pkg_items.append(Item.from_file(absolute_filepath, media_type=media_type, is_navigation=is_navigation, properties=properties)) # Ignore spine ordering, because it is not important # for our use cases. return cls(name, pkg_items, parser.metadata)",1
647,Python,randomly extract x items from a list,https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_list.py#L2155-L2189,"def sample_lists(items_list, num=1, seed=None): r"""""" Args: items_list (list): num (int): (default = 1) seed (None): (default = None) Returns: list: samples_list CommandLine: python -m utool.util_list --exec-sample_lists Example: >>> # DISABLE_DOCTEST >>> from utool.util_list import * # NOQA >>> items_list = [[], [1, 2, 3], [4], [5, 6], [7, 8, 9, 10]] >>> num = 2 >>> seed = 0 >>> samples_list = sample_lists(items_list, num, seed) >>> result = ('samples_list = %s' % (str(samples_list),)) >>> print(result) samples_list = [[], [3, 2], [4], [5, 6], [10, 9]] """""" if seed is not None: rng = np.random.RandomState(seed) else: rng = np.random def random_choice(items, num): size = min(len(items), num) return rng.choice(items, size, replace=False).tolist() samples_list = [random_choice(items, num) if len(items) > 0 else [] for items in items_list] return samples_list",3
2052,Python,randomly extract x items from a list,https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_numpy.py#L323-L365,"def random_sample(list_, nSample, strict=False, rng=None, seed=None): """""" Grabs data randomly Args: list_ (list): nSample (?): strict (bool): (default = False) rng (module): random number generator(default = numpy.random) seed (None): (default = None) Returns: list: sample_list CommandLine: python -m utool.util_numpy --exec-random_sample Example: >>> # DISABLE_DOCTEST >>> from utool.util_numpy import * # NOQA >>> list_ = np.arange(10) >>> nSample = 4 >>> strict = False >>> rng = np.random.RandomState(0) >>> seed = None >>> sample_list = random_sample(list_, nSample, strict, rng, seed) >>> result = ('sample_list = %s' % (str(sample_list),)) >>> print(result) """""" rng = ensure_rng(seed if rng is None else rng) if isinstance(list_, list): list2_ = list_[:] else: list2_ = np.copy(list_) if len(list2_) == 0 and not strict: return list2_ rng.shuffle(list2_) if nSample is None and strict is False: return list2_ if not strict: nSample = min(max(0, nSample), len(list2_)) sample_list = list2_[:nSample] return sample_list",3
9,Python,randomly extract x items from a list,https://github.com/rkargon/pixelsorter/blob/0775d1e487fbcb023e411e1818ba3290b0e8665e/pixelsorter/util.py#L72-L84,"def weighted_random_choice(items): """""" Returns a weighted random choice from a list of items. :param items: A list of tuples (object, weight) :return: A random object, whose likelihood is proportional to its weight. """""" l = list(items) r = random.random() * sum([i[1] for i in l]) for x, p in l: if p > r: return x r -= p return None",2
89,Python,randomly extract x items from a list,https://github.com/callowayproject/Calloway/blob/d22e98d41fbd298ab6393ba7bd84a75528be9f81/calloway/apps/django_ext/markov.py#L51-L64,"def random_output(self, max=100): """""" Generate a list of elements from the markov chain. The `max` value is in place in order to prevent excessive iteration. """""" output = [] item1 = item2 = MarkovChain.START for i in range(max-3): item3 = self[(item1, item2)].roll() if item3 is MarkovChain.END: break output.append(item3) item1 = item2 item2 = item3 return output",1
1039,Python,randomly extract x items from a list,https://github.com/pereorga/csvshuf/blob/70fdd4f512ef980bffe9cc51bfe59fea116d7c2f/csvshuf/csvshuf.py#L19-L24,"def shuffle_sattolo(items): """"""Shuffle items in place using Sattolo's algorithm."""""" _randrange = random.randrange for i in reversed(range(1, len(items))): j = _randrange(i) # 0 <= j < i items[j], items[i] = items[i], items[j]",1
1952,Python,randomly extract x items from a list,https://github.com/lazygunner/xunleipy/blob/cded7598a7bf04495156bae2d747883d1eacb3f4/xunleipy/rsa_lib.py#L167-L177,"def findAPrime(a, b, k): """"""Return a pseudo prime number roughly between a and b, (could be larger than b). Raise ValueError if cannot find a pseudo prime after 10 * ln(x) + 3 tries. """""" x = random.randint(a, b) for i in range(0, int(10 * math.log(x) + 3)): if millerRabin(x, k): return x else: x += 1 raise ValueError",1
3,Python,randomly extract x items from a list,https://github.com/CitrineInformatics/python-citrination-client/blob/409984fc65ce101a620f069263f155303492465c/citrination_client/search/core/query/base_returning_query.py#L9-L40,"def __init__(self, query=None, extraction_sort=None, from_index=None, size=None, random_results=None, random_seed=None, score_relevance=None, return_max_score=None, timeout=None, **kwargs): """""" Base class for all queries against datasets and the items that they contain on Citrination. :param query: One or more :class:`DataQuery` objects with the queries to run. :param extraction_sort: A single :class:`ExtractionSort` object for sorting. :param from_index: Index of the first hit that should be returned. :param size: Total number of hits the should be returned. :param random_results: Whether to return a random set of records. :param random_seed: The random seed to use. :param score_relevance: Whether to use relevance scoring. :param return_max_score: Whether to return the maximum score. :param timeout: The number of milliseconds to wait for the query to execute. """""" super(BaseReturningQuery, self).__init__(query=query, extraction_sort=extraction_sort, **kwargs) if 'from' in 'kwargs': self.from_index = kwargs['from'] self._from = None self.from_index = from_index self._size = None self.size = size self._random_results = None self.random_results = random_results self._random_seed = None self.random_seed = random_seed self._score_relevance = None self.score_relevance = score_relevance self._return_max_score = None self.return_max_score = return_max_score self._timeout = None self.timeout = timeout",0
260,Python,randomly extract x items from a list,https://github.com/acutesoftware/AIKIF/blob/fcf1582dc5f884b9a4fa7c6e20e9de9d94d21d03/aikif/toolbox/solve_knapsack.py#L51-L65,"def solve_expensive_items_first(capacity, items): taken = [0]*len(items) value = 0 weight = 0 taken = [0]*len(items) sortedList = sorted(items, key=lambda dens: dens[1], reverse=True) for item in sortedList: #print ('item [' + str(item.index) + '] value = ' + str(item.value) + ', item.weight = ' + str(item.weight) + ' density = ' + str(item.density)) if weight + item.weight <= capacity: taken[item.index] = 1 value += item.value weight += item.weight #print('Adding [' + str(item.index) + '], value = ' + str(item.value) + ' wght = ' + str(item.weight) ) return value, taken",0
398,Python,randomly extract x items from a list,https://github.com/mozilla/crontabber/blob/b510be349e71f165c1a9506db95bda0b88728f8b/crontabber/generic_app.py#L178-L184,def tear_down_logger(app_name): logger = logging.getLogger(app_name) # must have a copy of the handlers list since we cannot modify the original # list while we're deleting items from that list handlers = [x for x in logger.handlers] for x in handlers: logger.removeHandler(x),0
776,Python,randomly extract x items from a list,https://github.com/Unity-Technologies/ml-agents/blob/37d139af636e4a2351751fbf0f2fca5a9ed7457f/ml-agents/mlagents/trainers/tensorflow_to_barracuda.py#L496-L504,"def flatten(items,enter=lambda x:isinstance(x, list)): # http://stackoverflow.com/a/40857703 # https://github.com/ctmakro/canton/blob/master/canton/misc.py """"""Yield items from any nested iterable; see REF."""""" for x in items: if enter(x): yield from flatten(x) else: yield x",0
1,Python,priority queue,https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/queues/priority_queue.py#L38-L49,"def push(self, item, priority=None): """"""Push the item in the priority queue. if priority is not given, priority is set to the value of item. """""" priority = item if priority is None else priority node = PriorityQueueNode(item, priority) for index, current in enumerate(self.priority_queue_list): if current.priority < node.priority: self.priority_queue_list.insert(index, node) return # when traversed complete queue self.priority_queue_list.append(node)",3
1409,Python,priority queue,https://github.com/coleifer/huey/blob/416e8da1ca18442c08431a91bce373de7d2d200f/huey/storage.py#L401-L405,"def enqueue(self, data, priority=None): if priority: raise NotImplementedError('Task priorities are not supported by ' 'this storage.') self.conn.lpush(self.queue_key, data)",3
297,Python,priority queue,https://github.com/mushkevych/scheduler/blob/6740331360f49083c208085fb5a60ce80ebf418b/synergy/scheduler/garbage_collector.py#L81-L98,"def _flush_queue(self, q, ignore_priority=False): """""" :param q: PriorityQueue instance holding GarbageCollector entries :param ignore_priority: If True - all GarbageCollector entries should be resubmitted If False - only those entries whose waiting time has expired will be resubmitted """""" assert isinstance(q, PriorityQueue) current_timestamp = compute_release_time(lag_in_minutes=0) for _ in range(len(q)): entry = q.pop() assert isinstance(entry, PriorityEntry) if ignore_priority or entry.release_time < current_timestamp: self._resubmit_uow(entry.entry) else: q.put(entry) break",2
928,Python,priority queue,https://github.com/hubo1016/vlcp/blob/239055229ec93a99cc7e15208075724ccf543bd1/vlcp/event/pqueue.py#L1032-L1040,"def setPriority(self, queue, priority): ''' Set priority of a sub-queue ''' q = self.queueindex[queue] self.queues[q[0]].removeSubQueue(q[1]) newPriority = self.queues.setdefault(priority, CBQueue.MultiQueue(self, priority)) q[0] = priority newPriority.addSubQueue(q[1])",2
149,Python,priority queue,https://github.com/limpyd/redis-limpyd-jobs/blob/264c71029bad4377d6132bf8bb9c55c44f3b03a2/limpyd_jobs/workers.py#L492-L504,"def requeue_job(self, job, queue, priority, delayed_for=None): """""" Requeue a job in a queue with the given priority, possibly delayed """""" job.requeue(queue_name=queue._cached_name, priority=priority, delayed_for=delayed_for, queue_model=self.queue_model) if hasattr(job, 'on_requeued'): job.on_requeued(queue) self.log(self.job_requeue_message(job, queue))",1
473,Python,priority queue,https://github.com/ask/ghettoq/blob/22a0fcd865b618cbbbfd102efd88a7983507c24e/ghettoq/backends/beanstalk.py#L32-L34,"def put(self, queue, message, priority=0, **kwargs): self.client.use(queue) self.client.put(message, priority=priority)",1
1289,Python,priority queue,https://github.com/LogicalDash/LiSE/blob/fe6fd4f0a7c1780e065f4c9babb9bc443af6bb84/ELiDE/ELiDE/board/pawn.py#L105-L108,"def on_priority(self, *args): if self.proxy['_priority'] != self.priority: self.proxy['_priority'] = self.priority self.parent.restack()",1
1808,Python,priority queue,https://github.com/rfk/threading2/blob/7ec234ddd8c0d7e683b1a5c4a79a3d001143189f/threading2/t2_base.py#L372-L378,"def _set_priority(self,priority): if not 0 <= priority <= 1: raise ValueError(""priority must be between 0 and 1"") self.__priority = priority if self.is_alive(): self.priority = priority return priority priority = property(_get_priority,_set_priority)",1
116,Python,priority queue,https://github.com/FreshXOpenSource/wallaby-base/blob/5c75b1729e99af905c44b1293e67dbd5527d022d/wallaby/pf/room.py#L366-L370,"def __enqueue(self, catcher, pillow, feathers, passThrower=False, **ka): if pillow.endswith(""!""): self.__enqueueStatefull(catcher, pillow, passThrower) else: self._normalQueue.append( (catcher, (pillow, feathers), ka) )",0
1125,Python,priority queue,https://github.com/open-mmlab/mmcv/blob/0d77f61450aab4dde8b8585a577cc496acb95d7f/mmcv/runner/priority.py#L35-L53,"def get_priority(priority): """"""Get priority value. Args: priority (int or str or :obj:`Priority`): Priority. Returns: int: The priority value. """""" if isinstance(priority, int): if priority < 0 or priority > 100: raise ValueError('priority must be between 0 and 100') return priority elif isinstance(priority, Priority): return priority.value elif isinstance(priority, str): return Priority[priority.upper()].value else: raise TypeError('priority must be an integer or Priority enum value')",0
1224,Python,print model summary,https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/grid/grid_search.py#L439-L457,"def summary(self, header=True): """"""Print a detailed summary of the explored models."""""" table = [] for model in self.models: model_summary = model._model_json[""output""][""model_summary""] r_values = list(model_summary.cell_values[0]) r_values[0] = model.model_id table.append(r_values) # if h2o.can_use_pandas(): # import pandas # pandas.options.display.max_rows = 20 # print pandas.DataFrame(table,columns=self.col_header) # return print() if header: print('Grid Summary:') print() H2ODisplay(table, ['Model Id'] + model_summary.col_header[1:], numalign=""left"", stralign=""left"")",3
1403,Python,print model summary,https://github.com/Unity-Technologies/ml-agents/blob/37d139af636e4a2351751fbf0f2fca5a9ed7457f/ml-agents/mlagents/trainers/barracuda.py#L228-L258,"def summary(model, print_layer_links, print_barracuda_json, print_tensors): def array_without_brackets(arr): return str(arr)[1:-1] # array to string without brackets if print_layer_links: for l in model.layers: print(l.name, "" <= "", l.inputs) if print_barracuda_json: print(to_json(model)) if model.globals: if isinstance(model.globals, dict): model.globals = {x.name:x.shape for x in model.globals} print(""GLOBALS:"", array_without_brackets(model.globals)) for l in model.layers: if isinstance(model.inputs, dict): ins = {i:model.inputs[i] for i in l.inputs if i in model.inputs} else: ins = [i for i in l.inputs if i in model.inputs] if ins: print(""IN: %s => '%s'"" % (array_without_brackets(ins), l.name)) for mem_in, mem_out in zip(model.memories[1::3], model.memories[2::3]): print(""MEM: '%s' => '%s'"" % (mem_in, mem_out)) print(""OUT:"", array_without_brackets(model.outputs)) if (print_tensors): for l in model.layers: for x in l.tensors: print(x.name, x.shape, x.data.dtype, x.data)",3
1495,Python,print model summary,https://github.com/LIVVkit/LIVVkit/blob/680120cd437e408673e62e535fc0a246c7fc17db/livvkit/components/numerics.py#L82-L87,"def _print_summary(module, case, summary): try: module.print_summary(case, summary) except (NotImplementedError, AttributeError): print("" Ran "" + case + ""!"") print("""")",3
1692,Python,print model summary,https://github.com/LIVVkit/LIVVkit/blob/680120cd437e408673e62e535fc0a246c7fc17db/livvkit/components/validation.py#L141-L149,"def _print_summary(module, case, summary): try: try: module.print_summary(summary[case]) except TypeError: module.print_summary(case, summary[case]) except (NotImplementedError, AttributeError): print("" Ran "" + case + ""!"") print("""")",3
1825,Python,print model summary,https://github.com/MillionIntegrals/vel/blob/e0726e1f63742b728966ccae0c8b825ea0ba491a/vel/util/summary.py#L11-L86,"def summary(model, input_size): """""" Print summary of the model """""" def register_hook(module): def hook(module, input, output): class_name = str(module.__class__).split('.')[-1].split(""'"")[0] module_idx = len(summary) m_key = '%s-%i' % (class_name, module_idx + 1) summary[m_key] = OrderedDict() summary[m_key]['input_shape'] = list(input[0].size()) summary[m_key]['input_shape'][0] = -1 if isinstance(output, (list, tuple)): summary[m_key]['output_shape'] = [[-1] + list(o.size())[1:] for o in output] else: summary[m_key]['output_shape'] = list(output.size()) summary[m_key]['output_shape'][0] = -1 params = 0 if hasattr(module, 'weight') and hasattr(module.weight, 'size'): params += torch.prod(torch.LongTensor(list(module.weight.size()))) summary[m_key]['trainable'] = module.weight.requires_grad if hasattr(module, 'bias') and hasattr(module.bias, 'size'): params += torch.prod(torch.LongTensor(list(module.bias.size()))) summary[m_key]['nb_params'] = params if (not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList) and not (module == model)): hooks.append(module.register_forward_hook(hook)) if torch.cuda.is_available(): dtype = torch.cuda.FloatTensor model = model.cuda() else: dtype = torch.FloatTensor model = model.cpu() # check if there are multiple inputs to the network if isinstance(input_size[0], (list, tuple)): x = [Variable(torch.rand(2, *in_size)).type(dtype) for in_size in input_size] else: x = Variable(torch.rand(2, *input_size)).type(dtype) # print(type(x[0])) # create properties summary = OrderedDict() hooks = [] # register hook model.apply(register_hook) # make a forward pass # print(x.shape) model(x) # remove these hooks for h in hooks: h.remove() print('----------------------------------------------------------------') line_new = '{:>20} {:>25} {:>15}'.format('Layer (type)', 'Output Shape', 'Param #') print(line_new) print('================================================================') total_params = 0 trainable_params = 0 for layer in summary: # input_shape, output_shape, trainable, nb_params line_new = '{:>20} {:>25} {:>15}'.format(layer, str(summary[layer]['output_shape']), '{0:,}'.format(summary[layer]['nb_params'])) total_params += summary[layer]['nb_params'] if 'trainable' in summary[layer]: if summary[layer]['trainable'] == True: trainable_params += summary[layer]['nb_params'] print(line_new) print('================================================================') print('Total params: {0:,}'.format(total_params)) print('Trainable params: {0:,}'.format(trainable_params)) print('Non-trainable params: {0:,}'.format(total_params - trainable_params)) print('----------------------------------------------------------------') # return summary",3
125,Python,print model summary,https://github.com/MillionIntegrals/vel/blob/e0726e1f63742b728966ccae0c8b825ea0ba491a/vel/api/model.py#L37-L51,"def summary(self, input_size=None, hashsummary=False): """""" Print a model summary """""" if input_size is None: print(self) print(""-"" * 120) number = sum(p.numel() for p in self.model.parameters()) print(""Number of model parameters: {:,}"".format(number)) print(""-"" * 120) else: summary(self, input_size) if hashsummary: for idx, hashvalue in enumerate(self.hashsummary()): print(f""{idx}: {hashvalue}"")",2
1220,Python,print model summary,https://github.com/dswah/pyGAM/blob/b3e5c3cd580f0a3ad69f9372861624f67760c325/pygam/pygam.py#L1572-L1661,"def summary(self): """"""produce a summary of the model statistics Parameters ---------- None Returns ------- None """""" if not self._is_fitted: raise AttributeError('GAM has not been fitted. Call fit first.') # high-level model summary width_details = 47 width_results = 58 model_fmt = [ (self.__class__.__name__, 'model_details', width_details), ('', 'model_results', width_results) ] model_details = [] objective = 'UBRE' if self.distribution._known_scale else 'GCV' model_details.append({'model_details': space_row('Distribution:', self.distribution.__class__.__name__, total_width=width_details), 'model_results': space_row('Effective DoF:', str(np.round(self.statistics_['edof'], 4)), total_width=width_results)}) model_details.append({'model_details': space_row('Link Function:', self.link.__class__.__name__, total_width=width_details), 'model_results': space_row('Log Likelihood:', str(np.round(self.statistics_['loglikelihood'], 4)), total_width=width_results)}) model_details.append({'model_details': space_row('Number of Samples:', str(self.statistics_['n_samples']), total_width=width_details), 'model_results': space_row('AIC: ', str(np.round(self.statistics_['AIC'], 4)), total_width=width_results)}) model_details.append({'model_results': space_row('AICc: ', str(np.round(self.statistics_['AICc'], 4)), total_width=width_results)}) model_details.append({'model_results': space_row(objective + ':', str(np.round(self.statistics_[objective], 4)), total_width=width_results)}) model_details.append({'model_results': space_row('Scale:', str(np.round(self.statistics_['scale'], 4)), total_width=width_results)}) model_details.append({'model_results': space_row('Pseudo R-Squared:', str(np.round(self.statistics_['pseudo_r2']['explained_deviance'], 4)), total_width=width_results)}) # term summary data = [] for i, term in enumerate(self.terms): # TODO bug: if the number of samples is less than the number of coefficients # we cant get the edof per term if len(self.statistics_['edof_per_coef']) == len(self.coef_): idx = self.terms.get_coef_indices(i) edof = np.round(self.statistics_['edof_per_coef'][idx].sum(), 1) else: edof = '' term_data = { 'feature_func': repr(term), 'lam': '' if term.isintercept else np.round(flatten(term.lam), 4), 'rank': '{}'.format(term.n_coefs), 'edof': '{}'.format(edof), 'p_value': '%.2e'%(self.statistics_['p_values'][i]), 'sig_code': sig_code(self.statistics_['p_values'][i]) } data.append(term_data) fmt = [ ('Feature Function', 'feature_func', 33), ('Lambda', 'lam', 20), ('Rank', 'rank', 12), ('EDoF', 'edof', 12), ('P > x', 'p_value', 12), ('Sig. Code', 'sig_code', 12) ] print( TablePrinter(model_fmt, ul='=', sep=' ')(model_details) ) print(""=""*106) print( TablePrinter(fmt, ul='=')(data) ) print(""=""*106) print(""Significance codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"") print() print(""WARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n"" \ "" which can cause p-values to appear significant when they are not."") print() print(""WARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n"" \ "" known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n"" \ "" are typically lower than they should be, meaning that the tests reject the null too readily."") # P-VALUE BUG warnings.warn(""KNOWN BUG: p-values computed in this summary are likely ""\ ""much smaller than they should be. \n \n""\ ""Please do not make inferences based on these values! \n\n""\ ""Collaborate on a solution, and stay up to date at: \n""\ ""github.com/dswah/pyGAM/issues/163 \n"", stacklevel=2)",2
1287,Python,print model summary,https://github.com/timothyb0912/pylogit/blob/f83b0fd6debaa7358d87c3828428f6d4ead71357/pylogit/base_multinomial_cm_v2.py#L1556-L1572,"def print_summaries(self): """""" Returns None. Will print the measures of fit and the estimation results for the model. """""" if hasattr(self, ""fit_summary"") and hasattr(self, ""summary""): print(""\n"") print(self.fit_summary) print(""="" * 30) print(self.summary) else: msg = ""This {} object has not yet been estimated so there "" msg_2 = ""are no estimation summaries to print."" raise NotImplementedError(msg.format(self.model_type) + msg_2) return None",2
979,Python,print model summary,https://github.com/ioam/lancet/blob/1fbbf88fa0e8974ff9ed462e3cb11722ddebdd6e/lancet/dynamic.py#L257-L265,"def summary(self): print('Varying Keys: %r' % self.key) print('Maximum steps allowed: %d' % self.max_steps) self._trace_summary() (val, arg) = (self.trace[-1]) if self._termination_info: (success, best_val, arg) = self._termination_info condition = 'Successfully converged.' if success else 'Maximum step limit reached.' print(""%s Minimum value of %r at %s=%r."" % (condition, best_val, self.key, arg))",1
1222,Python,print model summary,https://github.com/lrq3000/pyFileFixity/blob/fd5ef23bb13835faf1e3baa773619b86a1cc9bdf/pyFileFixity/lib/profilers/visual/pympler/tracker.py#L126-L134,"def print_diff(self, summary1=None, summary2=None): """"""Compute diff between to summaries and print it. If no summary is provided, the diff from the last to the current summary is used. If summary1 is provided the diff from summary1 to the current summary is used. If summary1 and summary2 are provided, the diff between these two is used. """""" summary.print_(self.diff(summary1=summary1, summary2=summary2))",0
147,Python,pretty print json,https://github.com/profusion/sgqlc/blob/684afb059c93f142150043cafac09b7fd52bfa27/examples/github/github-agile-dashboard.py#L35-L39,"def json_pretty_print(d, file=None): args = {'sort_keys': True, 'indent': 2, 'separators': (',', ': ')} if file: return json.dump(d, file, **args) return json.dumps(d, **args)",3
243,Python,pretty print json,https://github.com/sendgrid/sendgrid-python/blob/266c2abde7a35dfcce263e06bedc6a0bbdebeac9/examples/helpers/stats/stats_example.py#L13-L14,"def pprint_json(json_raw): print(json.dumps(json.loads(json_raw), indent=2, sort_keys=True))",3
530,Python,pretty print json,https://github.com/bulkan/robotframework-requests/blob/11baa3277f1cb728712e26d996200703c15254a8/src/RequestsLibrary/RequestsKeywords.py#L460-L477,"def to_json(self, content, pretty_print=False): """""" Convert a string to a JSON object ``content`` String content to convert into JSON ``pretty_print`` If defined, will output JSON is pretty print format """""" if PY3: if isinstance(content, bytes): content = content.decode(encoding='utf-8') if pretty_print: json_ = self._json_pretty_print(content) else: json_ = json.loads(content) logger.info('To JSON using : content=%s ' % (content)) logger.info('To JSON using : pretty_print=%s ' % (pretty_print)) return json_",3
699,Python,pretty print json,https://github.com/onecodex/onecodex/blob/326a0a1af140e3a57ccf31c3c9c5e17a5775c13d/onecodex/utils.py#L112-L121,"def pprint(j, no_pretty): """""" Prints as formatted JSON """""" if not no_pretty: click.echo( json.dumps(j, cls=PotionJSONEncoder, sort_keys=True, indent=4, separators=("","", "": "")) ) else: click.echo(j)",3
896,Python,pretty print json,https://github.com/MacHu-GWU/angora-project/blob/689a60da51cd88680ddbe26e28dbe81e6b01d275/angora/dtypes/dicttree.py#L378-L382,"def prettyprint(d): """"""Print dicttree in Json-like format. keys are sorted """""" print(json.dumps(d, sort_keys=True, indent=4, separators=("","" , "": "")))",3
1022,Python,pretty print json,https://github.com/PSPC-SPAC-buyandsell/von_anchor/blob/78ac1de67be42a676274f4bf71fe12f66e72f309/von_anchor/frill.py#L30-L45,"def ppjson(dumpit: Any, elide_to: int = None) -> str: """""" JSON pretty printer, whether already json-encoded or not :param dumpit: object to pretty-print :param elide_to: optional maximum length including ellipses ('...') :return: json pretty-print """""" if elide_to is not None: elide_to = max(elide_to, 3) # make room for ellipses '...' try: rv = json.dumps(json.loads(dumpit) if isinstance(dumpit, str) else dumpit, indent=4) except TypeError: rv = '{}'.format(pformat(dumpit, indent=4, width=120)) return rv if elide_to is None or len(rv) <= elide_to else '{}...'.format(rv[0 : elide_to - 3])",3
1101,Python,pretty print json,https://github.com/alejandroesquiva/AutomaticApiRest-PythonConnector/blob/90e55d5cf953dc8d7186fc6df82a6c6a089a3bbe/build/lib/aarpy/AARConnector.py#L36-L38,"def printJson(self): jsonobject = self.getJson() print(json.dumps(jsonobject, indent=1, sort_keys=True))",3
1198,Python,pretty print json,https://github.com/mosesschwartz/scrypture/blob/d51eb0c9835a5122a655078268185ce8ab9ec86a/scrypture/demo_scripts/Utils/json_pretty_print.py#L8-L14,"def json_pretty_print(s): '''pretty print JSON''' s = json.loads(s) return json.dumps(s, sort_keys=True, indent=4, separators=(',', ': '))",3
2009,Python,pretty print json,https://github.com/deeshugupta/tes/blob/217db49aa211ebca2d9258380765a0c31abfca91/es_commands/base.py#L38-L40,"def pretty_print(response): parsed = json.loads(json.dumps(response)) click.echo(json.dumps(parsed, indent=4, sort_keys=True))",3
451,Python,pretty print json,https://github.com/markbaas/python-iresolve/blob/ba91e37221e91265e4ac5dbc6e8f5cffa955a04f/iresolve.py#L136-L141,"def output(results, output_format='pretty'): if output_format == 'pretty': for u, meta in results.items(): print('* {} can be imported from: {}'.format(u, ', '.join(meta['paths']))) elif output_format == 'json': print(json.dumps(results))",1
708,Python,postgresql connection,https://github.com/gabfl/dbschema/blob/37722e6654e9f0374fac5518ebdca22f4c39f92f/src/schema_change.py#L81-L91,"def get_connection(engine, host, user, port, password, database, ssl={}): """""" Returns a PostgreSQL or MySQL connection """""" if engine == 'mysql': # Connection return get_mysql_connection(host, user, port, password, database, ssl) elif engine == 'postgresql': # Connection return get_pg_connection(host, user, port, password, database, ssl) else: raise RuntimeError('`%s` is not a valid engine.' % engine)",3
256,Python,postgresql connection,https://github.com/Riffstation/flask-philo/blob/76c9d562edb4a77010c8da6dfdb6489fa29cbc9e/flask_philo/db/postgresql/connection.py#L58-L102,"def initialize(g, app): """""" If postgresql url is defined in configuration params a scoped session will be created """""" if 'DATABASES' in app.config and 'POSTGRESQL' in app.config['DATABASES']: # Database connection established for console commands for k, v in app.config['DATABASES']['POSTGRESQL'].items(): init_db_conn(k, v) if 'test' not in sys.argv: # Establish a new connection every request @app.before_request def before_request(): """""" Assign postgresql connection pool to the global flask object at the beginning of every request """""" # inject stack context if not testing from flask import _app_ctx_stack for k, v in app.config['DATABASES']['POSTGRESQL'].items(): init_db_conn(k, v, scopefunc=_app_ctx_stack) g.postgresql_pool = pool # avoid to close connections if testing @app.teardown_request def teardown_request(exception): """""" Releasing connection after finish request, not required in unit testing """""" pool = getattr(g, 'postgresql_pool', None) if pool is not None: for k, v in pool.connections.items(): v.session.remove() else: @app.before_request def before_request(): """""" Assign postgresql connection pool to the global flask object at the beginning of every request """""" for k, v in app.config['DATABASES']['POSTGRESQL'].items(): init_db_conn(k, v) g.postgresql_pool = pool",2
863,Python,postgresql connection,https://github.com/Azure/azure-cli-extensions/blob/3d4854205b0f0d882f688cfa12383d14506c2e35/src/db-up/azext_db_up/custom.py#L381-L394,"def _run_postgresql_commands(host, user, password, database): # Connect to postgresql and get cursor to run sql commands connection = psycopg2.connect(user=user, host=host, password=password, database=database) connection.set_session(autocommit=True) logger.warning('Successfully Connected to PostgreSQL.') cursor = connection.cursor() try: db_password = _create_db_password(database) cursor.execute(""CREATE USER root WITH ENCRYPTED PASSWORD '{}'"".format(db_password)) logger.warning(""Ran Database Query: `CREATE USER root WITH ENCRYPTED PASSWORD '%s'`"", db_password) except psycopg2.ProgrammingError: pass cursor.execute(""GRANT ALL PRIVILEGES ON DATABASE {} TO root"".format(database)) logger.warning(""Ran Database Query: `GRANT ALL PRIVILEGES ON DATABASE %s TO root`"", database)",2
1478,Python,postgresql connection,https://github.com/django-extensions/django-extensions/blob/7e0bef97ea6cb7f9eea5e2528e3a985a83a7b9b8/django_extensions/management/commands/sqldsn.py#L100-L141,"def _postgresql(self, dbhost, dbport, dbname, dbuser, dbpass, dsn_style=None): # noqa """"""PostgreSQL psycopg2 driver accepts two syntaxes Plus a string for .pgpass file """""" dsn = [] if dsn_style is None or dsn_style == 'all' or dsn_style == 'keyvalue': dsnstr = ""host='{0}' dbname='{2}' user='{3}' password='{4}'"" if dbport is not None: dsnstr += "" port='{1}'"" dsn.append(dsnstr.format(dbhost, dbport, dbname, dbuser, dbpass,)) if dsn_style == 'all' or dsn_style == 'kwargs': dsnstr = ""host='{0}', database='{2}', user='{3}', password='{4}'"" if dbport is not None: dsnstr += "", port='{1}'"" dsn.append(dsnstr.format(dbhost, dbport, dbname, dbuser, dbpass)) if dsn_style == 'all' or dsn_style == 'uri': dsnstr = ""postgresql://{user}:{password}@{host}/{name}"" dsn.append(dsnstr.format( host=""{host}:{port}"".format(host=dbhost, port=dbport) if dbport else dbhost, # noqa name=dbname, user=dbuser, password=dbpass)) if dsn_style == 'all' or dsn_style == 'pgpass': dsn.append(':'.join(map(str, filter( None, [dbhost, dbport, dbname, dbuser, dbpass])))) return dsn",2
1664,Python,postgresql connection,https://github.com/datajoint/datajoint-python/blob/4f29bb154a7ed2b8b64b4d3a9c8be4c16b39621c/datajoint/connection.py#L20-L44,"def conn(host=None, user=None, password=None, init_fun=None, reset=False): """""" Returns a persistent connection object to be shared by multiple modules. If the connection is not yet established or reset=True, a new connection is set up. If connection information is not provided, it is taken from config which takes the information from dj_local_conf.json. If the password is not specified in that file datajoint prompts for the password. :param host: hostname :param user: mysql user :param password: mysql password :param init_fun: initialization function :param reset: whether the connection should be reset or not """""" if not hasattr(conn, 'connection') or reset: host = host if host is not None else config['database.host'] user = user if user is not None else config['database.user'] password = password if password is not None else config['database.password'] if user is None: # pragma: no cover user = input(""Please enter DataJoint username: "") if password is None: # pragma: no cover password = getpass(prompt=""Please enter DataJoint password: "") init_fun = init_fun if init_fun is not None else config['connection.init_function'] conn.connection = Connection(host, user, password, init_fun) return conn.connection",2
1933,Python,postgresql connection,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/mssql_hook.py#L38-L49,"def get_conn(self): """""" Returns a mssql connection object """""" conn = self.get_connection(self.mssql_conn_id) conn = pymssql.connect( server=conn.host, user=conn.login, password=conn.password, database=self.schema or conn.schema, port=conn.port) return conn",2
192,Python,postgresql connection,https://github.com/Fizzadar/pyinfra/blob/006f751f7db2e07d32522c0285160783de2feb79/pyinfra/modules/postgresql.py#L48-L123,"def role( state, host, name, present=True, password=None, login=True, superuser=False, inherit=False, createdb=False, createrole=False, replication=False, connection_limit=None, # Details for speaking to PostgreSQL via `psql` CLI postgresql_user=None, postgresql_password=None, postgresql_host=None, postgresql_port=None, ): ''' Add/remove PostgreSQL roles. + name: name of the role + present: whether the role should be present or absent + login: whether the role can login + superuser: whether role will be a superuser + inherit: whether the role inherits from other roles + createdb: whether the role is allowed to create databases + createrole: whether the role is allowed to create new roles + replication: whether this role is allowed to replicate + connection_limit: the connection limit for the role + postgresql_*: global module arguments, see above Updates: pyinfra will not attempt to change existing roles - it will either create or drop roles, but not alter them (if the role exists this operation will make no changes). ''' roles = host.fact.postgresql_roles( postgresql_user, postgresql_password, postgresql_host, postgresql_port, ) is_present = name in roles # User not wanted? if not present: if is_present: yield make_execute_psql_command( 'DROP ROLE {0}'.format(name), user=postgresql_user, password=postgresql_password, host=postgresql_host, port=postgresql_port, ) return # If we want the user and they don't exist if not is_present: sql_bits = ['CREATE ROLE {0}'.format(name)] for key, value in ( ('LOGIN', login), ('SUPERUSER', superuser), ('INHERIT', inherit), ('CREATEDB', createdb), ('CREATEROLE', createrole), ('REPLICATION', replication), ): if value: sql_bits.append(key) if connection_limit: sql_bits.append('CONNECTION LIMIT {0}'.format(connection_limit)) if password: sql_bits.append(""PASSWORD '{0}'"".format(password)) yield make_execute_psql_command( ' '.join(sql_bits), user=postgresql_user, password=postgresql_password, host=postgresql_host, port=postgresql_port, )",1
270,Python,postgresql connection,https://github.com/CivicSpleen/ambry/blob/d7f2be4bf1f7ffd086f3fadd4fcae60c32473e42/ambry/orm/database.py#L785-L793,"def migrate(self, connection): # use transactions if connection.engine.name == 'sqlite': self._migrate_sqlite(connection) elif connection.engine.name == 'postgresql': self._migrate_postgresql(connection) else: raise DatabaseMissingError( 'Do not know how to migrate {} engine.'.format(self.connection))",1
406,Python,postgresql connection,https://github.com/nickw444/flask-ldap3-login/blob/3cf0faff52d0e04d4813119a2ba36d706e6fb31f/flask_ldap3_login/__init__.py#L705-L736,"def connection(self): """""" Convenience property for externally accessing an authenticated connection to the server. This connection is automatically handled by the appcontext, so you do not have to perform an unbind. Returns: ldap3.Connection: A bound ldap3.Connection Raises: ldap3.core.exceptions.LDAPException: Since this method is performing a bind on behalf of the caller. You should handle this case occuring, such as invalid service credentials. """""" ctx = stack.top if ctx is None: raise Exception(""Working outside of the Flask application "" ""context. If you wish to make a connection outside of a flask"" "" application context, please handle your connections "" ""and use manager.make_connection()"") if hasattr(ctx, 'ldap3_manager_main_connection'): return ctx.ldap3_manager_main_connection else: connection = self._make_connection( bind_user=self.config.get('LDAP_BIND_USER_DN'), bind_password=self.config.get('LDAP_BIND_USER_PASSWORD'), contextualise=False ) connection.bind() if ctx is not None: ctx.ldap3_manager_main_connection = connection return connection",1
82,Python,positions of substrings in string,https://github.com/SpheMakh/Stimela/blob/292e80461a0c3498da8e7e987e2891d3ae5981ad/stimela/utils/__init__.py#L321-L329,"def substitute_globals(string, globs=None): sub = set(re.findall('\{(.*?)\}', string)) globs = globs or inspect.currentframe().f_back.f_globals if sub: for item in map(str, sub): string = string.replace(""${%s}""%item, globs[item]) return string else: return False",3
138,Python,positions of substrings in string,https://github.com/ChrisCummins/labm8/blob/dd10d67a757aefb180cb508f86696f99440c94f5/text.py#L40-L52,"def get_substring_idxs(substr, string): """""" Return a list of indexes of substr. If substr not found, list is empty. Arguments: substr (str): Substring to match. string (str): String to match in. Returns: list of int: Start indices of substr. """""" return [match.start() for match in re.finditer(substr, string)]",3
1014,Python,positions of substrings in string,https://github.com/hollenstein/maspy/blob/f15fcfd24df306d8420540460d902aa3073ec133/maspy/auxiliary.py#L471-L491,"def findAllSubstrings(string, substring): """""" Returns a list of all substring starting positions in string or an empty list if substring is not present in string. :param string: a template string :param substring: a string, which is looked for in the ``string`` parameter. :returns: a list of substring starting positions in the template string """""" #TODO: solve with regex? what about '.': #return [m.start() for m in re.finditer('(?='+substring+')', string)] start = 0 positions = [] while True: start = string.find(substring, start) if start == -1: break positions.append(start) #+1 instead of +len(substring) to also find overlapping matches start += 1 return positions",3
1069,Python,positions of substrings in string,https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/expr/api.py#L1943-L1962,"def _string_substr(self, start, length=None): """""" Pull substrings out of each string value by position and maximum length. Parameters ---------- start : int First character to start splitting, indices starting at 0 (like Python) length : int, optional Maximum length of each substring. If not supplied, splits each string to the end Returns ------- substrings : type of caller """""" op = ops.Substring(self, start, length) return op.to_expr()",3
912,Python,positions of substrings in string,https://github.com/mcs07/ChemDataExtractor/blob/349a3bea965f2073141d62043b89319222e46af1/chemdataextractor/text/processors.py#L91-L95,"def __call__(self, value): for substring in self.substrings: if value.startswith(substring): return value[len(substring):] return value",2
949,Python,positions of substrings in string,https://github.com/cfusting/fastgp/blob/6cf3c5d14abedaea064feef6ca434ee806a11756/fastgp/algorithms/evolutionary_feature_synthesis.py#L314-L332,"def build_operation_stack(string): stack = [] start = 0 for i, s in enumerate(string): if s == '(': substring = string[start:i] start = i + 1 stack.append(substring) elif s == ',': if i != start: substring = string[start:i] stack.append(substring) start = i + 1 elif s == ')': if i != start: substring = string[start:i] stack.append(substring) start = i + 1 return stack",2
974,Python,positions of substrings in string,https://github.com/samuelcolvin/buildpg/blob/33cccff45279834d02ec7e97d8417da8fd2a875d/buildpg/funcs.py#L84-L85,"def position(substring, string): return logic.Func('position', logic.SqlBlock(substring).in_(string))",2
686,Python,positions of substrings in string,https://github.com/hydpy-dev/hydpy/blob/1bc6a82cf30786521d86b36e27900c6717d3348d/hydpy/core/selectiontools.py#L659-L721,"def search_nodenames(self, *substrings: str, name: str = 'nodenames') -> \ 'Selection': """"""Return a new selection containing all nodes of the current selection with a name containing at least one of the given substrings. >>> from hydpy.core.examples import prepare_full_example_2 >>> hp, pub, _ = prepare_full_example_2() Pass the (sub)strings as positional arguments and, optionally, the name of the newly created |Selection| object as a keyword argument: >>> test = pub.selections.complete.copy('test') >>> from hydpy import prepare_model >>> test.search_nodenames('dill', 'lahn_1') Selection(""nodenames"", nodes=(""dill"", ""lahn_1""), elements=()) Wrong string specifications result in errors like the following: >>> test.search_nodenames(['dill', 'lahn_1']) Traceback (most recent call last): ... TypeError: While trying to determine the nodes of selection \ `test` with names containing at least one of the given substrings \ `['dill', 'lahn_1']`, the following error occurred: 'in <string>' \ requires string as left operand, not list Method |Selection.select_nodenames| restricts the current selection to the one determined with the the method |Selection.search_nodenames|: >>> test.select_nodenames('dill', 'lahn_1') Selection(""test"", nodes=(""dill"", ""lahn_1""), elements=(""land_dill"", ""land_lahn_1"", ""land_lahn_2"", ""land_lahn_3"", ""stream_dill_lahn_2"", ""stream_lahn_1_lahn_2"", ""stream_lahn_2_lahn_3"")) On the contrary, the method |Selection.deselect_nodenames| restricts the current selection to all devices not determined by the method |Selection.search_nodenames|: >>> pub.selections.complete.deselect_nodenames('dill', 'lahn_1') Selection(""complete"", nodes=(""lahn_2"", ""lahn_3""), elements=(""land_dill"", ""land_lahn_1"", ""land_lahn_2"", ""land_lahn_3"", ""stream_dill_lahn_2"", ""stream_lahn_1_lahn_2"", ""stream_lahn_2_lahn_3"")) """""" try: selection = Selection(name) for node in self.nodes: for substring in substrings: if substring in node.name: selection.nodes += node break return selection except BaseException: values = objecttools.enumeration(substrings) objecttools.augment_excmessage( f'While trying to determine the nodes of selection ' f'`{self.name}` with names containing at least one ' f'of the given substrings `{values}`')",1
759,Python,positions of substrings in string,https://github.com/alvations/lazyme/blob/961a8282198588ff72e15643f725ce895e51d06d/lazyme/string.py#L45-L53,"def deduplicate(s, ch): """""" From http://stackoverflow.com/q/42216559/610569 s = 'this is an irritating string with random spacing .' deduplicate(s) 'this is an irritating string with random spacing .' """""" return ch.join([substring for substring in s.strip().split(ch) if substring])",1
999,Python,positions of substrings in string,https://github.com/mcs07/ChemDataExtractor/blob/349a3bea965f2073141d62043b89319222e46af1/chemdataextractor/text/processors.py#L104-L108,"def __call__(self, value): for substring in self.substrings: if value.endswith(substring): return value[:-len(substring)] return value",1
11,Python,parse query string in url,https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/strings/strip_url_params.py#L85-L95,"def strip_url_params3(url, strip=None): if not strip: strip = [] parse = urllib.parse.urlparse(url) query = urllib.parse.parse_qs(parse.query) query = {k: v[0] for k, v in query.items() if k not in strip} query = urllib.parse.urlencode(query) new = parse._replace(query=query) return new.geturl()",3
281,Python,parse query string in url,https://github.com/TeamHG-Memex/MaybeDont/blob/34721f67b69d426adda324a0ed905d3860828af9/maybedont/predict.py#L238-L241,"def _parse_url(url): p = urlsplit(url) query = {k: v[0] for k, v in parse_qs(p.query).items() if len(v) == 1} return ''.join([p.netloc, p.path]), query",3
1261,Python,parse query string in url,https://github.com/zeromake/aiko/blob/53b246fa88652466a9e38ac3d1a99a6198195b0f/aiko/request.py#L329-L336,"def url(self) -> str: """""" path + query 的url """""" url_str = self.parse_url.path or """" if self.parse_url.querystring is not None: url_str += ""?"" + self.parse_url.querystring return url_str",3
1278,Python,parse query string in url,https://github.com/knipknap/exscript/blob/72718eee3e87b345d5a5255be9824e867e42927b/Exscript/util/url.py#L72-L103,"def _urlparse_qs(url): """""" Parse a URL query string and return the components as a dictionary. Based on the cgi.parse_qs method.This is a utility function provided with urlparse so that users need not use cgi module for parsing the url query string. Arguments: :type url: str :param url: URL with query string to be parsed """""" # Extract the query part from the URL. querystring = urlparse(url)[4] # Split the query into name/value pairs. pairs = [s2 for s1 in querystring.split('&') for s2 in s1.split(';')] # Split the name/value pairs. result = OrderedDefaultDict(list) for name_value in pairs: pair = name_value.split('=', 1) if len(pair) != 2: continue if len(pair[1]) > 0: name = _unquote(pair[0].replace('+', ' ')) value = _unquote(pair[1].replace('+', ' ')) result[name].append(value) return result",3
1397,Python,parse query string in url,https://github.com/googleapis/google-auth-library-python/blob/2c6ad78917e936f38f87c946209c8031166dc96e/google/auth/_helpers.py#L130-L173,"def update_query(url, params, remove=None): """"""Updates a URL's query parameters. Replaces any current values if they are already present in the URL. Args: url (str): The URL to update. params (Mapping[str, str]): A mapping of query parameter keys to values. remove (Sequence[str]): Parameters to remove from the query string. Returns: str: The URL with updated query parameters. Examples: >>> url = 'http://example.com?a=1' >>> update_query(url, {'a': '2'}) http://example.com?a=2 >>> update_query(url, {'b': '3'}) http://example.com?a=1&b=3 >> update_query(url, {'b': '3'}, remove=['a']) http://example.com?b=3 """""" if remove is None: remove = [] # Split the URL into parts. parts = urllib.parse.urlparse(url) # Parse the query string. query_params = urllib.parse.parse_qs(parts.query) # Update the query parameters with the new parameters. query_params.update(params) # Remove any values specified in remove. query_params = { key: value for key, value in six.iteritems(query_params) if key not in remove} # Re-encoded the query string. new_query = urllib.parse.urlencode(query_params, doseq=True) # Unsplit the url. new_parts = parts._replace(query=new_query) return urllib.parse.urlunparse(new_parts)",3
580,Python,parse query string in url,https://github.com/ModisWorks/modis/blob/1f1225c9841835ec1d1831fc196306527567db8b/modis/discord_modis/modules/music/api_music.py#L94-L234,"def parse_query(query, ilogger): """""" Gets either a list of videos from a query, parsing links and search queries and playlists Args: query (str): The YouTube search query ilogger (logging.logger): The logger to log API calls to Returns: queue (list): The items obtained from the YouTube search """""" # Try parsing this as a link p = urlparse(query) if p and p.scheme and p.netloc: if ""youtube"" in p.netloc and p.query and ytdiscoveryapi is not None: query_parts = p.query.split('&') yturl_parts = {} for q in query_parts: s = q.split('=') if len(s) < 2: continue q_name = s[0] q_val = '='.join(s[1:]) # Add to the query if q_name not in yturl_parts: yturl_parts[q_name] = q_val if ""list"" in yturl_parts: ilogger.info(""Queued YouTube playlist from link"") return get_queue_from_playlist(yturl_parts[""list""]) elif ""v"" in yturl_parts: ilogger.info(""Queued YouTube video from link"") return [[""https://www.youtube.com/watch?v={}"".format(yturl_parts[""v""]), query]] elif ""soundcloud"" in p.netloc: if scclient is None: ilogger.error(""Could not queue from SoundCloud API, using link"") return [[query, query]] try: result = scclient.get('/resolve', url=query) track_list = [] if isinstance(result, ResourceList): for r in result.data: tracks = get_sc_tracks(r) if tracks is not None: for t in tracks: track_list.append(t) elif isinstance(result, Resource): tracks = get_sc_tracks(result) if tracks is not None: for t in tracks: track_list.append(t) if track_list is not None and len(track_list) > 0: ilogger.info(""Queued SoundCloud songs from link"") return track_list else: ilogger.error(""Could not queue from SoundCloud API"") return [[query, query]] except Exception as e: logger.exception(e) ilogger.error(""Could not queue from SoundCloud API, using link"") return [[query, query]] else: ilogger.debug(""Using url: {}"".format(query)) return [[query, query]] args = query.split(' ') if len(args) == 0: ilogger.error(""No query given"") return [] if args[0].lower() in [""sp"", ""spotify""] and spclient is not None: if spclient is None: ilogger.error(""Host does not support Spotify"") return [] try: if len(args) > 2 and args[1] in ['album', 'artist', 'song', 'track', 'playlist']: query_type = args[1].lower() query_search = ' '.join(args[2:]) else: query_type = 'track' query_search = ' '.join(args[1:]) query_type = query_type.replace('song', 'track') ilogger.info(""Queueing Spotify {}: {}"".format(query_type, query_search)) spotify_tracks = search_sp_tracks(query_type, query_search) if spotify_tracks is None or len(spotify_tracks) == 0: ilogger.error(""Could not queue Spotify {}: {}"".format(query_type, query_search)) return [] ilogger.info(""Queued Spotify {}: {}"".format(query_type, query_search)) return spotify_tracks except Exception as e: logger.exception(e) ilogger.error(""Error queueing from Spotify"") return [] elif args[0].lower() in [""sc"", ""soundcloud""]: if scclient is None: ilogger.error(""Host does not support SoundCloud"") return [] try: requests = ['song', 'songs', 'track', 'tracks', 'user', 'playlist', 'tagged', 'genre'] if len(args) > 2 and args[1] in requests: query_type = args[1].lower() query_search = ' '.join(args[2:]) else: query_type = 'track' query_search = ' '.join(args[1:]) query_type = query_type.replace('song', 'track') ilogger.info(""Queueing SoundCloud {}: {}"".format(query_type, query_search)) soundcloud_tracks = search_sc_tracks(query_type, query_search) ilogger.info(""Queued SoundCloud {}: {}"".format(query_type, query_search)) return soundcloud_tracks except Exception as e: logger.exception(e) ilogger.error(""Could not queue from SoundCloud"") return [] elif args[0].lower() in [""yt"", ""youtube""] and ytdiscoveryapi is not None: if ytdiscoveryapi is None: ilogger.error(""Host does not support YouTube"") return [] try: query_search = ' '.join(args[1:]) ilogger.info(""Queued Youtube search: {}"".format(query_search)) return get_ytvideos(query_search, ilogger) except Exception as e: logger.exception(e) ilogger.error(""Could not queue YouTube search"") return [] if ytdiscoveryapi is not None: ilogger.info(""Queued YouTube search: {}"".format(query)) return get_ytvideos(query, ilogger) else: ilogger.error(""Host does not support YouTube"".format(query)) return []",2
583,Python,parse query string in url,https://github.com/thomasjiangcy/django-rest-mock/blob/09e91de20d1a5efd5c47c6e3d7fe979443012e2c/rest_mock_server/core/parser.py#L43-L52,"def _parse_url(url_string): u = urlparse(url_string) url = u.path query = parse_qs(u.query) return { 'full_url': url_string.strip(), 'url': url.strip(), 'query': query }",2
1313,Python,parse query string in url,https://github.com/jdp/urp/blob/778c16d9a5eae75316ce20aad742af7122be558c/urp.py#L17-L22,"def parse(args, data): url = urlparse(data) query = url.query if not args.no_query_params: query = parse_qsl(url.query) return url, query",2
1489,Python,parse query string in url,https://github.com/wummel/linkchecker/blob/c2ce810c3fb00b895a841a7be6b2e78c64e7b042/linkcheck/url.py#L250-L272,"def url_parse_query (query, encoding=None): """"""Parse and re-join the given CGI query."""""" if isinstance(query, unicode): if encoding is None: encoding = url_encoding query = query.encode(encoding, 'ignore') # if ? is in the query, split it off, seen at msdn.microsoft.com append = """" while '?' in query: query, rest = query.rsplit('?', 1) append = '?'+url_parse_query(rest)+append l = [] for k, v, sep in parse_qsl(query, keep_blank_values=True): k = url_quote_part(k, '/-:,;') if v: v = url_quote_part(v, '/-:,;') l.append(""%s=%s%s"" % (k, v, sep)) elif v is None: l.append(""%s%s"" % (k, sep)) else: # some sites do not work when the equal sign is missing l.append(""%s=%s"" % (k, sep)) return ''.join(l) + append",2
1204,Python,parse query string in url,https://github.com/vfxetc/sgactions/blob/e2c6ee14a1f8092e97f57a501650581428a6ac6e/sgactions/dispatch.py#L14-L33,"def parse_url(url): # Parse the URL into scheme, path, and query. m = re.match(r'^(?:(\w+):)?(.*?)(?:/(.*?))?(?:\?(.*))?$', url) scheme, netloc, path, query = m.groups() kwargs = urlparse.parse_qs(query, keep_blank_values=True) if query else {} for k, v in kwargs.iteritems(): if len(v) == 1 and k not in ('cols', 'column_display_names'): kwargs[k] = v = v[0] if k.endswith('_id') and v.isdigit(): kwargs[k] = int(v) # Parse the path into an entrypoint. m = re.match(r'^([\w.]+:\w+)$', netloc) if not m: raise ValueError('entrypoint must be like ""package.module:function""; got ""%s""' % netloc) return 1 return m.group(1), kwargs",1
38,Python,parse json file,https://github.com/flatironinstitute/kbucket/blob/867915ebb0ea153a399c3e392698f89bf43c7903/kbucket/kbucketclient.py#L639-L645,def _read_json_file(path): try: with open(path) as f: return json.load(f) except: print ('Warning: Unable to read or parse json file: '+path) return None,3
99,Python,parse json file,https://github.com/s1s1ty/py-jsonq/blob/9625597a2578bddcbed4e540174d5253b1fc3b75/pyjsonq/query.py#L46-L60,"def __parse_json_file(self, file_path): """"""Process Json file data :@param file_path :@type file_path: string :@throws IOError """""" if file_path == '' or os.path.splitext(file_path)[1] != '.json': raise IOError('Invalid Json file') with open(file_path) as json_file: self._raw_data = json.load(json_file) self._json_data = copy.deepcopy(self._raw_data)",3
493,Python,parse json file,https://github.com/ewels/MultiQC/blob/2037d6322b2554146a74efbf869156ad20d4c4ec/multiqc/modules/bcl2fastq/bcl2fastq.py#L142-L268,"def parse_file_as_json(self, myfile): try: content = json.loads(myfile[""f""]) except ValueError: log.warn('Could not parse file as json: {}'.format(myfile[""fn""])) return runId = content[""RunId""] if runId not in self.bcl2fastq_data: self.bcl2fastq_data[runId] = dict() run_data = self.bcl2fastq_data[runId] for conversionResult in content.get(""ConversionResults"", []): l = conversionResult[""LaneNumber""] lane = 'L{}'.format(conversionResult[""LaneNumber""]) if lane in run_data: log.debug(""Duplicate runId/lane combination found! Overwriting: {}"".format(self.prepend_runid(runId, lane))) run_data[lane] = { ""total"": 0, ""total_yield"": 0, ""perfectIndex"": 0, ""samples"": dict(), ""yieldQ30"": 0, ""qscore_sum"": 0 } # simplify the population of dictionaries rlane = run_data[lane] # Add undetermined barcodes try: unknown_barcode = content['UnknownBarcodes'][l - 1]['Barcodes'] except IndexError: unknown_barcode = next( (item['Barcodes'] for item in content['UnknownBarcodes'] if item['Lane'] == 8), None ) run_data[lane]['unknown_barcodes'] = unknown_barcode for demuxResult in conversionResult.get(""DemuxResults"", []): if demuxResult[""SampleName""] == demuxResult[""SampleName""]: sample = demuxResult[""SampleName""] else: sample = ""{}-{}"".format(demuxResult[""SampleId""], demuxResult[""SampleName""]) if sample in run_data[lane][""samples""]: log.debug(""Duplicate runId/lane/sample combination found! Overwriting: {}, {}"".format(self.prepend_runid(runId, lane), sample)) run_data[lane][""samples""][sample] = { ""total"": 0, ""total_yield"": 0, ""perfectIndex"": 0, ""filename"": os.path.join(myfile['root'], myfile[""fn""]), ""yieldQ30"": 0, ""qscore_sum"": 0 } # simplify the population of dictionaries lsample = run_data[lane][""samples""][sample] for r in range(1,5): lsample[""R{}_yield"".format(r)] = 0 lsample[""R{}_Q30"".format(r)] = 0 lsample[""R{}_trimmed_bases"".format(r)] = 0 rlane[""total""] += demuxResult[""NumberReads""] rlane[""total_yield""] += demuxResult[""Yield""] lsample[""total""] += demuxResult[""NumberReads""] lsample[""total_yield""] += demuxResult[""Yield""] for indexMetric in demuxResult.get(""IndexMetrics"", []): rlane[""perfectIndex""] += indexMetric[""MismatchCounts""][""0""] lsample[""perfectIndex""] += indexMetric[""MismatchCounts""][""0""] for readMetric in demuxResult.get(""ReadMetrics"", []): r = readMetric[""ReadNumber""] rlane[""yieldQ30""] += readMetric[""YieldQ30""] rlane[""qscore_sum""] += readMetric[""QualityScoreSum""] lsample[""yieldQ30""] += readMetric[""YieldQ30""] lsample[""qscore_sum""] += readMetric[""QualityScoreSum""] lsample[""R{}_yield"".format(r)] += readMetric[""Yield""] lsample[""R{}_Q30"".format(r)] += readMetric[""YieldQ30""] lsample[""R{}_trimmed_bases"".format(r)] += readMetric[""TrimmedBases""] # Remove unpopulated read keys for r in range(1,5): if not lsample[""R{}_yield"".format(r)] and not lsample[""R{}_Q30"".format(r)] and not lsample[""R{}_trimmed_bases"".format(r)]: lsample.pop(""R{}_yield"".format(r)) lsample.pop(""R{}_Q30"".format(r)) lsample.pop(""R{}_trimmed_bases"".format(r)) undeterminedYieldQ30 = 0 undeterminedQscoreSum = 0 undeterminedTrimmedBases = 0 if ""Undetermined"" in conversionResult: for readMetric in conversionResult[""Undetermined""][""ReadMetrics""]: undeterminedYieldQ30 += readMetric[""YieldQ30""] undeterminedQscoreSum += readMetric[""QualityScoreSum""] undeterminedTrimmedBases += readMetric[""TrimmedBases""] run_data[lane][""samples""][""undetermined""] = { ""total"": conversionResult[""Undetermined""][""NumberReads""], ""total_yield"": conversionResult[""Undetermined""][""Yield""], ""perfectIndex"": 0, ""yieldQ30"": undeterminedYieldQ30, ""qscore_sum"": undeterminedQscoreSum, ""trimmed_bases"": undeterminedTrimmedBases } # Calculate Percents and averages for lane_id, lane in run_data.items(): try: lane[""percent_Q30""] = (float(lane[""yieldQ30""]) / float(lane[""total_yield""])) * 100.0 except ZeroDivisionError: lane[""percent_Q30""] = ""NA"" try: lane[""percent_perfectIndex""] = (float(lane[""perfectIndex""]) / float(lane[""total""])) * 100.0 except ZeroDivisionError: lane[""percent_perfectIndex""] = ""NA"" try: lane[""mean_qscore""] = float(lane[""qscore_sum""]) / float(lane[""total_yield""]) except ZeroDivisionError: lane[""mean_qscore""] = ""NA"" for sample_id, sample in lane[""samples""].items(): try: sample[""percent_Q30""] = (float(sample[""yieldQ30""]) / float(sample[""total_yield""])) * 100.0 except ZeroDivisionError: sample[""percent_Q30""] = ""NA"" try: sample[""percent_perfectIndex""] = (float(sample[""perfectIndex""]) / float(sample[""total""])) * 100.0 except ZeroDivisionError: sample[""percent_perfectIndex""] = ""NA"" try: sample[""mean_qscore""] = float(sample[""qscore_sum""]) / float(sample[""total_yield""]) except ZeroDivisionError: sample[""mean_qscore""] = ""NA""",2
817,Python,parse json file,https://github.com/danhper/python-i18n/blob/bbba4b7ec091997ea8df2067acd7af316ee00b31/i18n/loaders/json_loader.py#L10-L14,"def parse_file(self, file_content): try: return json.loads(file_content) except ValueError as e: raise I18nFileLoadError(""invalid JSON: {0}"".format(e.strerror))",2
982,Python,parse json file,https://github.com/lark-parser/lark/blob/a798dec77907e74520dd7e90c7b6a4acc680633a/examples/error_reporting_lalr.py#L32-L58,"def parse(json_text): try: j = json_parser.parse(json_text) except UnexpectedInput as u: exc_class = u.match_examples(json_parser.parse, { JsonMissingOpening: ['{""foo"": ]}', '{""foor"": }}', '{""foo"": }'], JsonMissingClosing: ['{""foo"": [}', '{', '{""a"": 1', '[1'], JsonMissingComma: ['[1 2]', '[false 1]', '[""b"" 1]', '{""a"":true 1:4}', '{""a"":1 1:4}', '{""a"":""b"" 1:4}'], JsonTrailingComma: ['[,]', '[1,]', '[1,2,]', '{""foo"":1,}', '{""foo"":false,""bar"":true,}'] }) if not exc_class: raise raise exc_class(u.get_context(json_text), u.line, u.column)",2
1026,Python,parse json file,https://github.com/ray-project/ray/blob/4eade036a0505e244c976f36aaa2d64386b5129b/python/ray/tune/automlboard/common/utils.py#L58-L92,"def parse_multiple_json(json_file, offset=None): """"""Parse multiple json records from the given file. Seek to the offset as the start point before parsing if offset set. return empty list if the json file does not exists or exception occurs. Args: json_file (str): File path to be parsed. offset (int): Initial seek position of the file. Returns: A dict of json info. New offset after parsing. """""" json_info_list = [] if not os.path.exists(json_file): return json_info_list try: with open(json_file, ""r"") as f: if offset: f.seek(offset) for line in f: if line[-1] != ""\n"": # Incomplete line break json_info = json.loads(line) json_info_list.append(json_info) offset += len(line) except BaseException as e: logging.error(e.message) return json_info_list, offset",2
1149,Python,parse json file,https://github.com/release-engineering/productmd/blob/49256bf2e8c84124f42346241140b986ad7bfc38/productmd/common.py#L302-L315,"def parse_file(self, f): # parse file, return parser or dict with data if hasattr(f, ""seekable""): if f.seekable(): f.seek(0) elif hasattr(f, ""seek""): f.seek(0) if six.PY3 and isinstance(f, six.moves.http_client.HTTPResponse): # HTTPResponse needs special handling in py3 reader = codecs.getreader(""utf-8"") parser = json.load(reader(f)) else: parser = json.load(f) return parser",2
1598,Python,parse json file,https://github.com/nkmathew/yasi-sexp-indenter/blob/6ec2a4675e79606c555bcb67494a0ba994b05805/yasi.py#L551-L568,"def parse_rc_json(): """""" Reads the json configuration file(.yasirc.json), parses it and returns the dictionary """""" fname = '.yasirc.json' path = os.path.expanduser('~/' + fname) if os.path.exists(fname): path = os.path.abspath(fname) elif not os.path.exists(path): path = '' content = '' if path: with open(path) as f: content = f.read() ret = {} if content: ret = json.loads(content) return collections.defaultdict(dict, ret)",2
248,Python,parse json file,https://github.com/rehive/rehive-python/blob/a7452a9cfecf76c5c8f0d443f122ed22167fb164/rehive/api/resources/base_resources.py#L22-L46,"def post(self, data=None, function=None, idempotent_key=None, **kwargs): # Allow us to parse through arbitrary request arguments if data is None: data = {} request_kwargs = {} if 'file' in kwargs: request_kwargs['files'] = { 'file': (open(kwargs.get('file'), 'rb')) } kwargs.pop('file', None) # We need this flag to force non-json on file uploads json = True if 'json' in kwargs: json = kwargs.get('json') kwargs.pop('json', None) data = {**data, **kwargs} url = self._build_url(function) response = self.client.post( url, data, json=json, idempotent_key=idempotent_key, **request_kwargs ) return self._handle_resource_data(response)",0
183,Python,parse command line argument,https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/rotate_poscar.py#L7-L13,"def parse_command_line_arguments(): parser = argparse.ArgumentParser( description='Rotates the cell lattice in VASP POSCAR files' ) parser.add_argument( 'poscar', help=""filename of the VASP POSCAR to be processed"" ) parser.add_argument( '-a', '--axis', nargs=3, type=float, help=""vector for rotation axis"", required=True ) parser.add_argument( '-d', '--degrees', type=int, help=""rotation angle in degrees"", required=True ) args = parser.parse_args() return( args )",3
536,Python,parse command line argument,https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/check_species.py#L14-L19,"def parse_command_line_arguments(): parser = argparse.ArgumentParser( description='Check species consistency between a VASP POSCAR file and a POTCAR file.' ) parser.add_argument( 'poscar', help=""filename of the VASP POSCAR to be processed"", nargs='?', default='POSCAR' ) parser.add_argument( 'potcar', help=""filename of the VASP POTCAR to be processed"", nargs='?', default='POTCAR' ) parser.add_argument( '-p', '--ppset', help=""check whether the POTCAR pseudopotentials belong to a specific pseudopotential set"", choices=potcar_sets ) return parser.parse_args()",3
662,Python,parse command line argument,https://github.com/intuition-io/intuition/blob/cd517e6b3b315a743eb4d0d0dc294e264ab913ce/intuition/core/configuration.py#L28-L56,"def parse_commandline(): parser = argparse.ArgumentParser( description='Intuition, the terrific trading system') parser.add_argument('-V', '--version', action='version', version='%(prog)s v{} Licence {}'.format( __version__, __licence__), help='Print program version') parser.add_argument('-v', '--showlog', action='store_true', help='Print logs on stdout') parser.add_argument('-b', '--bot', action='store_true', help='Allows the algorithm to process orders') parser.add_argument('-c', '--context', action='store', default='file::conf.yaml', help='Provides the way to build context') parser.add_argument('-i', '--id', action='store', default='gekko', help='Customize the session id') args = parser.parse_args() # Dict will be more generic to process than args namespace return { 'session': args.id, 'context': args.context, 'showlog': args.showlog, 'bot': args.bot }",3
1004,Python,parse command line argument,https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/xdatcar_to_disp.py#L7-L12,def parse_command_line_arguments(): # command line arguments parser = argparse.ArgumentParser() parser.add_argument( 'xdatcar' ) args = parser.parse_args() return( args ),3
1024,Python,parse command line argument,https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/proc_poscar.py#L7-L21,"def parse_command_line_arguments(): # command line arguments parser = argparse.ArgumentParser( description='Manipulates VASP POSCAR files' ) parser.add_argument( 'poscar', help=""filename of the VASP POSCAR to be processed"" ) parser.add_argument( '-l', '--label', type=int, choices=[ 1, 4 ], help=""label coordinates with atom name at position {1,4}"" ) parser.add_argument( '-c', '--coordinates-only', help='only output coordinates', action='store_true' ) parser.add_argument( '-t', '--coordinate-type', type=str, choices=[ 'c', 'cartesian', 'd', 'direct' ], default='direct', help=""specify coordinate type for output {(c)artesian|(d)irect} [default = (d)irect]"" ) parser.add_argument( '-g', '--group', help='group atoms within supercell', action='store_true' ) parser.add_argument( '-s', '--supercell', type=int, nargs=3, metavar=( 'h', 'k', 'l' ), help='construct supercell by replicating (h,k,l) times along [a b c]' ) parser.add_argument( '-b', '--bohr', action='store_true', help='assumes the input file is in Angstrom, and converts everything to bohr') parser.add_argument( '-n', '--number-atoms', action='store_true', help='label coordinates with atom number' ) parser.add_argument( '--scale', action='store_true', help='scale the lattice parameters by the scaling factor' ) parser.add_argument( '--selective', choices=[ 'T', 'F' ], help='generate Selective Dynamics POSCAR with all values set to T / F' ) args = parser.parse_args() return( args )",3
1383,Python,parse command line argument,https://github.com/bjmorgan/vasppy/blob/cc2d1449697b17ee1c43715a02cddcb1139a6834/vasppy/scripts/xdatcar_to_rdf.py#L9-L17,"def parse_command_line_arguments(): # command line arguments parser = argparse.ArgumentParser() parser.add_argument( 'xdatcar' ) parser.add_argument( 'label', nargs = 2 ) parser.add_argument( 'max_r', type = float ) parser.add_argument( 'n_bins', type = int ) args = parser.parse_args() return( args )",3
1571,Python,parse command line argument,https://github.com/chitamoor/Rester/blob/1865b17f70b7c597aeadde2d0907cb1b59f10c0f/rester/apirunner.py#L9-L18,"def parse_cmdln_args(): parser = argparse.ArgumentParser(description='Process command line args') parser.add_argument('--log', help='log help', default='INFO') parser.add_argument( '--tc', help='tc help') parser.add_argument( '--ts', help='ts help') args = parser.parse_args() return (args.log.upper(), args.tc, args.ts)",3
191,Python,parse command line argument,https://github.com/mozilla/Marketplace.Python/blob/88176b12201f766b6b96bccc1e4c3e82f0676283/example/main.py#L29-L61,"def main(): parser = argparse.ArgumentParser( description='Command line Marketplace client') parser.add_argument('method', type=str, help='command to be run on arguments', choices=COMMANDS.keys()) parser.add_argument('attrs', metavar='attr', type=str, nargs='*', help='command arguments') parser.add_argument('-v', action='store_true', default=False, dest='verbose', help='Switch to verbose mode') args = parser.parse_args() client = marketplace.Client( domain=config.MARKETPLACE_DOMAIN, protocol=config.MARKETPLACE_PROTOCOL, port=config.MARKETPLACE_PORT, consumer_key=config.CONSUMER_KEY, consumer_secret=config.CONSUMER_SECRET) if args.verbose: logger.setLevel(logging.DEBUG) if args.attrs: result = COMMANDS[args.method](client, *args.attrs) else: result = COMMANDS[args.method](client) if result['success']: sys.stdout.write('%s\n' % result['message']) else: sys.stderr.write('%s\n' % result['message']) sys.exit(1)",2
1411,Python,parse command line argument,https://github.com/diffeo/yakonfig/blob/412e195da29b4f4fc7b72967c192714a6f5eaeb5/yakonfig/cmd.py#L132-L148,"def parseline(self, line): cmd, arg, line = Cmd.parseline(self, line) if cmd and cmd.strip() != '': dof = getattr(self, 'do_' + cmd, None) argf = getattr(self, 'args_' + cmd, None) else: argf = None if argf: parser = argparse.ArgumentParser( prog=cmd, description=getattr(dof, '__doc__', None)) argf(parser) try: arg = parser.parse_args(shlex.split(arg)) except SystemExit, e: return '', '', '' return cmd, arg, line",2
630,Python,parse command line argument,https://github.com/clarkperkins/click-shell/blob/8d6e1a492176bc79e029d714f19d3156409656ea/click_shell/core.py#L82-L102,"def get_complete(command): """""" Get the Cmd complete function for the click command :param command: The click Command object :return: the complete_* method for Cmd :rtype: function """""" assert isinstance(command, click.Command) def complete_(self, text, line, begidx, endidx): # pylint: disable=unused-argument # Parse the args args = shlex.split(line[:begidx]) # Strip of the first item which is the name of the command args = args[1:] # Then pass them on to the get_choices method that click uses for completion return list(get_choices(command, command.name, args, text)) complete_.__name__ = 'complete_%s' % command.name return complete_",0
359,Python,parse binary file to custom class,https://github.com/mapillary/mapillary_tools/blob/816785e90c589cae6e8e34a5530ce8417d29591c/mapillary_tools/gpmf.py#L72-L133,"def parse_bin(path): f = open(path, 'rb') s = {} # the current Scale data to apply to next requester output = [] # handlers for various fourCC codes methods = { 'GPS5': parse_gps, 'GPSU': parse_time, 'GPSF': parse_fix, 'GPSP': parse_precision, 'ACCL': parse_accl, 'GYRO': parse_gyro, } d = {'gps': []} # up to date dictionary, iterate and fill then flush while True: label = f.read(4) if not label: # eof break desc = f.read(4) # null length if '00' == binascii.hexlify(desc[0]): continue val_size = struct.unpack('>b', desc[1])[0] num_values = struct.unpack('>h', desc[2:4])[0] length = val_size * num_values # print ""{} {} of size {} and type {}"".format(num_values, label, # val_size, desc[0]) if label == 'DVID': if len(d['gps']): # first one is empty output.append(d) d = {'gps': []} # reset for i in range(num_values): data = f.read(val_size) if label in methods: methods[label](data, d, s) if label == 'SCAL': if 2 == val_size: s[i] = struct.unpack('>h', data)[0] elif 4 == val_size: s[i] = struct.unpack('>i', data)[0] else: raise Exception('unknown scal size') # pack mod = length % 4 if mod != 0: seek = 4 - mod f.read(seek) # discarded return output",3
652,Python,parse binary file to custom class,https://github.com/python-xlib/python-xlib/blob/8901e831737e79fe5645f48089d70e1d1046d2f2/Xlib/ext/xinput.py#L382-L387,"def parse_binary(self, data, display): class_type, length = struct.unpack('=HH', data[:4]) class_struct = INFO_CLASSES.get(class_type, AnyInfo) class_data, _ = class_struct.parse_binary(data, display) data = data[length * 4:] return class_data, data",3
1617,Python,parse binary file to custom class,https://github.com/fossasia/knittingpattern/blob/8e608896b0ab82fea1ca9fbfa2b4ee023d8c8027/knittingpattern/Dumper/file.py#L109-L114,"def binary_file(self, file=None): """"""Same as :meth:`file` but for binary content."""""" if file is None: file = BytesIO() self._binary_file(file) return file",3
1641,Python,parse binary file to custom class,https://github.com/python-xlib/python-xlib/blob/8901e831737e79fe5645f48089d70e1d1046d2f2/Xlib/protocol/rq.py#L1135-L1210,"def parse_binary(self, data, display, rawdict = 0): """"""values, remdata = s.parse_binary(data, display, rawdict = 0) Convert a binary representation of the structure into Python values. DATA is a string or a buffer containing the binary data. DISPLAY should be a Xlib.protocol.display.Display object if there are any Resource fields or Lists with ResourceObjs. The Python values are returned as VALUES. If RAWDICT is true, a Python dictionary is returned, where the keys are field names and the values are the corresponding Python value. If RAWDICT is false, a DictWrapper will be returned where all fields are available as attributes. REMDATA are the remaining binary data, unused by the Struct object. """""" ret = {} val = struct.unpack(self.static_codes, data[:self.static_size]) lengths = {} formats = {} vno = 0 for f in self.static_fields: # Fields without name should be ignored. This is typically # pad and constant fields if not f.name: pass # Store index in val for Length and Format fields, to be used # when treating varfields. elif isinstance(f, LengthField): f_names = [f.name] if f.other_fields: f_names.extend(f.other_fields) field_val = val[vno] if f.parse_value is not None: field_val = f.parse_value(field_val, display) for f_name in f_names: lengths[f_name] = field_val elif isinstance(f, FormatField): formats[f.name] = val[vno] # Treat value fields the same was as in parse_value. else: if f.structvalues == 1: field_val = val[vno] else: field_val = val[vno:vno+f.structvalues] if f.parse_value is not None: field_val = f.parse_value(field_val, display) ret[f.name] = field_val vno = vno + f.structvalues data = data[self.static_size:] # Call parse_binary_value for each var_field, passing the # length and format values from the unpacked val. for f in self.var_fields: ret[f.name], data = f.parse_binary_value(data, display, lengths.get(f.name), formats.get(f.name), ) if not rawdict: ret = DictWrapper(ret) return ret, data",3
1718,Python,parse binary file to custom class,https://github.com/jldantas/libmft/blob/65a988605fe7663b788bd81dcb52c0a4eaad1549/libmft/attribute.py#L1599-L1630,"def _from_binary_reparse(cls, binary_stream): """"""See base class."""""" ''' Reparse type flags - 4 Reparse tag - 4 bits Reserved - 12 bits Reparse type - 2 bits Reparse data length - 2 Padding - 2 ''' #content = cls._REPR.unpack(binary_view[:cls._REPR.size]) reparse_tag, data_len = cls._REPR.unpack(binary_stream[:cls._REPR.size]) #reparse_tag (type, flags) data_len, guid, data reparse_type = ReparseType(reparse_tag & 0x0000FFFF) reparse_flags = ReparseFlags((reparse_tag & 0xF0000000) >> 28) guid = None #guid exists only in third party reparse points if reparse_flags & ReparseFlags.IS_MICROSOFT:#a microsoft tag if reparse_type is ReparseType.SYMLINK: data = SymbolicLink.create_from_binary(binary_stream[cls._REPR.size:]) elif reparse_type is ReparseType.MOUNT_POINT: data = JunctionOrMount.create_from_binary(binary_stream[cls._REPR.size:]) else: data = binary_stream[cls._REPR.size:].tobytes() else: guid = UUID(bytes_le=binary_stream[cls._REPR.size:cls._REPR.size+16].tobytes()) data = binary_stream[cls._REPR.size+16:].tobytes() nw_obj = cls((reparse_type, reparse_flags, data_len, guid, data)) _MOD_LOGGER.debug(""Attempted to unpack REPARSE_POINT from \""%s\""\nResult: %s"", binary_stream.tobytes(), nw_obj) return nw_obj",3
127,Python,parse binary file to custom class,https://github.com/log2timeline/plaso/blob/9c564698d2da3ffbe23607a3c54c0582ea18a6cc/plaso/parsers/custom_destinations.py#L85-L209,"def ParseFileObject(self, parser_mediator, file_object): """"""Parses a .customDestinations-ms file-like object. Args: parser_mediator (ParserMediator): mediates interactions between parsers and other components, such as storage and dfvfs. file_object (dfvfs.FileIO): a file-like object. Raises: UnableToParseFile: when the file cannot be parsed. """""" file_entry = parser_mediator.GetFileEntry() display_name = parser_mediator.GetDisplayName() file_header_map = self._GetDataTypeMap('custom_file_header') try: file_header, file_offset = self._ReadStructureFromFileObject( file_object, 0, file_header_map) except (ValueError, errors.ParseError) as exception: raise errors.UnableToParseFile(( 'Invalid Custom Destination: {0:s} - unable to parse file header ' 'with error: {1!s}').format(display_name, exception)) if file_header.unknown1 != 2: raise errors.UnableToParseFile(( 'Unsupported Custom Destination file: {0:s} - invalid unknown1: ' '{1:d}.').format(display_name, file_header.unknown1)) if file_header.header_values_type > 2: raise errors.UnableToParseFile(( 'Unsupported Custom Destination file: {0:s} - invalid header value ' 'type: {1:d}.').format(display_name, file_header.header_values_type)) if file_header.header_values_type == 0: data_map_name = 'custom_file_header_value_type_0' else: data_map_name = 'custom_file_header_value_type_1_or_2' file_header_value_map = self._GetDataTypeMap(data_map_name) try: _, value_data_size = self._ReadStructureFromFileObject( file_object, file_offset, file_header_value_map) except (ValueError, errors.ParseError) as exception: raise errors.UnableToParseFile(( 'Invalid Custom Destination: {0:s} - unable to parse file header ' 'value with error: {1!s}').format(display_name, exception)) file_offset += value_data_size file_size = file_object.get_size() remaining_file_size = file_size - file_offset entry_header_map = self._GetDataTypeMap('custom_entry_header') file_footer_map = self._GetDataTypeMap('custom_file_footer') # The Custom Destination file does not have a unique signature in # the file header that is why we use the first LNK class identifier (GUID) # as a signature. first_guid_checked = False while remaining_file_size > 4: try: entry_header, entry_data_size = self._ReadStructureFromFileObject( file_object, file_offset, entry_header_map) except (ValueError, errors.ParseError) as exception: if not first_guid_checked: raise errors.UnableToParseFile(( 'Invalid Custom Destination file: {0:s} - unable to parse ' 'entry header with error: {1!s}').format( display_name, exception)) parser_mediator.ProduceExtractionWarning( 'unable to parse entry header with error: {0!s}'.format( exception)) break if entry_header.guid != self._LNK_GUID: if not first_guid_checked: raise errors.UnableToParseFile(( 'Unsupported Custom Destination file: {0:s} - invalid entry ' 'header signature offset: 0x{1:08x}.').format( display_name, file_offset)) try: # Check if we found the footer instead of an entry header. file_footer, _ = self._ReadStructureFromFileObject( file_object, file_offset, file_footer_map) if file_footer.signature != self._FILE_FOOTER_SIGNATURE: parser_mediator.ProduceExtractionWarning( 'invalid entry header signature at offset: 0x{0:08x}'.format( file_offset)) except (ValueError, errors.ParseError) as exception: parser_mediator.ProduceExtractionWarning(( 'unable to parse footer at offset: 0x{0:08x} with error: ' '{1!s}').format(file_offset, exception)) break # TODO: add support for Jump List LNK file recovery. break first_guid_checked = True file_offset += entry_data_size remaining_file_size -= entry_data_size lnk_file_size = self._ParseLNKFile( parser_mediator, file_entry, file_offset, remaining_file_size) file_offset += lnk_file_size remaining_file_size -= lnk_file_size try: file_footer, _ = self._ReadStructureFromFileObject( file_object, file_offset, file_footer_map) if file_footer.signature != self._FILE_FOOTER_SIGNATURE: parser_mediator.ProduceExtractionWarning( 'invalid footer signature at offset: 0x{0:08x}'.format(file_offset)) except (ValueError, errors.ParseError) as exception: parser_mediator.ProduceExtractionWarning(( 'unable to parse footer at offset: 0x{0:08x} with error: ' '{1!s}').format(file_offset, exception))",2
271,Python,parse binary file to custom class,https://github.com/census-instrumentation/opencensus-python/blob/992b223f7e34c5dcb65922b7d5c827e7a1351e7d/opencensus/trace/propagation/binary_format.py#L95-L136,"def from_header(self, binary): """"""Generate a SpanContext object using the trace context header. The value of enabled parsed from header is int. Need to convert to bool. :type binary: bytes :param binary: Trace context header which was extracted from the request headers. :rtype: :class:`~opencensus.trace.span_context.SpanContext` :returns: SpanContext generated from the trace context header. """""" # If no binary provided, generate a new SpanContext if binary is None: return span_context_module.SpanContext(from_header=False) # If cannot parse, return a new SpanContext and ignore the context # from binary. try: data = Header._make(struct.unpack(BINARY_FORMAT, binary)) except struct.error: logging.warning( 'Cannot parse the incoming binary data {}, ' 'wrong format. Total bytes length should be {}.'.format( binary, FORMAT_LENGTH ) ) return span_context_module.SpanContext(from_header=False) # data.trace_id is in bytes with length 16, hexlify it to hex bytes # with length 32, then decode it to hex string using utf-8. trace_id = str(binascii.hexlify(data.trace_id).decode(UTF8)) span_id = str(binascii.hexlify(data.span_id).decode(UTF8)) trace_options = TraceOptions(data.trace_option) span_context = span_context_module.SpanContext( trace_id=trace_id, span_id=span_id, trace_options=trace_options, from_header=True) return span_context",2
134,Python,parse binary file to custom class,https://github.com/iclab/centinel/blob/9a25dcf30c6a1db3c046f7ccb8ab8873e455c1a4/centinel/cli.py#L114-L201,"def _run(): """"""Entry point for package and cli uses"""""" args = parse_args() # parse custom parameters custom_meta = None if args.custom_meta: print ""Adding custom parameters:"" custom_meta = {} try: for item in args.custom_meta.split(','): key, value = item.split(':') custom_meta[key] = value print 'key: %s, value: %s' % (key, value) except Exception as e: sys.stderr.write(""ERROR: Can not parse custom meta tags! %s\n"" % (str(e))) # we need to store some persistent info, so check if a config file # exists (default location is ~/.centinel/config.ini). If the file # does not exist, then create a new one at run time configuration = centinel.config.Configuration() if args.config: configuration.parse_config(args.config) else: # if the file does not exist, then the default config file # will be used new_configuration = None if os.path.exists(DEFAULT_CONFIG_FILE): configuration.parse_config(DEFAULT_CONFIG_FILE) else: print 'Configuration file does not exist. Creating a new one.' new_configuration = centinel.config.Configuration() if not ('version' in configuration.params and configuration.params['version']['version'] == centinel.__version__): if not args.update_config: print ('WARNING: configuration file is from ' 'a different version (%s) of ' 'Centinel. Run with --update-config to update ' 'it.' % (configuration.params['version']['version'])) else: new_configuration = centinel.config.Configuration() backup_path = DEFAULT_CONFIG_FILE + "".old"" new_configuration.update(configuration, backup_path) if new_configuration is not None: configuration = new_configuration configuration.write_out_config(DEFAULT_CONFIG_FILE) print 'New configuration written to %s' % (DEFAULT_CONFIG_FILE) if args.update_config: sys.exit(0) if args.verbose: if 'log' not in configuration.params: configuration.params['log'] = dict() configuration.params['log']['log_level'] = logging.DEBUG # add custom meta values from CLI if custom_meta is not None: if 'custom_meta' in configuration.params: configuration.params['custom_meta'].update(custom_meta) else: configuration.params['custom_meta'] = custom_meta centinel.conf = configuration.params client = centinel.client.Client(configuration.params) client.setup_logging() # disable cert verification if the flag is set if args.no_verify: configuration.params['server']['verify'] = False user = centinel.backend.User(configuration.params) # Note: because we have mutually exclusive arguments, we don't # have to worry about multiple arguments being called if args.sync: centinel.backend.sync(configuration.params) elif args.consent: user.informed_consent() elif args.daemonize: # if we don't have a valid binary location, then exit if not os.path.exists(args.binary): print ""Error: no binary found to daemonize"" exit(1) centinel.daemonize.daemonize(args.auto_update, args.binary, args.user) else: client.run()",0
1163,Python,parse binary file to custom class,https://github.com/williballenthin/python-evtx/blob/4e9e29544adde64c79ff9b743269ecb18c677eb4/Evtx/BinaryParser.py#L121-L128,"def __init__(self, value): """""" Constructor. Arguments: - `value`: A string description. """""" super(BinaryParserException, self).__init__() self._value = value",0
1243,Python,parse binary file to custom class,https://github.com/sci-bots/pygtkhelpers/blob/3a6e6d6340221c686229cd1c951d7537dae81b07/pygtkhelpers/debug/console.py#L645-L668,"def ConsoleType(t=gtk.TextView): class console_type(t, _Console): __gsignals__ = { 'command': (gobject.SIGNAL_RUN_LAST, gobject.TYPE_NONE, (object, )), 'key-press-event': 'override'} def __init__(self, *args, **kwargs): if gtk.pygtk_version[1] < 8: gobject.GObject.__init__(self) else: t.__init__(self) _Console.__init__(self, *args, **kwargs) def do_command(self, code): return _Console.do_command(self, code) def do_key_press_event(self, event): return _Console.do_key_press_event(self, event, t) if gtk.pygtk_version[1] < 8: gobject.type_register(console_type) return console_type",0
402,Python,output to html file,https://github.com/pandas-profiling/pandas-profiling/blob/003d236daee8b7aca39c62708b18d59bced0bc03/pandas_profiling/__init__.py#L106-L122,"def to_file(self, outputfile=DEFAULT_OUTPUTFILE): """"""Write the report to a file. By default a name is generated. Parameters: ---------- outputfile : str The name or the path of the file to generale including the extension (.html). """""" if outputfile != NO_OUTPUTFILE: if outputfile == DEFAULT_OUTPUTFILE: outputfile = 'profile_' + str(hash(self)) + "".html"" # TODO: should be done in the template with codecs.open(outputfile, 'w+b', encoding='utf8') as self.file: self.file.write(templates.template('wrapper').render(content=self.html))",3
551,Python,output to html file,https://github.com/dnanexus/dx-toolkit/blob/74befb53ad90fcf902d8983ae6d74580f402d619/doc/examples/dx-apps/report_example/src/report_example.py#L127-L137,"def generate_html(): """""" Generate an HTML file incorporating the images produced by this script """""" html_file = open(html_filename, ""w"") html_file.write(""<html><body>"") html_file.write(""<h1>Here are some graphs for you!</h1>"") for image in [lines_filename, bars_filename, histogram_filename]: html_file.write(""<div><h2>{0}</h2><img src='{0}' /></div>"".format(image)) html_file.write(""</body></html>"") html_file.close()",3
1059,Python,output to html file,https://github.com/swharden/SWHLab/blob/a86c3c65323cec809a4bd4f81919644927094bf5/swhlab/indexing/indexing.py#L239-L256,"def html_index(self,launch=True): html=""<h1>MENU</h1>"" htmlFiles=[x for x in self.files2 if x.endswith("".html"")] for htmlFile in cm.abfSort(htmlFiles): if not htmlFile.endswith('_basic.html'): continue name=htmlFile.split(""_"")[0] if name in self.groups.keys(): html+='<a href=""%s"" target=""content"">%s</a> '%(htmlFile,name) html+='<span style=""color: #CCC;"">' html+='[<a href=""%s"" target=""content"">int</a>]'%(name+""_plot.html"") html+='</span>' html+='<br>' style.save(html,os.path.abspath(self.folder2+""/index_menu.html"")) html=""<h1>SPLASH</h1>"" style.save(html,os.path.abspath(self.folder2+""/index_splash.html"")) style.frames(os.path.abspath(self.folder2+""/index.html""),launch=launch) return",3
1765,Python,output to html file,https://github.com/explosion/spaCy/blob/8ee4100f8ffb336886208a1ea827bf4c745e2709/examples/pipeline/custom_attr_methods.py#L43-L58,"def to_html(doc, output=""/tmp"", style=""dep""): """"""Doc method extension for saving the current state as a displaCy visualization. """""" # generate filename from first six non-punct tokens file_name = ""-"".join([w.text for w in doc[:6] if not w.is_punct]) + "".html"" html = displacy.render(doc, style=style, page=True) # render markup if output is not None: output_path = Path(output) if not output_path.exists(): output_path.mkdir() output_file = Path(output) / file_name output_file.open(""w"", encoding=""utf-8"").write(html) # save to file print(""Saved HTML to {}"".format(output_file)) else: print(html)",3
1870,Python,output to html file,https://github.com/thespacedoctor/polyglot/blob/98038d746aa67e343b73b3ccee1e02d31dab81ec/polyglot/ebook.py#L246-L282,"def _tmp_html_file( self, content): """"""*create a tmp html file with some content used for the header or footer of the ebook* **Key Arguments:** - ``content`` -- the content to include in the HTML file. """""" self.log.debug('starting the ``_tmp_html_file`` method') content = """""" <hr> <div style=""text-align: center""> %(content)s </div> <hr> """""" % locals() now = datetime.now() now = now.strftime(""%Y%m%dt%H%M%S%f"") pathToWriteFile = ""/tmp/%(now)s.html"" % locals() try: self.log.debug(""attempting to open the file %s"" % (pathToWriteFile,)) writeFile = codecs.open( pathToWriteFile, encoding='utf-8', mode='w') except IOError, e: message = 'could not open the file %s' % (pathToWriteFile,) self.log.critical(message) raise IOError(message) writeFile.write(content) writeFile.close() self.log.debug('completed the ``_tmp_html_file`` method') return pathToWriteFile",3
174,Python,output to html file,https://github.com/appknox/vendor/blob/0b85d3c8328c5102cdcb1f619cbff215d7b5f228/ak_vendor/report.py#L31-L34,"def to_html_file(self, path=''): with open('{}/output.html'.format(path), 'w') as file: tpl = self.to_html() file.write(tpl)",2
320,Python,output to html file,https://github.com/lrq3000/pyFileFixity/blob/fd5ef23bb13835faf1e3baa773619b86a1cc9bdf/pyFileFixity/lib/profilers/pyinstrument/profiler.py#L159-L185,"def output_html(self, root=False): resources_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'resources/') with open(os.path.join(resources_dir, 'style.css')) as f: css = f.read() with open(os.path.join(resources_dir, 'profile.js')) as f: js = f.read() with open(os.path.join(resources_dir, 'jquery-1.11.0.min.js')) as f: jquery_js = f.read() body = self.starting_frame(root).as_html() page = ''' <html> <head> <style>{css}</style> <script>{jquery_js}</script> </head> <body> {body} <script>{js}</script> </body> </html>'''.format(css=css, js=js, jquery_js=jquery_js, body=body) return page",2
1789,Python,output to html file,https://github.com/readbeyond/aeneas/blob/9d95535ad63eef4a98530cfdff033b8c35315ee1/aeneas/syncmap/__init__.py#L309-L368,"def output_html_for_tuning( self, audio_file_path, output_file_path, parameters=None ): """""" Output an HTML file for fine tuning the sync map manually. :param string audio_file_path: the path to the associated audio file :param string output_file_path: the path to the output file to write :param dict parameters: additional parameters .. versionadded:: 1.3.1 """""" if not gf.file_can_be_written(output_file_path): self.log_exc(u""Cannot output HTML file '%s'. Wrong permissions?"" % (output_file_path), None, True, OSError) if parameters is None: parameters = {} audio_file_path_absolute = gf.fix_slash(os.path.abspath(audio_file_path)) template_path_absolute = gf.absolute_path(self.FINETUNEAS_PATH, __file__) with io.open(template_path_absolute, ""r"", encoding=""utf-8"") as file_obj: template = file_obj.read() for repl in self.FINETUNEAS_REPLACEMENTS: template = template.replace(repl[0], repl[1]) template = template.replace( self.FINETUNEAS_REPLACE_AUDIOFILEPATH, u""audioFilePath = \""file://%s\"";"" % audio_file_path_absolute ) template = template.replace( self.FINETUNEAS_REPLACE_FRAGMENTS, u""fragments = (%s).fragments;"" % self.json_string ) if gc.PPN_TASK_OS_FILE_FORMAT in parameters: output_format = parameters[gc.PPN_TASK_OS_FILE_FORMAT] if output_format in self.FINETUNEAS_ALLOWED_FORMATS: template = template.replace( self.FINETUNEAS_REPLACE_OUTPUT_FORMAT, u""outputFormat = \""%s\"";"" % output_format ) if output_format == ""smil"": for key, placeholder, replacement in [ ( gc.PPN_TASK_OS_FILE_SMIL_AUDIO_REF, self.FINETUNEAS_REPLACE_SMIL_AUDIOREF, ""audioref = \""%s\"";"" ), ( gc.PPN_TASK_OS_FILE_SMIL_PAGE_REF, self.FINETUNEAS_REPLACE_SMIL_PAGEREF, ""pageref = \""%s\"";"" ), ]: if key in parameters: template = template.replace( placeholder, replacement % parameters[key] ) with io.open(output_file_path, ""w"", encoding=""utf-8"") as file_obj: file_obj.write(template)",2
893,Python,output to html file,https://github.com/pyecharts/pyecharts/blob/02050acb0e94bb9453b88a25028de7a0ce23f125/pyecharts/commons/utils.py#L39-L41,"def write_utf8_html_file(file_name, html_content): with open(file_name, ""w+"", encoding=""utf-8"") as html_file: html_file.write(html_content)",1
1463,Python,output to html file,https://github.com/openvax/topiary/blob/04f0077bc4bf1ad350a0e78c26fa48c55fe7813b/topiary/cli/outputs.py#L23-L61,"def add_output_args(arg_parser): output_group = arg_parser.add_argument_group( title=""Output"", description=""How and where to write results"") output_group.add_argument( ""--output-csv"", default=None, help=""Path to output CSV file"") output_group.add_argument( ""--output-html"", default=None, help=""Path to output HTML file"") output_group.add_argument( ""--output-csv-sep"", default="","", help=""Separator for CSV file"") output_group.add_argument( ""--subset-output-columns"", nargs=""*"") output_group.add_argument( ""--rename-output-column"", nargs=2, action=""append"", help=( ""Rename original column (first parameter) to new"" "" name (second parameter)"")) output_group.add_argument( ""--print-columns"", default=False, action=""store_true"", help=""Print columns before writing data to file(s)"") return output_group",0
514,Python,normal distribution,https://github.com/Microsoft/nni/blob/c7cc8db32da8d2ec77a382a55089f4e17247ce41/src/sdk/pynni/nni/curvefitting_assessor/model_factory.py#L204-L221,"def normal_distribution(self, pos, sample): """"""returns the value of normal distribution, given the weight's sample and target position Parameters ---------- pos: int the epoch number of the position you want to predict sample: list sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk} Returns ------- float the value of normal distribution """""" curr_sigma_sq = self.sigma_sq(sample) delta = self.trial_history[pos - 1] - self.f_comb(pos, sample) return np.exp(np.square(delta) / (-2.0 * curr_sigma_sq)) / np.sqrt(2 * np.pi * np.sqrt(curr_sigma_sq))",3
1098,Python,normal distribution,https://github.com/janpipek/physt/blob/6dd441b073514e7728235f50b2352d56aacf38d4/physt/examples/__init__.py#L12-L22,"def normal_h1(size: int = 10000, mean: float = 0, sigma: float = 1) -> Histogram1D: """"""A simple 1D histogram with normal distribution. Parameters ---------- size : Number of points mean : Mean of the distribution sigma : Sigma of the distribution """""" data = np.random.normal(mean, sigma, (size,)) return h1(data, name=""normal"", axis_name=""x"", title=""1D normal distribution"")",3
171,Python,normal distribution,https://github.com/ajyoon/blur/blob/25fcf083af112bb003956a7a7e1c6ff7d8fef279/blur/rand.py#L252-L300,"def normal_distribution(mean, variance, minimum=None, maximum=None, weight_count=23): """""" Return a list of weights approximating a normal distribution. Args: mean (float): The mean of the distribution variance (float): The variance of the distribution minimum (float): The minimum outcome possible to bound the output distribution to maximum (float): The maximum outcome possible to bound the output distribution to weight_count (int): The number of weights that will be used to approximate the distribution Returns: list: a list of ``(float, float)`` weight tuples approximating a normal distribution. Raises: ValueError: ``if maximum < minimum`` TypeError: if both ``minimum`` and ``maximum`` are ``None`` Example: >>> weights = normal_distribution(10, 3, ... minimum=0, maximum=20, ... weight_count=5) >>> rounded_weights = [(round(value, 2), round(strength, 2)) ... for value, strength in weights] >>> rounded_weights [(1.34, 0.0), (4.8, 0.0), (8.27, 0.14), (11.73, 0.14), (15.2, 0.0)] """""" # Pin 0 to +- 5 sigma as bounds, or minimum and maximum # if they cross +/- sigma standard_deviation = math.sqrt(variance) min_x = (standard_deviation * -5) + mean max_x = (standard_deviation * 5) + mean step = (max_x - min_x) / weight_count current_x = min_x weights = [] while current_x < max_x: weights.append( (current_x, _normal_function(current_x, mean, variance)) ) current_x += step if minimum is not None or maximum is not None: return bound_weights(weights, minimum, maximum) else: return weights",2
820,Python,normal distribution,https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/distributions/normal.py#L241-L261,"def _kl_normal_normal(n_a, n_b, name=None): """"""Calculate the batched KL divergence KL(n_a || n_b) with n_a and n_b Normal. Args: n_a: instance of a Normal distribution object. n_b: instance of a Normal distribution object. name: (optional) Name to use for created operations. default is ""kl_normal_normal"". Returns: Batchwise KL(n_a || n_b) """""" with tf.name_scope(name or ""kl_normal_normal""): one = tf.constant(1, dtype=n_a.dtype) two = tf.constant(2, dtype=n_a.dtype) half = tf.constant(0.5, dtype=n_a.dtype) s_a_squared = tf.square(n_a.scale) s_b_squared = tf.square(n_b.scale) ratio = s_a_squared / s_b_squared return (tf.square(n_a.loc - n_b.loc) / (two * s_b_squared) + half * (ratio - one - tf.math.log(ratio)))",2
883,Python,normal distribution,https://github.com/tensorforce/tensorforce/blob/520a8d992230e382f08e315ede5fc477f5e26bfb/tensorforce/core/distributions/gaussian.py#L83-L93,"def tf_sample(self, distr_params, deterministic): mean, stddev, _ = distr_params # Deterministic: mean as action definite = mean # Non-deterministic: sample action using default normal distribution normal_distribution = tf.random_normal(shape=tf.shape(input=mean)) sampled = mean + stddev * normal_distribution return tf.where(condition=deterministic, x=definite, y=sampled)",2
1682,Python,normal distribution,https://github.com/tensorflow/tensorboard/blob/8e5f497b48e40f2a774f85416b8a35ac0693c35e/tensorboard/plugins/histogram/histograms_demo.py#L32-L103,"def run_all(logdir, verbose=False, num_summaries=400): """"""Generate a bunch of histogram data, and write it to logdir."""""" del verbose tf.compat.v1.set_random_seed(0) k = tf.compat.v1.placeholder(tf.float32) # Make a normal distribution, with a shifting mean mean_moving_normal = tf.random.normal(shape=[1000], mean=(5*k), stddev=1) # Record that distribution into a histogram summary histogram_summary.op(""normal/moving_mean"", mean_moving_normal, description=""A normal distribution whose mean changes "" ""over time."") # Make a normal distribution with shrinking variance shrinking_normal = tf.random.normal(shape=[1000], mean=0, stddev=1-(k)) # Record that distribution too histogram_summary.op(""normal/shrinking_variance"", shrinking_normal, description=""A normal distribution whose variance "" ""shrinks over time."") # Let's combine both of those distributions into one dataset normal_combined = tf.concat([mean_moving_normal, shrinking_normal], 0) # We add another histogram summary to record the combined distribution histogram_summary.op(""normal/bimodal"", normal_combined, description=""A combination of two normal distributions, "" ""one with a moving mean and one with "" ""shrinking variance. The result is a "" ""distribution that starts as unimodal and "" ""becomes more and more bimodal over time."") # Add a gamma distribution gamma = tf.random.gamma(shape=[1000], alpha=k) histogram_summary.op(""gamma"", gamma, description=""A gamma distribution whose shape "" ""parameter, α, changes over time."") # And a poisson distribution poisson = tf.compat.v1.random_poisson(shape=[1000], lam=k) histogram_summary.op(""poisson"", poisson, description=""A Poisson distribution, which only "" ""takes on integer values."") # And a uniform distribution uniform = tf.random.uniform(shape=[1000], maxval=k*10) histogram_summary.op(""uniform"", uniform, description=""A simple uniform distribution."") # Finally, combine everything together! all_distributions = [mean_moving_normal, shrinking_normal, gamma, poisson, uniform] all_combined = tf.concat(all_distributions, 0) histogram_summary.op(""all_combined"", all_combined, description=""An amalgamation of five distributions: a "" ""uniform distribution, a gamma "" ""distribution, a Poisson distribution, and "" ""two normal distributions."") summaries = tf.compat.v1.summary.merge_all() # Setup a session and summary writer sess = tf.compat.v1.Session() writer = tf.summary.FileWriter(logdir) # Setup a loop and write the summaries to disk N = num_summaries for step in xrange(N): k_val = step/float(N) summ = sess.run(summaries, feed_dict={k: k_val}) writer.add_summary(summ, global_step=step)",2
299,Python,normal distribution,https://github.com/alejoe91/MEAutility/blob/7c2b0da52c2752a3baf04e8e248e26b0769cd088/MEAutility/core.py#L326-L338,"def _set_normal(self, normal): ''' Parameters ---------- electrodes Returns ------- ''' for i, el in enumerate(self.electrodes): el.normal = normal/np.linalg.norm(normal)",1
419,Python,normal distribution,https://github.com/tensorflow/tensor2tensor/blob/272500b6efe353aeb638d2745ed56e519462ca31/tensor2tensor/layers/common_layers.py#L3985-L4004,"def kl_divergence(mu, log_var, mu_p=0.0, log_var_p=0.0): """"""KL divergence of diagonal gaussian N(mu,exp(log_var)) and N(0,1). Args: mu: mu parameter of the distribution. log_var: log(var) parameter of the distribution. mu_p: optional mu from a learned prior distribution log_var_p: optional log(var) from a learned prior distribution Returns: the KL loss. """""" batch_size = shape_list(mu)[0] prior_distribution = tfp.distributions.Normal( mu_p, tf.exp(tf.multiply(0.5, log_var_p))) posterior_distribution = tfp.distributions.Normal( mu, tf.exp(tf.multiply(0.5, log_var))) kld = tfp.distributions.kl_divergence(posterior_distribution, prior_distribution) return tf.reduce_sum(kld) / to_float(batch_size)",1
2018,Python,normal distribution,https://github.com/tensorflow/probability/blob/e87fe34111d68c35db0f9eeb4935f1ece9e1a8f5/tensorflow_probability/python/layers/distribution_layer.py#L1210-L1215,"def __call__(self, distribution_a): # TODO(b/126056144): Remove reacquisition of distribution handle once we # identify how/why Keras lost it. if hasattr(distribution_a, '_tfp_distribution'): distribution_a = distribution_a._tfp_distribution # pylint: disable=protected-access return self._kl_divergence_fn(distribution_a)",1
538,Python,normal distribution,https://github.com/ceph/ceph-deploy/blob/86943fcc454cd4c99a86e3493e9e93a59c661fef/ceph_deploy/hosts/__init__.py#L112-L132,"def _normalized_distro_name(distro): distro = distro.lower() if distro.startswith(('redhat', 'red hat')): return 'redhat' elif distro.startswith(('scientific', 'scientific linux')): return 'scientific' elif distro.startswith('oracle'): return 'oracle' elif distro.startswith(('suse', 'opensuse', 'sles')): return 'suse' elif distro.startswith('centos'): return 'centos' elif distro.startswith('linuxmint'): return 'ubuntu' elif distro.startswith('virtuozzo'): return 'virtuozzo' elif distro.startswith('arch'): return 'arch' elif distro.startswith(('alt', 'altlinux', 'basealt', 'alt linux')): return 'alt' return distro",0
1294,Python,nelder mead optimize,https://github.com/zblz/naima/blob/d6a6781d73bf58fd8269e8b0e3b70be22723cd5b/naima/extern/minimize.py#L66-L67,"def minimize(func, x0, args=(), options={}, method=None): return _minimize_neldermead(func, x0, args=args, **options)",3
754,Python,nelder mead optimize,https://github.com/atztogo/phonopy/blob/869cc2ba9e7d495d5f4cf6942415ab3fc9e2a10f/phonopy/qha/eos.py#L111-L140,"def fit(self, initial_parameters): import sys import logging import warnings try: from scipy.optimize import leastsq import scipy except ImportError: print(""You need to install python-scipy."") sys.exit(1) warnings.filterwarnings('error') def residuals(p, eos, v, e): return eos(v, *p) - e try: result = leastsq(residuals, initial_parameters, args=(self._eos, self._volume, self._energy), full_output=1) except RuntimeError: logging.exception('Fitting to EOS failed.') raise except (RuntimeWarning, scipy.optimize.optimize.OptimizeWarning): logging.exception('Difficulty in fitting to EOS.') raise else: self.parameters = result[0]",2
869,Python,nelder mead optimize,https://github.com/SheffieldML/GPyOpt/blob/255539dc5927819ca701e44fe3d76cd4864222fa/GPyOpt/optimization/optimizer.py#L36-L61,"def optimize(self, x0, f=None, df=None, f_df=None): """""" :param x0: initial point for a local optimizer. :param f: function to optimize. :param df: gradient of the function to optimize. :param f_df: returns both the function to optimize and its gradient. """""" import scipy.optimize if f_df is None and df is not None: f_df = lambda x: float(f(x)), df(x) if f_df is not None: def _f_df(x): return f(x), f_df(x)[1][0] if f_df is None and df is None: res = scipy.optimize.fmin_l_bfgs_b(f, x0=x0, bounds=self.bounds,approx_grad=True, maxiter=self.maxiter) else: res = scipy.optimize.fmin_l_bfgs_b(_f_df, x0=x0, bounds=self.bounds, maxiter=self.maxiter) ### --- We check here if the the optimizer moved. If it didn't we report x0 and f(x0) as scipy can return NaNs if res[2]['task'] == b'ABNORMAL_TERMINATION_IN_LNSRCH': result_x = np.atleast_2d(x0) result_fx = np.atleast_2d(f(x0)) else: result_x = np.atleast_2d(res[0]) result_fx = np.atleast_2d(res[1]) return result_x, result_fx",2
1738,Python,nelder mead optimize,https://github.com/sods/paramz/blob/ae6fc6274b70fb723d91e48fc5026a9bc5a06508/paramz/optimization/scg.py#L44-L174,"def SCG(f, gradf, x, optargs=(), maxiters=500, max_f_eval=np.inf, xtol=None, ftol=None, gtol=None): """""" Optimisation through Scaled Conjugate Gradients (SCG) f: the objective function gradf : the gradient function (should return a 1D np.ndarray) x : the initial condition Returns x the optimal value for x flog : a list of all the objective values function_eval number of fn evaluations status: string describing convergence status """""" if xtol is None: xtol = 1e-6 if ftol is None: ftol = 1e-6 if gtol is None: gtol = 1e-5 sigma0 = 1.0e-7 fold = f(x, *optargs) # Initial function value. function_eval = 1 fnow = fold gradnew = gradf(x, *optargs) # Initial gradient. function_eval += 1 #if any(np.isnan(gradnew)): # raise UnexpectedInfOrNan, ""Gradient contribution resulted in a NaN value"" current_grad = np.dot(gradnew, gradnew) gradold = gradnew.copy() d = -gradnew # Initial search direction. success = True # Force calculation of directional derivs. nsuccess = 0 # nsuccess counts number of successes. beta = 1.0 # Initial scale parameter. betamin = 1.0e-15 # Lower bound on scale. betamax = 1.0e15 # Upper bound on scale. status = ""Not converged"" flog = [fold] iteration = 0 # Main optimization loop. while iteration < maxiters: # Calculate first and second directional derivatives. if success: mu = np.dot(d, gradnew) if mu >= 0: # pragma: no cover d = -gradnew mu = np.dot(d, gradnew) kappa = np.dot(d, d) sigma = sigma0 / np.sqrt(kappa) xplus = x + sigma * d gplus = gradf(xplus, *optargs) function_eval += 1 theta = np.dot(d, (gplus - gradnew)) / sigma # Increase effective curvature and evaluate step size alpha. delta = theta + beta * kappa if delta <= 0: # pragma: no cover delta = beta * kappa beta = beta - theta / kappa alpha = -mu / delta # Calculate the comparison ratio. xnew = x + alpha * d fnew = f(xnew, *optargs) function_eval += 1 Delta = 2.*(fnew - fold) / (alpha * mu) if Delta >= 0.: success = True nsuccess += 1 x = xnew fnow = fnew else: success = False fnow = fold # Store relevant variables flog.append(fnow) # Current function value iteration += 1 if success: # Test for termination if (np.abs(fnew - fold) < ftol): status = 'converged - relative reduction in objective' break # return x, flog, function_eval, status elif (np.max(np.abs(alpha * d)) < xtol): status = 'converged - relative stepsize' break else: # Update variables for new position gradold = gradnew gradnew = gradf(x, *optargs) function_eval += 1 current_grad = np.dot(gradnew, gradnew) fold = fnew # If the gradient is zero then we are done. if current_grad <= gtol: status = 'converged - relative reduction in gradient' break # return x, flog, function_eval, status # Adjust beta according to comparison ratio. if Delta < 0.25: beta = min(4.0 * beta, betamax) if Delta > 0.75: beta = max(0.25 * beta, betamin) # Update search direction using Polak-Ribiere formula, or re-start # in direction of negative gradient after nparams steps. if nsuccess == x.size: d = -gradnew beta = 1. # This is not in the original paper nsuccess = 0 elif success: Gamma = np.dot(gradold - gradnew, gradnew) / (mu) d = Gamma * d - gradnew else: # If we get here, then we haven't terminated in the given number of # iterations. status = ""maxiter exceeded"" return x, flog, function_eval, status",2
2053,Python,nelder mead optimize,https://github.com/SheffieldML/GPyOpt/blob/255539dc5927819ca701e44fe3d76cd4864222fa/GPyOpt/acquisitions/base.py#L52-L60,"def optimize(self, duplicate_manager=None): """""" Optimizes the acquisition function (uses a flag from the model to use gradients or not). """""" if not self.analytical_gradient_acq: out = self.optimizer.optimize(f=self.acquisition_function, duplicate_manager=duplicate_manager) else: out = self.optimizer.optimize(f=self.acquisition_function, f_df=self.acquisition_function_withGradients, duplicate_manager=duplicate_manager) return out",2
939,Python,nelder mead optimize,https://github.com/SheffieldML/GPyOpt/blob/255539dc5927819ca701e44fe3d76cd4864222fa/GPyOpt/util/epmgp.py#L122-L208,"def min_faktor(Mu, Sigma, k, gamma=1): D = Mu.shape[0] logS = np.zeros((D - 1,)) # mean time first moment MP = np.zeros((D - 1,)) # precision, second moment P = np.zeros((D - 1,)) M = np.copy(Mu) V = np.copy(Sigma) b = False d = np.NaN for count in range(50): diff = 0 for i in range(D - 1): l = i if i < k else i + 1 try: M, V, P[i], MP[i], logS[i], d = lt_factor(k, l, M, V, MP[i], P[i], gamma) except Exception as e: raise if np.isnan(d): break diff += np.abs(d) if np.isnan(d): break if np.abs(diff) < 0.001: b = True break if np.isnan(d): logZ = -np.Infinity yield logZ dlogZdMu = np.zeros((D, 1)) yield dlogZdMu dlogZdMudMu = np.zeros((D, D)) yield dlogZdMudMu dlogZdSigma = np.zeros((int(0.5 * (D * (D + 1))), 1)) yield dlogZdSigma mvmin = [Mu[k], Sigma[k, k]] yield mvmin else: # evaluate log Z: C = np.eye(D) / sq2 C[k, :] = -1 / sq2 C = np.delete(C, k, 1) R = np.sqrt(P.T) * C r = np.sum(MP.T * C, 1) mp_not_zero = np.where(MP != 0) mpm = MP[mp_not_zero] * MP[mp_not_zero] / P[mp_not_zero] mpm = sum(mpm) s = sum(logS) IRSR = (np.eye(D - 1) + np.dot(np.dot(R.T, Sigma), R)) rSr = np.dot(np.dot(r.T, Sigma), r) A = np.dot(R, np.linalg.solve(IRSR, R.T)) A = 0.5 * (A.T + A) # ensure symmetry. b = (Mu + np.dot(Sigma, r)) Ab = np.dot(A, b) try: cIRSR = np.linalg.cholesky(IRSR) except np.linalg.LinAlgError: try: cIRSR = np.linalg.cholesky(IRSR + 1e-10 * np.eye(IRSR.shape[0])) except np.linalg.LinAlgError: cIRSR = np.linalg.cholesky(IRSR + 1e-6 * np.eye(IRSR.shape[0])) dts = 2 * np.sum(np.log(np.diagonal(cIRSR))) logZ = 0.5 * (rSr - np.dot(b.T, Ab) - dts) + np.dot(Mu.T, r) + s - 0.5 * mpm yield logZ btA = np.dot(b.T, A) dlogZdMu = r - Ab yield dlogZdMu dlogZdMudMu = -A yield dlogZdMudMu dlogZdSigma = -A - 2 * np.outer(r, Ab.T) + np.outer(r, r.T)\ + np.outer(btA.T, Ab.T) dlogZdSigma2 = np.zeros_like(dlogZdSigma) np.fill_diagonal(dlogZdSigma2, np.diagonal(dlogZdSigma)) dlogZdSigma = 0.5 * (dlogZdSigma + dlogZdSigma.T - dlogZdSigma2) dlogZdSigma = np.rot90(dlogZdSigma, k=2)[np.triu_indices(D)][::-1] yield dlogZdSigma",1
87,Python,nelder mead optimize,https://github.com/apache/incubator-mxnet/blob/1af29e9c060a4c7d60eeaacba32afdb9a7775ba7/example/image-classification/symbols/vgg.py#L27-L35,"def get_feature(internel_layer, layers, filters, batch_norm = False, **kwargs): for i, num in enumerate(layers): for j in range(num): internel_layer = mx.sym.Convolution(data = internel_layer, kernel=(3, 3), pad=(1, 1), num_filter=filters[i], name=""conv%s_%s"" %(i + 1, j + 1)) if batch_norm: internel_layer = mx.symbol.BatchNorm(data=internel_layer, name=""bn%s_%s"" %(i + 1, j + 1)) internel_layer = mx.sym.Activation(data=internel_layer, act_type=""relu"", name=""relu%s_%s"" %(i + 1, j + 1)) internel_layer = mx.sym.Pooling(data=internel_layer, pool_type=""max"", kernel=(2, 2), stride=(2,2), name=""pool%s"" %(i + 1)) return internel_layer",0
545,Python,nelder mead optimize,https://github.com/neherlab/treetime/blob/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0/treetime/merger_models.py#L153-L168,"def optimize_Tc(self): ''' determines the coalescent time scale that optimizes the coalescent likelihood of the tree ''' from scipy.optimize import minimize_scalar initial_Tc = self.Tc def cost(Tc): self.set_Tc(Tc) return -self.total_LH() sol = minimize_scalar(cost, bounds=[ttconf.TINY_NUMBER,10.0]) if ""success"" in sol and sol[""success""]: self.set_Tc(sol['x']) else: self.logger(""merger_models:optimze_Tc: optimization of coalescent time scale failed: "" + str(sol), 0, warn=True) self.set_Tc(initial_Tc.y, T=initial_Tc.x)",0
78,Python,memoize to disk   persistent memoization,https://github.com/aboSamoor/polyglot/blob/d0d2aa8d06cec4e03bd96618ae960030f7069a17/polyglot/decorators.py#L23-L32,"def memoize(obj): cache = obj.cache = {} @functools.wraps(obj) def memoizer(*args, **kwargs): key = tuple(list(args) + sorted(kwargs.items())) if key not in cache: cache[key] = obj(*args, **kwargs) return cache[key] return memoizer",3
1132,Python,memoize to disk   persistent memoization,https://github.com/amperser/proselint/blob/cb619ee4023cc7856f5fb96aec2a33a2c9f1a2e2/proselint/tools.py#L99-L147,"def memoize(f): """"""Cache results of computations on disk."""""" # Determine the location of the cache. cache_dirname = os.path.join(_get_xdg_cache_home(), 'proselint') legacy_cache_dirname = os.path.join(os.path.expanduser(""~""), "".proselint"") if not os.path.isdir(cache_dirname): # Migrate the cache from the legacy path to XDG complaint location. if os.path.isdir(legacy_cache_dirname): os.rename(legacy_cache_dirname, cache_dirname) # Create the cache if it does not already exist. else: os.makedirs(cache_dirname) cache_filename = f.__module__ + ""."" + f.__name__ cachepath = os.path.join(cache_dirname, cache_filename) @functools.wraps(f) def wrapped(*args, **kwargs): # handle instance methods if hasattr(f, '__self__'): args = args[1:] signature = (f.__module__ + '.' + f.__name__).encode(""utf-8"") tempargdict = inspect.getcallargs(f, *args, **kwargs) for item in list(tempargdict.items()): signature += item[1].encode(""utf-8"") key = hashlib.sha256(signature).hexdigest() try: cache = _get_cache(cachepath) return cache[key] except KeyError: value = f(*args, **kwargs) cache[key] = value cache.sync() return value except TypeError: call_to = f.__module__ + '.' + f.__name__ print('Warning: could not disk cache call to %s;' 'it probably has unhashable args. Error: %s' % (call_to, traceback.format_exc())) return f(*args, **kwargs) return wrapped",3
1547,Python,memoize to disk   persistent memoization,https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_decor.py#L601-L651,"def memoize(func): """""" simple memoization decorator References: https://wiki.python.org/moin/PythonDecoratorLibrary#Memoize Args: func (function): live python function Returns: func: CommandLine: python -m utool.util_decor memoize Example: >>> # ENABLE_DOCTEST >>> from utool.util_decor import * # NOQA >>> import utool as ut >>> closure = {'a': 'b', 'c': 'd'} >>> incr = [0] >>> def foo(key): >>> value = closure[key] >>> incr[0] += 1 >>> return value >>> foo_memo = memoize(foo) >>> assert foo('a') == 'b' and foo('c') == 'd' >>> assert incr[0] == 2 >>> print('Call memoized version') >>> assert foo_memo('a') == 'b' and foo_memo('c') == 'd' >>> assert incr[0] == 4 >>> assert foo_memo('a') == 'b' and foo_memo('c') == 'd' >>> print('Counter should no longer increase') >>> assert incr[0] == 4 >>> print('Closure changes result without memoization') >>> closure = {'a': 0, 'c': 1} >>> assert foo('a') == 0 and foo('c') == 1 >>> assert incr[0] == 6 >>> assert foo_memo('a') == 'b' and foo_memo('c') == 'd' """""" cache = func._util_decor_memoize_cache = {} # @functools.wraps(func) def memoizer(*args, **kwargs): key = str(args) + str(kwargs) if key not in cache: cache[key] = func(*args, **kwargs) return cache[key] memoizer = preserve_sig(memoizer, func) memoizer.cache = cache return memoizer",3
1575,Python,memoize to disk   persistent memoization,https://github.com/fridiculous/estimators/blob/ab5b3d70f16f8372ae1114ac7e54e7791631eb74/estimators/hashing.py#L120-L127,"def memoize(self, obj): # We want hashing to be sensitive to value instead of reference. # For example we want ['aa', 'aa'] and ['aa', 'aaZ'[:2]] # to hash to the same value and that's why we disable memoization # for strings if isinstance(obj, _bytes_or_unicode): return Pickler.memoize(self, obj)",3
1694,Python,memoize to disk   persistent memoization,https://github.com/noahbenson/pimms/blob/9051b86d6b858a7a13511b72c48dc21bc903dab2/pimms/calculation.py#L516-L573,"def _run_node(self, node): ''' calc_dict._run_node(node) calculates the results of the given calculation node in the calc_dict's calculation plan and caches the results in the calc_dict. This should only be called by calc_dict itself internally. ''' #print IMap.indent, ('Node: %s' % node.name) #dbg #IMap.indent = ' ' + IMap.indent #dbg # # We need to pause here and handle caching, if needed. res = None if ('memoize' in self.afferents and self.afferents['memoize']) and node.memoize: memdat = self.plan._memoized_data try: h = qhash({k:self.afferents[k] for k in self.plan.afferent_dependencies[node.name]}) ho = (node.name, h) if ho in memdat: # memoization success; no need to memoize the result after processing it res = memdat[ho] h = None ho = None #print(IMap.indent, 'retrieved') #dbg else: cpath = self.afferents['cache_directory'] \ if node.cache and 'cache_directory' in self.afferents else \ None if cpath is not None: ureg = self.afferents['unit_registry'] \ if 'unit_registry' in self.afferents else \ 'pimms' cpath = os.path.join(cpath, node.name, ('0' + str(-h)) if h < 0 else str(h)) try: res = self._uncache(cpath, node, ureg) cpath = None #print(IMap.indent, 'loaded cache') #dbg except: pass except: # memoization failure, must run the node normally (don't memoize/cache) h = None ho = None res = None else: h = None ho = None # ensure we have a result if res is None: res = node(self) # process the result: effs = reduce(lambda m,v: m.set(v[0],v[1]), six.iteritems(res), self.efferents) object.__setattr__(self, 'efferents', effs) # Handle the caching if needed: if h is not None: memdat[ho] = res #if cpath is None: print IMap.indent, 'hashed' #dbg if cpath is not None: try: self._cache(cpath, res) #print IMap.indent, 'saved cache' #dbg except: pass #elif node.memoize: print IMap.indent, 'simple-calc' #dbg",3
741,Python,memoize to disk   persistent memoization,https://github.com/lisael/fastidious/blob/2542db9de779ddabc3a64e9eb19a4e2de99741dc/fastidious/expressions.py#L287-L301,"def memoize(self, code): pk = hash(self.as_grammar()) return"""""" start_pos_{2}= self.pos if ({0}, start_pos_{2}) in self._p_memoized: result, self.pos = self._p_memoized[({0}, self.pos)] else: {1} self._p_memoized[({0}, start_pos_{2})] = result, self.pos """""".format( pk, self._indent(code, 1), self.id, repr(self.rulename) )",2
1123,Python,memoize to disk   persistent memoization,https://github.com/KelSolaar/Foundations/blob/5c141330faf09dad70a12bc321f4c564917d0a91/foundations/decorators.py#L80-L129,"def memoize(cache=None): """""" | Implements method / definition memoization. | Any method / definition decorated will get its return value cached and restored whenever called with the same arguments. :param cache: Alternate cache. :type cache: dict :return: Object. :rtype: object """""" if cache is None: cache = {} def memoize_decorator(object): """""" Implements method / definition memoization. :param object: Object to decorate. :type object: object :return: Object. :rtype: object """""" @functools.wraps(object) def memoize_wrapper(*args, **kwargs): """""" Implements method / definition memoization. :param \*args: Arguments. :type \*args: \* :param \*\*kwargs: Keywords arguments. :type \*\*kwargs: \*\* :return: Object. :rtype: object """""" if kwargs: key = args, frozenset(kwargs.iteritems()) else: key = args if key not in cache: cache[key] = object(*args, **kwargs) return cache[key] return memoize_wrapper return memoize_decorator",2
811,Python,memoize to disk   persistent memoization,https://github.com/peri-source/peri/blob/61beed5deaaf978ab31ed716e8470d86ba639867/peri/util.py#L1055-L1114,"def memoize(cache_max_size=1e9): def memoize_inner(obj): cache_name = str(obj) @functools.wraps(obj) def wrapper(self, *args, **kwargs): # add the memoize cache to the object first, if not present # provide a method to the object to clear the cache too if not hasattr(self, '_memoize_caches'): def clear_cache(self): for k,v in iteritems(self._memoize_caches): self._memoize_caches[k] = newcache() self._memoize_caches = {} self._memoize_clear = types.MethodType(clear_cache, self) # next, add the particular cache for this method if it does # not already exist in the parent 'self' cache = self._memoize_caches.get(cache_name) if not cache: cache = newcache() self._memoize_caches[cache_name] = cache size = 0 hashed = [] # let's hash the arguments (both args, kwargs) and be mindful of # numpy arrays -- that is, only take care of its data, not the obj # itself for arg in args: if isinstance(arg, np.ndarray): hashed.append(arg.tostring()) else: hashed.append(arg) for k,v in iteritems(kwargs): if isinstance(v, np.ndarray): hashed.append(v.tostring()) else: hashed.append(v) hashed = tuple(hashed) if hashed not in cache: ans = obj(self, *args, **kwargs) # if it is not too much to ask, place the answer in the cache if isinstance(ans, np.ndarray): size = ans.nbytes newsize = size + cache['size'] if newsize < cache_max_size: cache[hashed] = ans cache['misses'] += 1 cache['size'] = newsize return ans cache['hits'] += 1 return cache[hashed] return wrapper return memoize_inner",1
2028,Python,memoize to disk   persistent memoization,https://github.com/fedora-python/pyp2rpm/blob/853eb3d226689a5ccdcdb9358b1a3394fafbd2b5/pyp2rpm/utils.py#L44-L56,"def memoize_by_args(func): """"""Memoizes return value of a func based on args."""""" memory = {} @functools.wraps(func) def memoized(*args): if args not in memory.keys(): value = func(*args) memory[args] = value return memory[args] return memoized",1
571,Python,memoize to disk   persistent memoization,https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/psutil/_pslinux.py#L122-L146,"def virtual_memory(): total, free, buffers, shared, _, _ = _psutil_linux.get_sysinfo() cached = active = inactive = None f = open('/proc/meminfo', 'r') try: for line in f: if line.startswith('Cached:'): cached = int(line.split()[1]) * 1024 elif line.startswith('Active:'): active = int(line.split()[1]) * 1024 elif line.startswith('Inactive:'): inactive = int(line.split()[1]) * 1024 if cached is not None \ and active is not None \ and inactive is not None: break else: raise RuntimeError(""line(s) not found"") finally: f.close() avail = free + buffers + cached used = total - free percent = usage_percent((total - avail), total, _round=1) return nt_virtmem_info(total, avail, percent, used, free, active, inactive, buffers, cached)",0
175,Python,matrix multiply,https://github.com/mabuchilab/QNET/blob/cc20d26dad78691d34c67173e5cd67dcac94208a/src/qnet/algebra/core/matrix_algebra.py#L136-L140,"def __mul__(self, other): if isinstance(other, Matrix): return Matrix(self.matrix.dot(other.matrix)) else: return Matrix(self.matrix * other)",3
515,Python,matrix multiply,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/linalg/distributed.py#L373-L389,"def multiply(self, matrix): """""" Multiply this matrix by a local dense matrix on the right. :param matrix: a local dense matrix whose number of rows must match the number of columns of this matrix :returns: :py:class:`RowMatrix` >>> rm = RowMatrix(sc.parallelize([[0, 1], [2, 3]])) >>> rm.multiply(DenseMatrix(2, 2, [0, 2, 1, 3])).rows.collect() [DenseVector([2.0, 3.0]), DenseVector([6.0, 11.0])] """""" if not isinstance(matrix, DenseMatrix): raise ValueError(""Only multiplication with DenseMatrix "" ""is supported."") j_model = self._java_matrix_wrapper.call(""multiply"", matrix) return RowMatrix(j_model)",3
560,Python,matrix multiply,https://github.com/churchill-lab/emase/blob/ae3c6955bb175c1dec88dbf9fac1a7dcc16f4449/emase/Sparse3DMatrix.py#L130-L157,"def __mul__(self, other): if self.finalized: dmat = self.__class__() dmat.shape = self.shape if isinstance(other, Sparse3DMatrix): # element-wise multiplication between same kind if other.finalized: for hid in xrange(self.shape[1]): dmat.data.append(self.data[hid].multiply(other.data[hid])) else: raise RuntimeError('Both matrices must be finalized.') elif isinstance(other, (np.ndarray, csc_matrix, csr_matrix)): # matrix-matrix multiplication for hid in xrange(self.shape[1]): dmat.data.append(self.data[hid] * other) dmat.shape = (other.shape[1], self.shape[1], self.shape[2]) elif isinstance(other, (coo_matrix, lil_matrix)): # matrix-matrix multiplication other_csc = other.tocsc() for hid in xrange(self.shape[1]): dmat.data.append(self.data[hid] * other_csc) dmat.shape = (other_csc.shape[1], self.shape[1], self.shape[2]) elif isinstance(other, Number): # rescaling of matrix for hid in xrange(self.shape[1]): dmat.data.append(self.data[hid] * other) else: raise TypeError('This operator is not supported between the given types.') dmat.finalized = True return dmat else: raise RuntimeError('The original matrix must be finalized.')",3
895,Python,matrix multiply,https://github.com/AndrewAnnex/SpiceyPy/blob/fc20a9b9de68b58eed5b332f0c051fb343a6e335/spiceypy/spiceypy.py#L8663-L8689,"def mxmg(m1, m2, nrow1, ncol1, ncol2): """""" Multiply two double precision matrices of arbitrary size. http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/mxmg_c.html :param m1: nrow1 X ncol1 double precision matrix. :type m1: NxM-Element Array of floats :param m2: ncol1 X ncol2 double precision matrix. :type m2: NxM-Element Array of floats :param nrow1: Row dimension of m1 :type nrow1: int :param ncol1: Column dimension of m1 and row dimension of m2. :type ncol1: int :param ncol2: Column dimension of m2 :type ncol2: int :return: nrow1 X ncol2 double precision matrix. :rtype: NxM-Element Array of floats """""" m1 = stypes.toDoubleMatrix(m1) m2 = stypes.toDoubleMatrix(m2) mout = stypes.emptyDoubleMatrix(x=ncol2, y=nrow1) nrow1 = ctypes.c_int(nrow1) ncol1 = ctypes.c_int(ncol1) ncol2 = ctypes.c_int(ncol2) libspice.mxmg_c(m1, m2, nrow1, ncol1, ncol2, mout) return stypes.cMatrixToNumpy(mout)",3
1401,Python,matrix multiply,https://github.com/fogleman/pg/blob/124ea3803c788b2c98c4f3a428e5d26842a67b58/pg/core.py#L93-L97,"def multiply(self, matrix): positions = [matrix * x for x in self.positions] normals = list(self.normals) uvs = list(self.uvs) return Mesh(positions, normals, uvs) def bounding_box(self):",3
430,Python,matrix multiply,https://github.com/pymupdf/PyMuPDF/blob/917f2d83482510e26ba0ff01fd2392c26f3a8e90/fitz/fitz.py#L270-L275,"def __mul__(self, m): if hasattr(m, ""__float__""): return Matrix(self.a * m, self.b * m, self.c * m, self.d * m, self.e * m, self.f * m) m1 = Matrix(1,1) return m1.concat(self, m)",2
692,Python,matrix multiply,https://github.com/ladybug-tools/ladybug/blob/c08b7308077a48d5612f644943f92d5b5dade583/ladybug/euclid.py#L883-L955,"def __mul__(self, other): if isinstance(other, Matrix4): # Cache attributes in local vars (see Matrix3.__mul__). Aa = self.a Ab = self.b Ac = self.c Ad = self.d Ae = self.e Af = self.f Ag = self.g Ah = self.h Ai = self.i Aj = self.j Ak = self.k Al = self.l Am = self.m An = self.n Ao = self.o Ap = self.p Ba = other.a Bb = other.b Bc = other.c Bd = other.d Be = other.e Bf = other.f Bg = other.g Bh = other.h Bi = other.i Bj = other.j Bk = other.k Bl = other.l Bm = other.m Bn = other.n Bo = other.o Bp = other.p C = Matrix4() C.a = Aa * Ba + Ab * Be + Ac * Bi + Ad * Bm C.b = Aa * Bb + Ab * Bf + Ac * Bj + Ad * Bn C.c = Aa * Bc + Ab * Bg + Ac * Bk + Ad * Bo C.d = Aa * Bd + Ab * Bh + Ac * Bl + Ad * Bp C.e = Ae * Ba + Af * Be + Ag * Bi + Ah * Bm C.f = Ae * Bb + Af * Bf + Ag * Bj + Ah * Bn C.g = Ae * Bc + Af * Bg + Ag * Bk + Ah * Bo C.h = Ae * Bd + Af * Bh + Ag * Bl + Ah * Bp C.i = Ai * Ba + Aj * Be + Ak * Bi + Al * Bm C.j = Ai * Bb + Aj * Bf + Ak * Bj + Al * Bn C.k = Ai * Bc + Aj * Bg + Ak * Bk + Al * Bo C.l = Ai * Bd + Aj * Bh + Ak * Bl + Al * Bp C.m = Am * Ba + An * Be + Ao * Bi + Ap * Bm C.n = Am * Bb + An * Bf + Ao * Bj + Ap * Bn C.o = Am * Bc + An * Bg + Ao * Bk + Ap * Bo C.p = Am * Bd + An * Bh + Ao * Bl + Ap * Bp return C elif isinstance(other, Point3): A = self B = other P = Point3(0, 0, 0) P.x = A.a * B.x + A.b * B.y + A.c * B.z + A.d P.y = A.e * B.x + A.f * B.y + A.g * B.z + A.h P.z = A.i * B.x + A.j * B.y + A.k * B.z + A.l return P elif isinstance(other, Vector3): A = self B = other V = Vector3(0, 0, 0) V.x = A.a * B.x + A.b * B.y + A.c * B.z V.y = A.e * B.x + A.f * B.y + A.g * B.z V.z = A.i * B.x + A.j * B.y + A.k * B.z return V else: other = other.copy() other._apply_transform(self) return other",2
743,Python,matrix multiply,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/linalg/distributed.py#L705-L720,"def multiply(self, matrix): """""" Multiply this matrix by a local dense matrix on the right. :param matrix: a local dense matrix whose number of rows must match the number of columns of this matrix :returns: :py:class:`IndexedRowMatrix` >>> mat = IndexedRowMatrix(sc.parallelize([(0, (0, 1)), (1, (2, 3))])) >>> mat.multiply(DenseMatrix(2, 2, [0, 2, 1, 3])).rows.collect() [IndexedRow(0, [2.0,3.0]), IndexedRow(1, [6.0,11.0])] """""" if not isinstance(matrix, DenseMatrix): raise ValueError(""Only multiplication with DenseMatrix "" ""is supported."") return IndexedRowMatrix(self._java_matrix_wrapper.call(""multiply"", matrix))",2
2005,Python,matrix multiply,https://github.com/cmbruns/pyopenvr/blob/68395d26bb3df6ab1f0f059c38d441f962938be6/src/openvr/gl_renderer.py#L17-L32,"def matrixForOpenVrMatrix(mat): if len(mat.m) == 4: # HmdMatrix44_t? result = numpy.matrix( ((mat.m[0][0], mat.m[1][0], mat.m[2][0], mat.m[3][0]), (mat.m[0][1], mat.m[1][1], mat.m[2][1], mat.m[3][1]), (mat.m[0][2], mat.m[1][2], mat.m[2][2], mat.m[3][2]), (mat.m[0][3], mat.m[1][3], mat.m[2][3], mat.m[3][3]),) , numpy.float32) elif len(mat.m) == 3: # HmdMatrix34_t? result = numpy.matrix( ((mat.m[0][0], mat.m[1][0], mat.m[2][0], 0.0), (mat.m[0][1], mat.m[1][1], mat.m[2][1], 0.0), (mat.m[0][2], mat.m[1][2], mat.m[2][2], 0.0), (mat.m[0][3], mat.m[1][3], mat.m[2][3], 1.0),) , numpy.float32) return result",2
1902,Python,matrix multiply,https://github.com/mabuchilab/QNET/blob/cc20d26dad78691d34c67173e5cd67dcac94208a/src/qnet/algebra/core/matrix_algebra.py#L127-L131,"def __add__(self, other): if isinstance(other, Matrix): return Matrix(self.matrix + other.matrix) else: return Matrix(self.matrix + other)",1
1089,Python,map to json,https://github.com/grst/geos/blob/ea15abcc5d8f86c9051df55e489b7d941b51a638/geos/server.py#L25-L53,"def maps_json(): """""" Generates a json object which serves as bridge between the web interface and the map source collection. All attributes relevant for openlayers are converted into JSON and served through this route. Returns: Response: All map sources as JSON object. """""" map_sources = { id: { ""id"": map_source.id, ""name"": map_source.name, ""folder"": map_source.folder, ""min_zoom"": map_source.min_zoom, ""max_zoom"": map_source.max_zoom, ""layers"": [ { ""min_zoom"": layer.min_zoom, ""max_zoom"": layer.max_zoom, ""tile_url"": layer.tile_url.replace(""$"", """"), } for layer in map_source.layers ] } for id, map_source in app.config[""mapsources""].items() } return jsonify(map_sources)",3
151,Python,map to json,https://github.com/hackedd/gw2api/blob/5543a78e6e3ed0573b7e84c142c44004b4779eac/gw2api/map.py#L50-L105,"def maps(map_id=None, lang=""en""): """"""This resource returns details about maps in the game, including details about floor and translation data on how to translate between world coordinates and map coordinates. :param map_id: Only list this map. :param lang: Show localized texts in the specified language. The response is a dictionary where the key is the map id and the value is a dictionary containing the following properties: map_name (string) The map name. min_level (number) The minimal level of this map. max_level (number) The maximum level of this map. default_floor (number) The default floor of this map. floors (list) A list of available floors for this map. region_id (number) The id of the region this map belongs to. region_name (string) The name of the region this map belongs to. continent_id (number) The id of the continent this map belongs to. continent_name (string) The name of the continent this map belongs to. map_rect (rect) The dimensions of the map. continent_rect (rect) The dimensions of the map within the continent coordinate system. If a map_id is given, only the values for that map are returned. """""" if map_id: cache_name = ""maps.%s.%s.json"" % (map_id, lang) params = {""map_id"": map_id, ""lang"": lang} else: cache_name = ""maps.%s.json"" % lang params = {""lang"": lang} data = get_cached(""maps.json"", cache_name, params=params).get(""maps"") return data.get(str(map_id)) if map_id else data",2
976,Python,map to json,https://github.com/ontio/ontology-python-sdk/blob/ac88bdda941896c5d2ced08422a9c5179d3f9b19/ontology/smart_contract/native_contract/governance.py#L403-L414,"def to_json(self): map = dict() map[""peer_pubkey""] = self.peer_pubkey map[""max_authorize""] = self.max_authorize map[""old_peerCost""] = self.old_peerCost map[""new_peer_cost""] = self.new_peer_cost map[""set_cost_view""] = self.set_cost_view map[""field1""] = self.field1 map[""field2""] = self.field2 map[""field3""] = self.field3 map[""field4""] = self.field4 return map",2
983,Python,map to json,https://github.com/tensorflow/tensor2tensor/blob/272500b6efe353aeb638d2745ed56e519462ca31/tensor2tensor/utils/hparam.py#L558-L572,"def parse_json(self, values_json): """"""Override existing hyperparameter values, parsing new values from a json object. Args: values_json: String containing a json object of name:value pairs. Returns: The `HParams` instance. Raises: KeyError: If a hyperparameter in `values_json` doesn't exist. ValueError: If `values_json` cannot be parsed. """""" values_map = json.loads(values_json) return self.override_from_dict(values_map)",2
1286,Python,map to json,https://github.com/GNS3/gns3-server/blob/a221678448fb5d24e977ef562f81d56aacc89ab1/gns3server/compute/vmware/vmware_vm.py#L75-L91,"def __json__(self): json = {""name"": self.name, ""node_id"": self.id, ""console"": self.console, ""console_type"": self.console_type, ""project_id"": self.project.id, ""vmx_path"": self.vmx_path, ""headless"": self.headless, ""acpi_shutdown"": self.acpi_shutdown, ""adapters"": self._adapters, ""adapter_type"": self.adapter_type, ""use_any_adapter"": self.use_any_adapter, ""status"": self.status, ""node_directory"": self.working_path, ""linked_clone"": self.linked_clone} return json",2
375,Python,map to json,https://github.com/aliyun/aliyun-log-python-sdk/blob/ac383db0a16abf1e5ef7df36074374184b43516e/aliyun/log/index_config.py#L64-L72,"def from_json(self, json_value) : self.index_all = json_value.get(""index_all"", True) self.max_depth = json_value.get(""max_depth"", -1) self.alias = None self.json_keys = {} if ""alias"" in json_value: self.alias = json_value[""alias""] if ""json_keys"" in json_value: self.json_keys = json_value[""json_keys""]",1
471,Python,map to json,https://github.com/sosreport/sos/blob/2ebc04da53dc871c8dd5243567afa4f8592dca29/sos/plugins/kernel.py#L41-L51,"def get_bpftool_map_ids(self, map_file): out = [] try: map_data = json.load(open(map_file)) except Exception as e: self._log_info(""Could not parse bpftool map list as JSON: %s"" % e) return out for item in range(len(map_data)): if ""id"" in map_data[item]: out.append(map_data[item][""id""]) return out",1
1355,Python,map to json,https://github.com/kyper-data/python-highcharts/blob/a4c488ae5c2e125616efad5a722f3dfd8a9bc450/highcharts/highmaps/highmaps.py#L292-L315,"def set_map_source(self, map_src, jsonp_map = False): """"""set map data use if the mapData is loaded directly from a https source the map_src is the https link for the mapData geojson (from jsonp) or .js formates are acceptable default is js script from highcharts' map collection: https://code.highcharts.com/mapdata/ """""" if not map_src: raise OptionTypeError(""No map source input, please refer to: https://code.highcharts.com/mapdata/"") if jsonp_map: self.jsonp_map_flag = True self.map = 'geojson' self.jsonp_map_url = json.dumps(map_src) else: self.add_JSsource(map_src) map_name = self._get_jsmap_name(map_src) self.map = 'geojson' self.jsmap = self.map + ' = Highcharts.geojson(' + map_name + ');' self.add_JSscript('var ' + self.jsmap, 'head') if self.data_temp: self.data_temp[0].__options__().update({'mapData': MapObject(self.map)})",1
1509,Python,map to json,https://github.com/mapbox/mapboxgl-jupyter/blob/f6e403c13eaa910e70659c7d179e8e32ce95ae34/mapboxgl/viz.py#L20-L37,"def generate_vector_color_map(self): """"""Generate color stops array for use with match expression in mapbox template"""""" vector_stops = [] # if join data specified as filename or URL, parse JSON to list of Python dicts if type(self.data) == str: self.data = geojson_to_dict_list(self.data) # loop through features in self.data to create join-data map for row in self.data: # map color to JSON feature using color_property color = color_map(row[self.color_property], self.color_stops, self.color_default) # link to vector feature using data_join_property (from JSON object) vector_stops.append([row[self.data_join_property], color]) return vector_stops",1
1791,Python,map to json,https://github.com/stianaske/pybotvac/blob/e3f655e81070ff209aaa4efb7880016cf2599e6d/pybotvac/robot.py#L65-L120,"def start_cleaning(self, mode=2, navigation_mode=1, category=None, boundary_id=None): # mode & navigation_mode used if applicable to service version # mode: 1 eco, 2 turbo # navigation_mode: 1 normal, 2 extra care, 3 deep # category: 2 non-persistent map, 4 persistent map # boundary_id: the id of the zone to clean # Default to using the persistent map if we support basic-3 or basic-4. if category is None: category = 4 if self.service_version in ['basic-3', 'basic-4'] and self.has_persistent_maps else 2 if self.service_version == 'basic-1': json = {'reqId': ""1"", 'cmd': ""startCleaning"", 'params': { 'category': category, 'mode': mode, 'modifier': 1} } elif self.service_version == 'basic-3' or 'basic-4': json = {'reqId': ""1"", 'cmd': ""startCleaning"", 'params': { 'category': category, 'mode': mode, 'modifier': 1, ""navigationMode"": navigation_mode} } if boundary_id: json['params']['boundaryId'] = boundary_id elif self.service_version == 'minimal-2': json = {'reqId': ""1"", 'cmd': ""startCleaning"", 'params': { 'category': category, ""navigationMode"": navigation_mode} } else: # self.service_version == 'basic-2' json = {'reqId': ""1"", 'cmd': ""startCleaning"", 'params': { 'category': category, 'mode': mode, 'modifier': 1, ""navigationMode"": navigation_mode} } response = self._message(json) response_dict = response.json() # Fall back to category 2 if we tried and failed with category 4 if category == 4 and 'alert' in response_dict and response_dict['alert'] == 'nav_floorplan_load_fail': json['params']['category'] = 2 return self._message(json) return response",1
55,Python,linear regression,https://github.com/quantopian/zipline/blob/77ad15e6dc4c1cbcdc133653bac8a63fc704f7fe/zipline/pipeline/factors/factor.py#L786-L843,"def linear_regression(self, target, regression_length, mask=NotSpecified): """""" Construct a new Factor that performs an ordinary least-squares regression predicting the columns of `self` from `target`. This method can only be called on factors which are deemed safe for use as inputs to other factors. This includes `Returns` and any factors created from `Factor.rank` or `Factor.zscore`. Parameters ---------- target : zipline.pipeline.Term with a numeric dtype The term to use as the predictor/independent variable in each regression. This may be a Factor, a BoundColumn or a Slice. If `target` is two-dimensional, regressions are computed asset-wise. regression_length : int Length of the lookback window over which to compute each regression. mask : zipline.pipeline.Filter, optional A Filter describing which assets should be regressed with the target slice each day. Returns ------- regressions : zipline.pipeline.factors.RollingLinearRegression A new Factor that will compute linear regressions of `target` against the columns of `self`. Examples -------- Suppose we want to create a factor that regresses AAPL's 10-day returns against the 10-day returns of all other assets, computing each regression over 30 days. This can be achieved by doing the following:: returns = Returns(window_length=10) returns_slice = returns[sid(24)] aapl_regressions = returns.linear_regression( target=returns_slice, regression_length=30, ) This is equivalent to doing:: aapl_regressions = RollingLinearRegressionOfReturns( target=sid(24), returns_length=10, regression_length=30, ) See Also -------- :func:`scipy.stats.linregress` :class:`zipline.pipeline.factors.RollingLinearRegressionOfReturns` """""" from .statistical import RollingLinearRegression return RollingLinearRegression( dependent=self, independent=target, regression_length=regression_length, mask=mask, )",3
85,Python,linear regression,https://github.com/SoftwareDefinedBuildings/XBOS/blob/c12d4fb14518ea3ae98c471c28e0710fdf74dd25/apps/Data_quality_analysis/Main.py#L13-L18,"def func(X, y): from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score model = LinearRegression() model.fit(X, y) return model.predict(X)",3
315,Python,linear regression,https://github.com/dshean/pygeotools/blob/5ac745717c0098d01eb293ff1fe32fd7358c76ab/pygeotools/lib/malib.py#L653-L667,"def linreg(self, rsq=False, conf_test=False): model='linear' if not self.datestack: self.compute_dt_stats() if np.isnan(self.min_dt_ptp): #This could fail if stack contains a small number of inputs max_dt_ptp = calcperc(self.dt_stack_ptp, (4, 96))[1] self.min_dt_ptp = 0.20 * max_dt_ptp if self.robust: model='theilsen' #model='ransac' print(""Compute stack linear trend with model: %s"" % model) self.stack_trend, self.stack_intercept, self.stack_detrended_std = \ ma_linreg(self.ma_stack, self.date_list, dt_stack_ptp=self.dt_stack_ptp, min_dt_ptp=self.min_dt_ptp, \ n_thresh=self.n_thresh, model=model, rsq=False, conf_test=False, smooth=False, n_cpu=self.n_cpu) #self.stack_trend = np.ma.array(self.stack_trend, dtype=self.dtype)",3
1071,Python,linear regression,https://github.com/tmoerman/arboreto/blob/3ff7b6f987b32e5774771751dea646fa6feaaa52/arboreto/core.py#L105-L141,"def fit_model(regressor_type, regressor_kwargs, tf_matrix, target_gene_expression, early_stop_window_length=EARLY_STOP_WINDOW_LENGTH, seed=DEMON_SEED): """""" :param regressor_type: string. Case insensitive. :param regressor_kwargs: a dictionary of key-value pairs that configures the regressor. :param tf_matrix: the predictor matrix (transcription factor matrix) as a numpy array. :param target_gene_expression: the target (y) gene expression to predict in function of the tf_matrix (X). :param early_stop_window_length: window length of the early stopping monitor. :param seed: (optional) random seed for the regressors. :return: a trained regression model. """""" regressor_type = regressor_type.upper() assert tf_matrix.shape[0] == len(target_gene_expression) def do_sklearn_regression(): regressor = SKLEARN_REGRESSOR_FACTORY[regressor_type](random_state=seed, **regressor_kwargs) with_early_stopping = is_oob_heuristic_supported(regressor_type, regressor_kwargs) if with_early_stopping: regressor.fit(tf_matrix, target_gene_expression, monitor=EarlyStopMonitor(early_stop_window_length)) else: regressor.fit(tf_matrix, target_gene_expression) return regressor if is_sklearn_regressor(regressor_type): return do_sklearn_regression() # elif is_xgboost_regressor(regressor_type): # raise ValueError('XGB regressor not yet supported') else: raise ValueError('Unsupported regressor type: {0}'.format(regressor_type))",3
1166,Python,linear regression,https://github.com/synw/dataswim/blob/4a4a53f80daa7cd8e8409d76a19ce07296269da2/dataswim/data/stats.py#L8-L21,"def lreg(self, xcol, ycol, name=""Regression""): """""" Add a column to the main dataframe populted with the model's linear regression for a column """""" try: x = self.df[xcol].values.reshape(-1, 1) y = self.df[ycol] lm = linear_model.LinearRegression() lm.fit(x, y) predictions = lm.predict(x) self.df[name] = predictions except Exception as e: self.err(e, ""Can not calculate linear regression"")",3
1191,Python,linear regression,https://github.com/SoftwareDefinedBuildings/XBOS/blob/c12d4fb14518ea3ae98c471c28e0710fdf74dd25/apps/Data_quality_analysis/Model_Data.py#L176-L203,"def linear_regression(self): """""" Linear Regression. This function runs linear regression and stores the, 1. Model 2. Model name 3. Mean score of cross validation 4. Metrics """""" model = LinearRegression() scores = [] kfold = KFold(n_splits=self.cv, shuffle=True, random_state=42) for i, (train, test) in enumerate(kfold.split(self.baseline_in, self.baseline_out)): model.fit(self.baseline_in.iloc[train], self.baseline_out.iloc[train]) scores.append(model.score(self.baseline_in.iloc[test], self.baseline_out.iloc[test])) mean_score = sum(scores) / len(scores) self.models.append(model) self.model_names.append('Linear Regression') self.max_scores.append(mean_score) self.metrics['Linear Regression'] = {} self.metrics['Linear Regression']['R2'] = mean_score self.metrics['Linear Regression']['Adj R2'] = self.adj_r2(mean_score, self.baseline_in.shape[0], self.baseline_in.shape[1])",3
1381,Python,linear regression,https://github.com/raphaelvallat/pingouin/blob/58b19fa4fffbfe09d58b456e3926a148249e4d9b/pingouin/regression.py#L10-L223,"def linear_regression(X, y, add_intercept=True, coef_only=False, alpha=0.05, as_dataframe=True, remove_na=False): """"""(Multiple) Linear regression. Parameters ---------- X : np.array or list Predictor(s). Shape = (n_samples, n_features) or (n_samples,). y : np.array or list Dependent variable. Shape = (n_samples). add_intercept : bool If False, assume that the data are already centered. If True, add a constant term to the model. In this case, the first value in the output dict is the intercept of the model. coef_only : bool If True, return only the regression coefficients. alpha : float Alpha value used for the confidence intervals. CI = [alpha / 2 ; 1 - alpha / 2] as_dataframe : bool If True, returns a pandas DataFrame. If False, returns a dictionnary. remove_na : bool If True, apply a listwise deletion of missing values (i.e. the entire row is removed). Returns ------- stats : dataframe or dict Linear regression summary:: 'names' : name of variable(s) in the model (e.g. x1, x2...) 'coef' : regression coefficients 'se' : standard error of the estimate 'T' : T-values 'pval' : p-values 'r2' : coefficient of determination (R2) 'adj_r2' : adjusted R2 'CI[2.5%]' : lower confidence interval 'CI[97.5%]' : upper confidence interval Notes ----- The beta coefficients of the regression are estimated using the :py:func:`numpy.linalg.lstsq` function. It is generally recommanded to include a constant term (intercept) to the model to limit the bias and force the residual mean to equal zero. Note that intercept coefficient and p-values are however rarely meaningful. The standard error of the estimates is a measure of the accuracy of the prediction defined as: .. math:: se = \\sqrt{MSE \\cdot (X^TX)^{-1}} where :math:`MSE` is the mean squared error, .. math:: MSE = \\frac{\\sum{(true - pred)^2}}{n - p - 1} :math:`p` is the total number of explanatory variables in the model (excluding the intercept) and :math:`n` is the sample size. Using the coefficients and the standard errors, the T-values can be obtained: .. math:: T = \\frac{coef}{se} and the p-values can then be approximated using a T-distribution with :math:`n - p - 1` degrees of freedom. The coefficient of determination (:math:`R^2`) is defined as: .. math:: R^2 = 1 - (\\frac{SS_{resid}}{SS_{total}}) The adjusted :math:`R^2` is defined as: .. math:: \\overline{R}^2 = 1 - (1 - R^2) \\frac{n - 1}{n - p - 1} Results have been compared against sklearn, statsmodels and JASP. This function will not run if NaN values are either present in the target or predictors variables. Please remove them before runing the function. Examples -------- 1. Simple linear regression >>> import numpy as np >>> from pingouin import linear_regression >>> np.random.seed(123) >>> mean, cov, n = [4, 6], [[1, 0.5], [0.5, 1]], 30 >>> x, y = np.random.multivariate_normal(mean, cov, n).T >>> lm = linear_regression(x, y) >>> lm.round(2) names coef se T pval r2 adj_r2 CI[2.5%] CI[97.5%] 0 Intercept 4.40 0.54 8.16 0.00 0.24 0.21 3.29 5.50 1 x1 0.39 0.13 2.99 0.01 0.24 0.21 0.12 0.67 2. Multiple linear regression >>> np.random.seed(42) >>> z = np.random.normal(size=n) >>> X = np.column_stack((x, z)) >>> lm = linear_regression(X, y) >>> print(lm['coef'].values) [4.54123324 0.36628301 0.17709451] 3. Using a Pandas DataFrame >>> import pandas as pd >>> df = pd.DataFrame({'x': x, 'y': y, 'z': z}) >>> lm = linear_regression(df[['x', 'z']], df['y']) >>> print(lm['coef'].values) [4.54123324 0.36628301 0.17709451] 4. No intercept and return coef only >>> linear_regression(X, y, add_intercept=False, coef_only=True) array([ 1.40935593, -0.2916508 ]) 5. Return a dictionnary instead of a DataFrame >>> lm_dict = linear_regression(X, y, as_dataframe=False) 6. Remove missing values >>> X[4, 1] = np.nan >>> y[7] = np.nan >>> linear_regression(X, y, remove_na=True, coef_only=True) array([4.64069731, 0.35455398, 0.1888135 ]) """""" # Extract names if X is a Dataframe or Series if isinstance(X, pd.DataFrame): names = X.keys().tolist() elif isinstance(X, pd.Series): names = [X.name] else: names = [] assert 0 < alpha < 1 assert y.ndim == 1, 'y must be one-dimensional.' # Convert input to numpy array X = np.asarray(X) y = np.asarray(y) if X.ndim == 1: # Convert to (n_samples, n_features) shape X = X[..., np.newaxis] # Check for NaN / Inf if remove_na: X, y = rm_na(X, y[..., np.newaxis], paired=True, axis='rows') y = np.squeeze(y) y_gd = np.isfinite(y).all() X_gd = np.isfinite(X).all() assert y_gd, 'Target (y) contains NaN or Inf. Please remove them.' assert X_gd, 'Predictors (X) contain NaN or Inf. Please remove them.' # Check that X and y have same length assert y.shape[0] == X.shape[0], 'X and y must have same number of samples' if not names: names = ['x' + str(i + 1) for i in range(X.shape[1])] if add_intercept: # Add intercept X = np.column_stack((np.ones(X.shape[0]), X)) names.insert(0, ""Intercept"") # Compute beta coefficient and predictions coef = np.linalg.lstsq(X, y, rcond=None)[0] if coef_only: return coef pred = np.dot(X, coef) resid = np.square(y - pred) ss_res = resid.sum() n, p = X.shape[0], X.shape[1] # Degrees of freedom should not include the intercept dof = n - p if add_intercept else n - p - 1 # Compute mean squared error, variance and SE MSE = ss_res / dof beta_var = MSE * (np.linalg.pinv(np.dot(X.T, X)).diagonal()) beta_se = np.sqrt(beta_var) # Compute R2, adjusted R2 and RMSE ss_tot = np.square(y - y.mean()).sum() # ss_exp = np.square(pred - y.mean()).sum() r2 = 1 - (ss_res / ss_tot) adj_r2 = 1 - (1 - r2) * (n - 1) / dof # Compute T and p-values T = coef / beta_se pval = np.array([2 * t.sf(np.abs(i), dof) for i in T]) # Compute confidence intervals crit = t.ppf(1 - alpha / 2, dof) marg_error = crit * beta_se ll = coef - marg_error ul = coef + marg_error # Rename CI ll_name = 'CI[%.1f%%]' % (100 * alpha / 2) ul_name = 'CI[%.1f%%]' % (100 * (1 - alpha / 2)) # Create dict stats = {'names': names, 'coef': coef, 'se': beta_se, 'T': T, 'pval': pval, 'r2': r2, 'adj_r2': adj_r2, ll_name: ll, ul_name: ul} if as_dataframe: return pd.DataFrame.from_dict(stats) else: return stats",3
312,Python,linear regression,https://github.com/neherlab/treetime/blob/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0/treetime/treeregression.py#L292-L310,"def regression(self, slope=None): """"""regress tip values against branch values Parameters ---------- slope : None, optional if given, the slope isn't optimized Returns ------- dict regression parameters """""" self._calculate_averages() clock_model = base_regression(self.tree.root.Q, slope) clock_model['r_val'] = self.explained_variance() return clock_model",1
18,Python,linear regression,https://github.com/romanorac/discomll/blob/a4703daffb2ba3c9f614bc3dbe45ae55884aea00/discomll/utils/model_view.py#L87-L93,"def _linreg_model(fitmodel): output = ""Linear regression model\n\n"" for k, v in result_iterator(fitmodel): if k == ""thetas"": output += ""Thetas\n"" output += "", "".join(map(str, v)) + ""\n\n"" return output",0
23,Python,k means clustering,https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/filtering/clustering.py#L35-L56,"def cluster_kmeans(data, n_clusters, **kwargs): """""" Identify clusters using K - Means algorithm. Parameters ---------- data : array_like array of size [n_samples, n_features]. n_clusters : int The number of clusters expected in the data. Returns ------- dict boolean array for each identified cluster. """""" km = cl.KMeans(n_clusters, **kwargs) kmf = km.fit(data) labels = kmf.labels_ return labels, [np.nan]",3
159,Python,k means clustering,https://github.com/szairis/sakmapper/blob/ac462fd2674e6aa1aa3b209222d8ac4e9268a790/sakmapper/network.py#L111-L170,"def optimal_clustering(df, patch, method='kmeans', statistic='gap', max_K=5): if len(patch) == 1: return [patch] if statistic == 'db': if method == 'kmeans': if len(patch) <= 5: K_max = 2 else: K_max = min(len(patch) / 2, max_K) clustering = {} db_index = [] X = df.ix[patch, :] for k in range(2, K_max + 1): kmeans = cluster.KMeans(n_clusters=k).fit(X) clustering[k] = pd.DataFrame(kmeans.predict(X), index=patch) dist_mu = squareform(pdist(kmeans.cluster_centers_)) sigma = [] for i in range(k): points_in_cluster = clustering[k][clustering[k][0] == i].index sigma.append(sqrt(X.ix[points_in_cluster, :].var(axis=0).sum())) db_index.append(davies_bouldin(dist_mu, np.array(sigma))) db_index = np.array(db_index) k_optimal = np.argmin(db_index) + 2 return [list(clustering[k_optimal][clustering[k_optimal][0] == i].index) for i in range(k_optimal)] elif method == 'agglomerative': if len(patch) <= 5: K_max = 2 else: K_max = min(len(patch) / 2, max_K) clustering = {} db_index = [] X = df.ix[patch, :] for k in range(2, K_max + 1): agglomerative = cluster.AgglomerativeClustering(n_clusters=k, linkage='average').fit(X) clustering[k] = pd.DataFrame(agglomerative.fit_predict(X), index=patch) tmp = [list(clustering[k][clustering[k][0] == i].index) for i in range(k)] centers = np.array([np.mean(X.ix[c, :], axis=0) for c in tmp]) dist_mu = squareform(pdist(centers)) sigma = [] for i in range(k): points_in_cluster = clustering[k][clustering[k][0] == i].index sigma.append(sqrt(X.ix[points_in_cluster, :].var(axis=0).sum())) db_index.append(davies_bouldin(dist_mu, np.array(sigma))) db_index = np.array(db_index) k_optimal = np.argmin(db_index) + 2 return [list(clustering[k_optimal][clustering[k_optimal][0] == i].index) for i in range(k_optimal)] elif statistic == 'gap': X = np.array(df.ix[patch, :]) if method == 'kmeans': f = cluster.KMeans gaps = gap(X, ks=range(1, min(max_K, len(patch))), method=f) k_optimal = list(gaps).index(max(gaps))+1 clustering = pd.DataFrame(f(n_clusters=k_optimal).fit_predict(X), index=patch) return [list(clustering[clustering[0] == i].index) for i in range(k_optimal)] else: raise 'error: only db and gat statistics are supported'",3
261,Python,k means clustering,https://github.com/atarashansky/self-assembling-manifold/blob/4db4793f65af62047492327716932ba81a67f679/SAM.py#L1318-L1350,"def kmeans_clustering(self, numc, X=None, npcs=15): """"""Performs k-means clustering. Parameters ---------- numc - int Number of clusters npcs - int, optional, default 15 Number of principal components to use as inpute for k-means clustering. """""" from sklearn.cluster import KMeans if X is None: D_sub = self.adata.uns['X_processed'] X = ( D_sub - D_sub.mean(0)).dot( self.adata.uns['pca_obj'].components_[ :npcs, :].T) save = True else: save = False cl = KMeans(n_clusters=numc).fit_predict(Normalizer().fit_transform(X)) if save: self.adata.obs['kmeans_clusters'] = pd.Categorical(cl) else: return cl",3
1356,Python,k means clustering,https://github.com/markovmodel/PyEMMA/blob/5c3124398217de05ba5ce9c8fb01519222481ab8/pyemma/coordinates/api.py#L1695-L1829,"def cluster_kmeans(data=None, k=None, max_iter=10, tolerance=1e-5, stride=1, metric='euclidean', init_strategy='kmeans++', fixed_seed=False, n_jobs=None, chunksize=None, skip=0, keep_data=False, clustercenters=None, **kwargs): r""""""k-means clustering If data is given, it performs a k-means clustering and then assigns the data using a Voronoi discretization. It returns a :class:`KmeansClustering <pyemma.coordinates.clustering.KmeansClustering>` object that can be used to extract the discretized data sequences, or to assign other data points to the same partition. If data is not given, an empty :class:`KmeansClustering <pyemma.coordinates.clustering.KmeansClustering>` will be created that still needs to be parametrized, e.g. in a :func:`pipeline`. Parameters ---------- data: ndarray (T, d) or list of ndarray (T_i, d) or a reader created by :func:`source` input data, if available in memory k: int the number of cluster centers. When not specified (None), min(sqrt(N), 5000) is chosen as default value, where N denotes the number of data points max_iter : int maximum number of iterations before stopping. When not specified (None), min(sqrt(N),5000) is chosen as default value, where N denotes the number of data points tolerance : float stop iteration when the relative change in the cost function :math:`C(S) = \sum_{i=1}^{k} \sum_{\mathbf x \in S_i} \left\| \mathbf x - \boldsymbol\mu_i \right\|^2` is smaller than tolerance. stride : int, optional, default = 1 If set to 1, all input data will be used for estimation. Note that this could cause this calculation to be very slow for large data sets. Since molecular dynamics data is usually correlated at short timescales, it is often sufficient to estimate transformations at a longer stride. Note that the stride option in the get_output() function of the returned object is independent, so you can parametrize at a long stride, and still map all frames through the transformer. metric : str metric to use during clustering ('euclidean', 'minRMSD') init_strategy : str determines if the initial cluster centers are chosen according to the kmeans++-algorithm or drawn uniformly distributed from the provided data set fixed_seed : bool or (positive) integer if set to true, the random seed gets fixed resulting in deterministic behavior; default is false. If an integer >= 0 is given, use this to initialize the random generator. n_jobs : int or None, default None Number of threads to use during assignment of the data. If None, all available CPUs will be used. chunksize: int, default=None Number of data frames to process at once. Choose a higher value here, to optimize thread usage and gain processing speed. If None is passed, use the default value of the underlying reader/data source. Choose zero to disable chunking at all. skip : int, default=0 skip the first initial n frames per trajectory. keep_data: boolean, default=False if you intend to quickly resume a non-converged kmeans iteration, set this to True. Otherwise the linear memory array will have to be re-created. Note that the data will also be deleted, if and only if the estimation converged within the given tolerance parameter. clustercenters: ndarray (k, dim), default=None if passed, the init_strategy is ignored and these centers will be iterated. Returns ------- kmeans : a :class:`KmeansClustering <pyemma.coordinates.clustering.KmeansClustering>` clustering object Object for kmeans clustering. It holds discrete trajectories and cluster center information. Examples -------- >>> import numpy as np >>> from pyemma.util.contexts import settings >>> import pyemma.coordinates as coor >>> traj_data = [np.random.random((100, 3)), np.random.random((100,3))] >>> with settings(show_progress_bars=False): ... cluster_obj = coor.cluster_kmeans(traj_data, k=20, stride=1) ... cluster_obj.get_output() # doctest: +ELLIPSIS [array([... .. seealso:: **Theoretical background**: `Wiki page <http://en.wikipedia.org/wiki/K-means_clustering>`_ .. autoclass:: pyemma.coordinates.clustering.kmeans.KmeansClustering :members: :undoc-members: .. rubric:: Methods .. autoautosummary:: pyemma.coordinates.clustering.kmeans.KmeansClustering :methods: .. rubric:: Attributes .. autoautosummary:: pyemma.coordinates.clustering.kmeans.KmeansClustering :attributes: References ---------- The k-means algorithms was invented in [1]_. The term k-means was first used in [2]_. .. [1] Steinhaus, H. (1957). Sur la division des corps materiels en parties. Bull. Acad. Polon. Sci. (in French) 4, 801-804. .. [2] MacQueen, J. B. (1967). Some Methods for classification and Analysis of Multivariate Observations. Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability 1. University of California Press. pp. 281-297 """""" from pyemma.coordinates.clustering.kmeans import KmeansClustering res = KmeansClustering(n_clusters=k, max_iter=max_iter, metric=metric, tolerance=tolerance, init_strategy=init_strategy, fixed_seed=fixed_seed, n_jobs=n_jobs, skip=skip, keep_data=keep_data, clustercenters=clustercenters, stride=stride) from pyemma.util.reflection import get_default_args cs = _check_old_chunksize_arg(chunksize, get_default_args(cluster_kmeans)['chunksize'], **kwargs) if data is not None: res.estimate(data, chunksize=cs) else: res.chunksize = cs return res",3
1368,Python,k means clustering,https://github.com/markovmodel/PyEMMA/blob/5c3124398217de05ba5ce9c8fb01519222481ab8/pyemma/coordinates/api.py#L1645-L1692,"def cluster_mini_batch_kmeans(data=None, k=100, max_iter=10, batch_size=0.2, metric='euclidean', init_strategy='kmeans++', n_jobs=None, chunksize=None, skip=0, clustercenters=None, **kwargs): r""""""k-means clustering with mini-batch strategy Mini-batch k-means is an approximation to k-means which picks a randomly selected subset of data points to be updated in each iteration. Usually much faster than k-means but will likely deliver a less optimal result. Returns ------- kmeans_mini : a :class:`MiniBatchKmeansClustering <pyemma.coordinates.clustering.MiniBatchKmeansClustering>` clustering object Object for mini-batch kmeans clustering. It holds discrete trajectories and cluster center information. See also -------- :func:`kmeans <pyemma.coordinates.kmeans>` : for full k-means clustering .. autoclass:: pyemma.coordinates.clustering.kmeans.MiniBatchKmeansClustering :members: :undoc-members: .. rubric:: Methods .. autoautosummary:: pyemma.coordinates.clustering.kmeans.MiniBatchKmeansClustering :methods: .. rubric:: Attributes .. autoautosummary:: pyemma.coordinates.clustering.kmeans.MiniBatchKmeansClustering :attributes: References ---------- .. [1] http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf """""" from pyemma.coordinates.clustering.kmeans import MiniBatchKmeansClustering res = MiniBatchKmeansClustering(n_clusters=k, max_iter=max_iter, metric=metric, init_strategy=init_strategy, batch_size=batch_size, n_jobs=n_jobs, skip=skip, clustercenters=clustercenters) from pyemma.util.reflection import get_default_args cs = _check_old_chunksize_arg(chunksize, get_default_args(cluster_mini_batch_kmeans)['chunksize'], **kwargs) if data is not None: res.estimate(data, chunksize=cs) else: res.chunksize = chunksize return res",3
597,Python,k means clustering,https://github.com/vmirly/pyclust/blob/bdb12be4649e70c6c90da2605bc5f4b314e2d07e/pyclust/_kmeans.py#L78-L101,"def _kmeans(X, n_clusters, max_iter, n_trials, tol): """""" Run multiple trials of k-means clustering, and outputt he best centers, and cluster labels """""" n_samples, n_features = X.shape[0], X.shape[1] centers_best = np.empty(shape=(n_clusters,n_features), dtype=float) labels_best = np.empty(shape=n_samples, dtype=int) for i in range(n_trials): centers, labels, sse_tot, sse_arr, n_iter = _kmeans_run(X, n_clusters, max_iter, tol) if i==0: sse_tot_best = sse_tot sse_arr_best = sse_arr n_iter_best = n_iter centers_best = centers.copy() labels_best = labels.copy() if sse_tot < sse_tot_best: sse_tot_best = sse_tot sse_arr_best = sse_arr n_iter_best = n_iter centers_best = centers.copy() labels_best = labels.copy() return(centers_best, labels_best, sse_arr_best, n_iter_best)",2
885,Python,k means clustering,https://github.com/neuropsychology/NeuroKit.py/blob/c9589348fbbde0fa7e986048c48f38e6b488adfe/examples/UnderDev/eeg/eeg_microstates.py#L201-L253,"def eeg_microstates_clustering(data, n_microstates=4, clustering_method=""kmeans"", n_jobs=1, n_init=25, occurence_rejection_treshold=0.05, max_refitting=5, verbose=True): """""" Fit the clustering algorithm. """""" # Create training set training_set = data.copy() if verbose is True: print(""- Initializing the clustering algorithm..."") if clustering_method == ""kmeans"": algorithm = sklearn.cluster.KMeans(init='k-means++', n_clusters=n_microstates, n_init=n_init, n_jobs=n_jobs) elif clustering_method == ""spectral"": algorithm = sklearn.cluster.SpectralClustering(n_clusters=n_microstates, n_init=n_init, n_jobs=n_jobs) elif clustering_method == ""agglom"": algorithm = sklearn.cluster.AgglomerativeClustering(n_clusters=n_microstates, linkage=""complete"") elif clustering_method == ""dbscan"": algorithm = sklearn.cluster.DBSCAN(min_samples=100) elif clustering_method == ""affinity"": algorithm = sklearn.cluster.AffinityPropagation(damping=0.5) else: print(""NeuroKit Error: eeg_microstates(): clustering_method must be 'kmeans', 'spectral', 'dbscan', 'affinity' or 'agglom'"") refitting = 0 # Initialize the number of refittings good_fit_achieved = False while good_fit_achieved is False: good_fit_achieved = True if verbose is True: print(""- Fitting the classifier..."") # Fit the algorithm algorithm.fit(training_set) if verbose is True: print(""- Clustering back the initial data..."") # Predict the more likely cluster for each observation predicted = algorithm.fit_predict(training_set) if verbose is True: print(""- Check for abnormalities..."") # Check for abnormalities and prune the training set until none found occurences = dict(collections.Counter(predicted)) masks = [np.array([True]*len(training_set))] for microstate in occurences: # is the frequency of one microstate inferior to a treshold if occurences[microstate] < len(data)*occurence_rejection_treshold: good_fit_achieved = False refitting += 1 # Increment the refitting print(""NeuroKit Warning: eeg_microstates(): detected some outliers: refitting the classifier (n="" + str(refitting) + "")."") masks.append(predicted!=microstate) mask = np.all(masks, axis=0) training_set = training_set[mask] return(algorithm) # ==============================================================================",2
1523,Python,k means clustering,https://github.com/VIVelev/PyDojoML/blob/773fdce6866aa6decd306a5a85f94129fed816eb/dojo/cluster/hierarchical.py#L43-L62,"def cluster(self, X): X = super().cluster(X) self._distances = linkage(X, method=self.linkage) if self.mode == ""n_clusters"": return fcluster( self._distances, self.n_clusters, criterion=""maxclust"" ) elif self.mode == ""max_distance"": return fcluster( self._distances, self.max_distance, criterion=""distance"" ) else: raise ParameterError(f""Unknown / unsupported clustering mode: \""{self.mode}\"""")",2
165,Python,k means clustering,https://github.com/DEIB-GECO/PyGMQL/blob/e58b2f9402a86056dcda484a32e3de0bb06ed991/gmql/ml/algorithms/clustering.py#L307-L350,"def elbow_method(data, k_min, k_max, distance='euclidean'): """""" Calculates and plots the plot of variance explained - number of clusters Implementation reference: https://github.com/sarguido/k-means-clustering.rst :param data: The dataset :param k_min: lowerbound of the cluster range :param k_max: upperbound of the cluster range :param distance: the distance metric, 'euclidean' by default :return: """""" # Determine your k range k_range = range(k_min, k_max) # Fit the kmeans model for each n_clusters = k k_means_var = [Clustering.kmeans(k).fit(data) for k in k_range] # Pull out the cluster centers for each model centroids = [X.model.cluster_centers_ for X in k_means_var] # Calculate the Euclidean distance from # each point to each cluster center k_euclid = [cdist(data, cent, distance) for cent in centroids] dist = [np.min(ke, axis=1) for ke in k_euclid] # Total within-cluster sum of squares wcss = [sum(d ** 2) for d in dist] # The total sum of squares tss = sum(pdist(data) ** 2) / data.shape[0] # The between-cluster sum of squares bss = tss - wcss # elbow curve fig = plt.figure() ax = fig.add_subplot(111) ax.plot(k_range, bss / tss * 100, 'b*-') ax.set_ylim((0, 100)) plt.grid(True) plt.xlabel('n_clusters') plt.ylabel('Percentage of variance explained') plt.title('Variance Explained vs. k') plt.show()",1
1867,Python,json to xml conversion,https://github.com/codeforamerica/three/blob/67b4a4b233a57aa7995d01f6b0f69c2e85aea6c0/three/core.py#L158-L170,"def convert(self, content, conversion): """"""Convert content to Python data structures."""""" if not conversion: data = content elif self.format == 'json': data = json.loads(content) elif self.format == 'xml': content = xml(content) first = list(content.keys())[0] data = content[first] else: data = content return data",3
1345,Python,json to xml conversion,https://github.com/open511/open511/blob/3d573f59d7efa06ff1b5419ea5ff4d90a90b3cf8/open511/converter/o5json.py#L9-L63,"def xml_to_json(root): """"""Convert an Open511 XML document or document fragment to JSON. Takes an lxml Element object. Returns a dict ready to be JSON-serialized."""""" j = {} if len(root) == 0: # Tag with no children, return str/int return _maybe_intify(root.text) if len(root) == 1 and root[0].tag.startswith('{' + NS_GML): # GML return gml_to_geojson(root[0]) if root.tag == 'open511': j['meta'] = {'version': root.get('version')} for elem in root: name = elem.tag if name == 'link' and elem.get('rel'): name = elem.get('rel') + '_url' if name == 'self_url': name = 'url' if root.tag == 'open511': j['meta'][name] = elem.get('href') continue elif name.startswith('{' + NS_PROTECTED): name = '!' + name[name.index('}') + 1:] elif name[0] == '{': # Namespace! name = '+' + name[name.index('}') + 1:] if name in j: continue # duplicate elif elem.tag == 'link' and not elem.text: j[name] = elem.get('href') elif len(elem): if name == 'grouped_events': # An array of URLs j[name] = [xml_link_to_json(child, to_dict=False) for child in elem] elif name in ('attachments', 'media_files'): # An array of JSON objects j[name] = [xml_link_to_json(child, to_dict=True) for child in elem] elif all((name == pluralize(child.tag) for child in elem)): # <something><somethings> serializes to a JSON array j[name] = [xml_to_json(child) for child in elem] else: j[name] = xml_to_json(elem) else: if root.tag == 'open511' and name.endswith('s') and not elem.text: # Special case: an empty e.g. <events /> container at the root level # should be serialized to [], not null j[name] = [] else: j[name] = _maybe_intify(elem.text) return j",2
1639,Python,json to xml conversion,https://github.com/yyuu/botornado/blob/fffb056f5ff2324d1d5c1304014cfb1d899f602e/boto/cloudfront/identity.py#L77-L84,"def to_xml(self): s = '<?xml version=""1.0"" encoding=""UTF-8""?>\n' s += '<CloudFrontOriginAccessIdentityConfig xmlns=""http://cloudfront.amazonaws.com/doc/2009-09-09/"">\n' s += ' <CallerReference>%s</CallerReference>\n' % self.caller_reference if self.comment: s += ' <Comment>%s</Comment>\n' % self.comment s += '</CloudFrontOriginAccessIdentityConfig>\n' return s",2
233,Python,json to xml conversion,https://github.com/open511/open511/blob/3d573f59d7efa06ff1b5419ea5ff4d90a90b3cf8/open511/converter/tmdd.py#L9-L15,"def tmdd_to_json(doc): converters = TMDDEventConverter.list_from_document(doc) events = [converter.to_json() for converter in converters] return { ""meta"": dict(version='v1'), ""events"": events }",1
585,Python,json to xml conversion,https://github.com/erinxocon/requests-xml/blob/923571ceae4ddd4f2f57a2fc8780d89b50f3e7a1/requests_xml.py#L177-L203,"def json(self, conversion: _Text = 'badgerfish') -> Mapping: """"""A JSON Representation of the XML. Default is badgerfish. :param conversion: Which conversion method to use. (`learn more <https://github.com/sanand0/xmljson#conventions>`_) """""" if not self._json: if conversion is 'badgerfish': from xmljson import badgerfish as serializer elif conversion is 'abdera': from xmljson import abdera as serializer elif conversion is 'cobra': from xmljson import cobra as serializer elif conversion is 'gdata': from xmljson import gdata as serializer elif conversion is 'parker': from xmljson import parker as serializer elif conversion is 'yahoo': from xmljson import yahoo as serializer self._json = json.dumps(serializer.data(etree.fromstring(self.xml))) return self._json",1
760,Python,json to xml conversion,https://github.com/amaas-fintech/amaas-core-sdk-python/blob/347b71f8e776b2dde582b015e31b4802d91e8040/amaascore/core/amaas_model.py#L30-L31,"def to_json_string(dict_to_convert): return json.dumps(dict_to_convert, ensure_ascii=False, default=json_handler, indent=4, separators=(',', ': '))",1
1090,Python,json to xml conversion,https://github.com/elifesciences/elife-tools/blob/4b9e38cbe485c61a4ed7cbd8970c6b318334fd86/elifetools/parseJATS.py#L2064-L2067,"def keywords_json(soup, html_flag=True): # Configure the XML to HTML conversion preference for shorthand use below convert = lambda xml_string: xml_to_html(html_flag, xml_string) return list(map(convert, full_keywords(soup)))",1
1352,Python,json to xml conversion,https://github.com/geopy/geopy/blob/02c838d965e76497f3c3d61f53808c86b5c58224/geopy/geocoders/ignfrance.py#L444-L527,"def _xml_to_json_places(tree, is_reverse=False): """""" Transform the xml ElementTree due to XML webservice return to json """""" select_multi = ( 'GeocodedAddress' if not is_reverse else 'ReverseGeocodedLocation' ) adresses = tree.findall('.//' + select_multi) places = [] sel_pl = './/Address/Place[@type=""{}""]' for adr in adresses: el = {} el['pos'] = adr.find('./Point/pos') el['street'] = adr.find('.//Address/StreetAddress/Street') el['freeformaddress'] = adr.find('.//Address/freeFormAddress') el['municipality'] = adr.find(sel_pl.format('Municipality')) el['numero'] = adr.find(sel_pl.format('Numero')) el['feuille'] = adr.find(sel_pl.format('Feuille')) el['section'] = adr.find(sel_pl.format('Section')) el['departement'] = adr.find(sel_pl.format('Departement')) el['commune_absorbee'] = adr.find(sel_pl.format('CommuneAbsorbee')) el['commune'] = adr.find(sel_pl.format('Commune')) el['insee'] = adr.find(sel_pl.format('INSEE')) el['qualite'] = adr.find(sel_pl.format('Qualite')) el['territoire'] = adr.find(sel_pl.format('Territoire')) el['id'] = adr.find(sel_pl.format('ID')) el['id_tr'] = adr.find(sel_pl.format('ID_TR')) el['bbox'] = adr.find(sel_pl.format('Bbox')) el['nature'] = adr.find(sel_pl.format('Nature')) el['postal_code'] = adr.find('.//Address/PostalCode') el['extended_geocode_match_code'] = adr.find( './/ExtendedGeocodeMatchCode' ) place = {} def testContentAttrib(selector, key): """""" Helper to select by attribute and if not attribute, value set to empty string """""" return selector.attrib.get( key, None ) if selector is not None else None place['accuracy'] = testContentAttrib( adr.find('.//GeocodeMatchCode'), 'accuracy') place['match_type'] = testContentAttrib( adr.find('.//GeocodeMatchCode'), 'matchType') place['building'] = testContentAttrib( adr.find('.//Address/StreetAddress/Building'), 'number') place['search_centre_distance'] = testContentAttrib( adr.find('.//SearchCentreDistance'), 'value') for key, value in iteritems(el): if value is not None: place[key] = value.text if value.text is None: place[key] = None else: place[key] = None # We check if lat lng is not empty and unpack accordingly if place['pos']: lat, lng = place['pos'].split(' ') place['lat'] = lat.strip() place['lng'] = lng.strip() else: place['lat'] = place['lng'] = None # We removed the unused key place.pop(""pos"", None) places.append(place) return places",1
446,Python,json to xml conversion,https://github.com/jfear/sramongo/blob/82a9a157e44bda4100be385c644b3ac21be66038/sramongo/xml_helpers.py#L75-L95,"def xml_to_root(xml: Union[str, IO]) -> ElementTree.Element: """"""Parse XML into an ElemeTree object. Parameters ---------- xml : str or file-like object A filename, file object or string version of xml can be passed. Returns ------- Elementree.Element """""" if isinstance(xml, str): if '<' in xml: return ElementTree.fromstring(xml) else: with open(xml) as fh: xml_to_root(fh) tree = ElementTree.parse(xml) return tree.getroot()",0
938,Python,initializing array,https://github.com/apache/incubator-mxnet/blob/1af29e9c060a4c7d60eeaacba32afdb9a7775ba7/python/mxnet/ndarray/sparse.py#L1507-L1543,"def zeros(stype, shape, ctx=None, dtype=None, **kwargs): """"""Return a new array of given shape and type, filled with zeros. Parameters ---------- stype: string The storage type of the empty array, such as 'row_sparse', 'csr', etc shape : int or tuple of int The shape of the empty array ctx : Context, optional An optional device context (default is the current default context) dtype : str or numpy.dtype, optional An optional value type (default is `float32`) Returns ------- RowSparseNDArray or CSRNDArray A created array Examples -------- >>> mx.nd.sparse.zeros('csr', (1,2)) <CSRNDArray 1x2 @cpu(0)> >>> mx.nd.sparse.zeros('row_sparse', (1,2), ctx=mx.cpu(), dtype='float16').asnumpy() array([[ 0., 0.]], dtype=float16) """""" # pylint: disable= no-member, protected-access if stype == 'default': return _zeros_ndarray(shape, ctx=ctx, dtype=dtype, **kwargs) if ctx is None: ctx = current_context() dtype = mx_real_t if dtype is None else dtype if stype in ('row_sparse', 'csr'): aux_types = _STORAGE_AUX_TYPES[stype] else: raise ValueError(""unknown storage type"" + stype) out = _ndarray_cls(_new_alloc_handle(stype, shape, ctx, True, dtype, aux_types)) return _internal._zeros(shape=shape, ctx=ctx, dtype=dtype, out=out, **kwargs) # pylint: enable= no-member, protected-access",3
950,Python,initializing array,https://github.com/AshleySetter/optoanalysis/blob/9d390acc834d70024d47b574aea14189a5a5714e/optoanalysis/optoanalysis/optoanalysis.py#L3311-L3329,"def _GetRealImagArray(Array): """""" Returns the real and imaginary components of each element in an array and returns them in 2 resulting arrays. Parameters ---------- Array : ndarray Input array Returns ------- RealArray : ndarray The real components of the input array ImagArray : ndarray The imaginary components of the input array """""" ImagArray = _np.array([num.imag for num in Array]) RealArray = _np.array([num.real for num in Array]) return RealArray, ImagArray",3
1330,Python,initializing array,https://github.com/mbakker7/timml/blob/91e99ad573cb8a9ad8ac1fa041c3ca44520c2390/timml/linesink.py#L567-L595,"def initialize(self): for ls in self.lslist: ls.initialize() # Same order for all elements in string self.ncp = self.nls * self.lslist[0].ncp self.nparam = self.nls * self.lslist[0].nparam self.nunknowns = self.nparam self.xls = np.empty((self.nls, 2)) self.yls = np.empty((self.nls, 2)) for i, ls in enumerate(self.lslist): self.xls[i, :] = [ls.x1, ls.x2] self.yls[i, :] = [ls.y1, ls.y2] if self.aq is None: self.aq = self.model.aq.find_aquifer_data(self.lslist[0].xc, self.lslist[0].yc) self.parameters = np.zeros((self.nparam, 1)) # As parameters are only stored for the element not the list, # we need to combine the following self.xc = np.array([ls.xc for ls in self.lslist]).flatten() self.yc = np.array([ls.yc for ls in self.lslist]).flatten() self.xcin = np.array([ls.xcin for ls in self.lslist]).flatten() self.ycin = np.array([ls.ycin for ls in self.lslist]).flatten() self.xcout = np.array([ls.xcout for ls in self.lslist]).flatten() self.ycout = np.array([ls.ycout for ls in self.lslist]).flatten() self.cosnorm = np.array([ls.cosnorm for ls in self.lslist]).flatten() self.sinnorm = np.array([ls.sinnorm for ls in self.lslist]).flatten() self.aqin = self.model.aq.find_aquifer_data(self.xcin[0], self.ycin[0]) self.aqout = self.model.aq.find_aquifer_data(self.xcout[0], self.ycout[0])",3
1433,Python,initializing array,https://github.com/SiLab-Bonn/pixel_clusterizer/blob/d2c8c3072fb03ebb7c6a3e8c57350fbbe38efd4d/pixel_clusterizer/clusterizer.py#L120-L126,"def _init_arrays(self, size=0): if self.initialized: self._cluster_hits = np.zeros(shape=(size, ), dtype=np.dtype(self._cluster_hits_descr)) self._clusters = np.zeros(shape=(size, ), dtype=np.dtype(self._cluster_descr)) self._assigned_hit_array = np.zeros(shape=(size, ), dtype=np.bool) self._cluster_hit_indices = np.empty(shape=(size, ), dtype=np_int_type_chooser(size)) self._cluster_hit_indices.fill(-1)",3
306,Python,initializing array,https://github.com/mbakker7/timml/blob/91e99ad573cb8a9ad8ac1fa041c3ca44520c2390/timml/linesink.py#L651-L670,"def initialize(self): LineSinkStringBase.initialize(self) self.aq.add_element(self) # self.pc = np.array([ls.pc for ls in self.lslist]).flatten() if len(self.hls) == 1: self.pc = self.hls * self.aq.T[self.layers] * np.ones(self.nparam) elif len(self.hls) == self.nls: # head specified at centers self.pc = (self.hls[:, np.newaxis] * self.aq.T[self.layers]).flatten() elif len(self.hls) == 2: L = np.array([ls.L for ls in self.lslist]) Ltot = np.sum(L) xp = np.zeros(self.nls) xp[0] = 0.5 * L[0] for i in range(1, self.nls): xp[i] = xp[i - 1] + 0.5 * (L[i - 1] + L[i]) self.hls = np.interp(xp, [0, Ltot], self.hls) self.pc = (self.hls[:, np.newaxis] * self.aq.T[self.layers]).flatten() else: print('Error: hls entry not supported') self.resfac = 0.0",1
661,Python,initializing array,https://github.com/JamesPHoughton/pysd/blob/bf1b1d03954e9ba5acac9ba4f1ada7cd93352eda/pysd/py_backend/functions.py#L150-L154,"def initialize(self): self.state = self.init_func() if isinstance(self.state, xr.DataArray): self.shape_info = {'dims': self.state.dims, 'coords': self.state.coords}",1
927,Python,initializing array,https://github.com/skorch-dev/skorch/blob/5b9b8b7b7712cb6e5aaa759d9608ea6269d5bcd3/skorch/net.py#L541-L554,"def initialize(self): """"""Initializes all components of the :class:`.NeuralNet` and returns self. """""" self.initialize_virtual_params() self.initialize_callbacks() self.initialize_criterion() self.initialize_module() self.initialize_optimizer() self.initialize_history() self.initialized_ = True return self",1
130,Python,initializing array,https://github.com/mnick/scikit-tensor/blob/fe517e9661a08164b8d30d2dddf7c96aeeabcf36/sktensor/cp.py#L190-L205,"def _init(init, X, N, rank, dtype): """""" Initialization for CP models """""" Uinit = [None for _ in range(N)] if isinstance(init, list): Uinit = init elif init == 'random': for n in range(1, N): Uinit[n] = array(rand(X.shape[n], rank), dtype=dtype) elif init == 'nvecs': for n in range(1, N): Uinit[n] = array(nvecs(X, n, rank), dtype=dtype) else: raise 'Unknown option (init=%s)' % str(init) return Uinit",0
618,Python,initializing array,https://github.com/gbiggs/rtsprofile/blob/fded6eddcb0b25fe9808b1b12336a4413ea00905/rtsprofile/rts_profile.py#L500-L503,"def initializing(self, initializing): validate_attribute(initializing, 'rts_profile.Initializing', expected_type=Initialize, required=False) self._initializing = initializing",0
771,Python,initializing array,https://github.com/Chilipp/psyplot/blob/75a0a15a9a1dd018e79d2df270d56c4bf5f311d5/psyplot/plotter.py#L1529-L1563,"def _plot_by_priority(self, priority, fmtos, initializing=False): def update(fmto): other_fmto = self._shared.get(fmto.key) if other_fmto: self.logger.debug(""%s is shared with %s"", fmto.key, other_fmto.plotter.logger.name) other_fmto.share(fmto, initializing=initializing) # but if not, share them else: if initializing: self.logger.debug(""Initializing %s"", fmto.key) fmto.initialize_plot(fmto.value) else: self.logger.debug(""Updating %s"", fmto.key) fmto.update(fmto.value) try: fmto.lock.release() except RuntimeError: pass self._initializing = initializing self.logger.debug( ""%s formatoptions with priority %i"", ""Initializing"" if initializing else ""Updating"", priority) if priority >= START or priority == END: for fmto in fmtos: update(fmto) elif priority == BEFOREPLOTTING: for fmto in fmtos: update(fmto) self._make_plot() self._initializing = False",0
109,Python,httpclient post json,https://github.com/rckclmbr/pyportify/blob/696a1caad8a47b191f3bec44cc8fc3c437779512/pyportify/google.py#L98-L112,"async def _http_post(self, url, data): data = json.dumps(data) headers = {""Authorization"": ""GoogleLogin auth={0}"".format(self.token), ""Content-type"": ""application/json""} res = await self.session.request( 'POST', FULL_SJ_URL + url, data=data, headers=headers, params={'tier': 'aa', 'hl': 'en_US', 'dv': 0, 'alt': 'json'}) ret = await res.json() return ret",3
300,Python,httpclient post json,https://github.com/LasLabs/python-helpscout/blob/84bf669417d72ca19641a02c9a660e1ae4271de4/helpscout/request_paginator/__init__.py#L114-L123,"def post(self, json=None): """"""Send a POST request and return the JSON decoded result. Args: json (dict, optional): Object to encode and send in request. Returns: mixed: JSON decoded response data. """""" return self._call('post', url=self.endpoint, json=json)",3
364,Python,httpclient post json,https://github.com/Adyen/adyen-python-api-library/blob/928f6409ab6e2fac300b9fa29d89f3f508b23445/Adyen/httpclient.py#L282-L316,"def request(self, url, json="""", data="""", username="""", password="""", headers=None, timout=30): """"""This is overridden on module initialization. This function will make an HTTP POST to a given url. Either json/data will be what is posted to the end point. he HTTP request needs to be basicAuth when username and password are provided. a headers dict maybe provided, whatever the values are should be applied. Args: url (str): url to send the POST json (dict, optional): Dict of the JSON to POST data (dict, optional): Dict, presumed flat structure of key/value of request to place as www-form username (str, optional): Username for basic auth. Must be uncluded as part of password. password (str, optional): Password for basic auth. Must be included as part of username. headers (dict, optional): Key/Value pairs of headers to include Returns: str: Raw request placed str: Raw response received int: HTTP status code, eg 200,404,401 dict: Key/Value pairs of the headers received. :param timout: """""" raise NotImplementedError('request of HTTPClient should have been ' 'overridden on initialization. ' 'Otherwise, can be overridden to ' 'supply your own post method')",3
718,Python,httpclient post json,https://github.com/rehive/rehive-python/blob/a7452a9cfecf76c5c8f0d443f122ed22167fb164/rehive/api/client.py#L50-L51,"def post(self, path, data, json=True, **kwargs): return self._request('post', path, data, json=json, **kwargs)",3
1122,Python,httpclient post json,https://github.com/vladcalin/gemstone/blob/325a49d17621b9d45ffd2b5eca6f0de284de8ba4/gemstone/discovery/default.py#L19-L31,"def make_jsonrpc_call(self, url, method, params): client = HTTPClient() body = json.dumps({ ""jsonrpc"": ""2.0"", ""method"": method, ""params"": params, ""id"": """".join([random.choice(string.ascii_letters) for _ in range(10)]) }) request = HTTPRequest(url, method=""POST"", headers={""content-type"": ""application/json""}, body=body) result = client.fetch(request) return result",3
1180,Python,httpclient post json,https://github.com/fvalverd/AutoApi-client-Python/blob/a6b04947f80bf988c1515d622c78c587b341b3f4/auto_api_client/__init__.py#L91-L94,"def post(self, json=None): response = self._http(requests.post, json=json) if response.status_code == 201: return response.json()",3
1386,Python,httpclient post json,https://github.com/docker/docker-py/blob/613d6aad83acc9931ff2ecfd6a6c7bd8061dc125/docker/api/client.py#L275-L289,"def _post_json(self, url, data, **kwargs): # Go <1.1 can't unserialize null to a string # so we do this disgusting thing here. data2 = {} if data is not None and isinstance(data, dict): for k, v in six.iteritems(data): if v is not None: data2[k] = v elif data is not None: data2 = data if 'headers' not in kwargs: kwargs['headers'] = {} kwargs['headers']['Content-Type'] = 'application/json' return self._post(url, data=json.dumps(data2), **kwargs)",3
1849,Python,httpclient post json,https://github.com/rigetti/pyquil/blob/ec98e453084b0037d69d8c3245f6822a5422593d/pyquil/api/_base_connection.py#L52-L59,"def post_json(session, url, json): """""" Post JSON to the Forest endpoint. """""" res = session.post(url, json=json) if res.status_code >= 400: raise parse_error(res) return res",3
323,Python,httpclient post json,https://github.com/proycon/flat/blob/f14eea61edcae8656dadccd9a43481ff7e710ffb/flat/comm.py#L116-L135,"def postjson( request, url, data): if isinstance(data, dict) or isinstance(data,list) or isinstance(data, tuple): data = json.dumps(data) if url and url[0] == '/': url = url[1:] docservereq = Request(""http://"" + settings.FOLIADOCSERVE_HOST + "":"" + str(settings.FOLIADOCSERVE_PORT) + ""/"" + url + '/' + sid) #or opener.open() setsid(docservereq, getsid(request)) docservereq.add_header('Content-Type', 'application/json') f = urlopen(docservereq, urlencode(data).encode('utf-8')) if sys.version < '3': contents = unicode(f.read(),'utf-8') else: contents = str(f.read(),'utf-8') f.close() if contents and contents[0] == '{': #assume this is json return json.loads(contents) elif contents: return contents else: return None",2
435,Python,html entities replace,https://github.com/honzajavorek/tipi/blob/cbe51192725608b6fba1244a48610ae231b13e08/tipi/repl.py#L65-L74,"def replace(html, replacements=None): """"""Performs replacements on given HTML string."""""" if not replacements: return html # no replacements html = HTMLFragment(html) for r in replacements: r.replace(html) return unicode(html)",3
1127,Python,html entities replace,https://github.com/seleniumbase/SeleniumBase/blob/62e5b43ee1f90a9ed923841bdd53b1b38358f43a/seleniumbase/fixtures/email_manager.py#L454-L481,"def replace_entities(self, html): """""" Replace htmlentities with unicode characters @Params html - html source to replace entities in @Returns String html with entities replaced """""" def fixup(text): """"""replace the htmlentities in some text"""""" text = text.group(0) if text[:2] == ""&#"": # character reference try: if text[:3] == ""&#x"": return chr(int(text[3:-1], 16)) else: return chr(int(text[2:-1])) except ValueError: pass else: # named entity try: text = chr(htmlentitydefs.name2codepoint[text[1:-1]]) except KeyError: pass return text # leave as is return re.sub(r""&#?\w+;"", fixup, html)",3
1172,Python,html entities replace,https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/html.py#L375-L386,"def escape(t): """"""HTML-escape the text in `t`."""""" return (t # Convert HTML special chars into HTML entities. .replace(""&"", ""&amp;"").replace(""<"", ""&lt;"").replace("">"", ""&gt;"") .replace(""'"", ""&#39;"").replace('""', ""&quot;"") # Convert runs of spaces: ""......"" -> ""&nbsp;.&nbsp;.&nbsp;."" .replace("" "", ""&nbsp; "") # To deal with odd-length runs, convert the final pair of spaces # so that ""....."" -> ""&nbsp;.&nbsp;&nbsp;."" .replace("" "", ""&nbsp; "") )",3
1715,Python,html entities replace,https://github.com/fboender/ansible-cmdb/blob/ebd960ac10684e8c9ec2b12751bba2c4c9504ab7/lib/mako/filters.py#L159-L174,"def htmlentityreplace_errors(ex): """"""An encoding error handler. This python `codecs`_ error handler replaces unencodable characters with HTML entities, or, if no HTML entity exists for the character, XML character references. >>> u'The cost was \u20ac12.'.encode('latin1', 'htmlentityreplace') 'The cost was &euro;12.' """""" if isinstance(ex, UnicodeEncodeError): # Handle encoding errors bad_text = ex.object[ex.start:ex.end] text = _html_entities_escaper.escape(bad_text) return (compat.text_type(text), ex.end) raise ex",3
71,Python,html entities replace,https://github.com/LiftoffSoftware/htmltag/blob/f6989f9a3301e7c96ee613e5dbbe43b2bde615c7/htmltag.py#L442-L448,"def escape(self, string): """""" Returns *string* with all instances of '<', '>', and '&' converted into HTML entities. """""" html_entities = {""&"": ""&amp;"", '<': '&lt;', '>': '&gt;'} return HTML("""".join(html_entities.get(c, c) for c in string))",2
322,Python,html entities replace,https://github.com/nickoala/telepot/blob/3792fde251d0f1d5a6ca16c8ad1a71f89360c41d/telepot/text.py#L65-L88,"def apply_entities_as_html(text, entities): """""" Format text as HTML. Also take care of escaping special characters. Returned value can be passed to :meth:`.Bot.sendMessage` with appropriate ``parse_mode``. :param text: plain text :param entities: a list of `MessageEntity <https://core.telegram.org/bots/api#messageentity>`_ objects """""" escapes = {'<': '&lt;', '>': '&gt;', '&': '&amp;',} formatters = {'bold': lambda s,e: '<b>'+s+'</b>', 'italic': lambda s,e: '<i>'+s+'</i>', 'text_link': lambda s,e: '<a href=""'+e['url']+'"">'+s+'</a>', 'text_mention': lambda s,e: '<a href=""tg://user?id='+str(e['user']['id'])+'"">'+s+'</a>', 'code': lambda s,e: '<code>'+s+'</code>', 'pre': lambda s,e: '<pre>'+s+'</pre>'} return _apply_entities(text, entities, escapes, formatters)",2
947,Python,html entities replace,https://github.com/shoebot/shoebot/blob/d554c1765c1899fa25727c9fc6805d221585562b/lib/web/html.py#L109-L123,"def prepare(self, html): # Clean up faulty HTML before parsing. html = html.replace(""<br/>"", ""<br />"") html = html.replace(""<hr/>"", ""<hr />"") # Display list items with an asterisk. #html = html.replace(""li>"", ""li>*"") html = re.sub(r""<li.*?>"", ""\n<li>* "", html) #html = html.replace(""li>\n"", ""li>"") # Make sure there is a space between elements. html = html.replace(""><"", ""> <"") # Linebreaks in the source should not end up in the output. if not self.linebreaks: html = html.replace(""\r"", ""\n"") html = html.replace(""\n"", "" "") return html",2
1001,Python,html entities replace,https://github.com/html5lib/html5lib-python/blob/4b2275497b624c6e97150fa2eb16a7db7ed42111/utils/entities.py#L82-L89,"def make_entities_code(entities): entities_text = ""\n"".join("" \""%s\"": u\""%s\"","" % ( name, entities[name].encode( ""unicode-escape"").replace(""\"""", ""\\\"""")) for name in sorted(entities.keys())) return """"""entities = { %s }"""""" % entities_text",2
972,Python,html entities replace,https://github.com/markfinger/django-node/blob/a2f56bf027fd3c4cbc6a0213881922a50acae1d6/django_node/utils.py#L183-L193,"def decode_html_entities(html): """""" Decodes a limited set of HTML entities. """""" if not html: return html for entity, char in six.iteritems(html_entity_map): html = html.replace(entity, char) return html",1
277,Python,html entities replace,https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/nexson_syntax/nexson2nexml.py#L101-L137,"def _partition_keys_for_xml(self, o): """"""Breaks o into four content type by key syntax: attrib keys (start with '@'), text (value associated with the '$' or None), child element keys (all others) meta element """""" ak = {} tk = None ck = {} mc = {} # _LOG.debug('o = {o}'.format(o=o)) for k, v in o.items(): if k.startswith('@'): if k == '@xmlns': if '$' in v: ak['xmlns'] = v['$'] for nsk, nsv in v.items(): if nsk != '$': ak['xmlns:' + nsk] = nsv else: s = k[1:] if isinstance(v, bool): v = u'true' if v else u'false' ak[s] = UNICODE(v) elif k == '$': tk = v elif k.startswith('^') and (not self._migrating_from_bf): s = k[1:] val = _convert_hbf_meta_val_for_xml(s, v) _add_value_to_dict_bf(mc, s, val) elif (k == u'meta') and self._migrating_from_bf: s, val = _convert_bf_meta_val_for_xml(v) _add_value_to_dict_bf(mc, s, val) else: ck[k] = v return ak, tk, ck, mc",0
289,Python,html encode string,https://github.com/hit9/rux/blob/d7f60722658a3b83ac6d7bb3ca2790ac9c926b59/rux/parser.py#L33-L42,"def _code_no_lexer(self, text): # encode to utf8 string text = text.encode(charset).strip() return( """""" <div class=""highlight""> <pre><code>%s</code></pre> </div> """""" % houdini.escape_html(text) )",3
1432,Python,html encode string,https://github.com/kennethreitz/requests-html/blob/b59a9f2fb9333d7d467154a0fd82978efdb9d23b/requests_html.py#L110-L111,"def html(self, html: str) -> None: self._html = html.encode(self.encoding)",3
503,Python,html encode string,https://github.com/sprymix/metamagic.json/blob/c95d3cacd641d433af44f0774f51a085cb4888e6/metamagic/json/encoder.py#L156-L178,"def _encode_str(self, obj, escape_quotes=True): """"""Return an ASCII-only JSON representation of a Python string"""""" def replace(match): s = match.group(0) try: if escape_quotes: return ESCAPE_DCT[s] else: return BASE_ESCAPE_DCT[s] except KeyError: n = ord(s) if n < 0x10000: return '\\u{0:04x}'.format(n) else: # surrogate pair n -= 0x10000 s1 = 0xd800 | ((n >> 10) & 0x3ff) s2 = 0xdc00 | (n & 0x3ff) return '\\u{0:04x}\\u{1:04x}'.format(s1, s2) if escape_quotes: return '""' + ESCAPE_ASCII.sub(replace, obj) + '""' else: return BASE_ESCAPE_ASCII.sub(replace, obj)",2
1414,Python,html encode string,https://github.com/polyaxon/polyaxon/blob/e1724f0756b1a42f9e7aa08a976584a84ef7f016/polyaxon/libs/json_utils.py#L75-L79,"def dumps(value, escape=False, **kwargs): # Prefer to use dumps_htmlsafe if escape: return _default_escaped_encoder.encode(value) return _default_encoder.encode(value)",2
1992,Python,html encode string,https://github.com/honzajavorek/danube-delta/blob/d0a72f0704d52b888e7fb2b68c4fdc696d370018/danube_delta/plugins/utils.py#L8-L17,"def modify_html(content, prop='_content'): html_string = getattr(content, prop) html_tree = html.fromstring(html_string) yield html_tree html_string = html.tostring(html_tree, encoding='unicode') html_string = re.sub(r'%7B(\w+)%7D', r'{\1}', html_string) html_string = re.sub(r'%7C(\w+)%7C', r'|\1|', html_string) setattr(content, prop, html_string)",2
2037,Python,html encode string,https://github.com/adsabs/adsutils/blob/fb9d6b4f6ed5e6ca19c552efc3cdd6466c587fdb/adsutils/Unicode.py#L153-L159,"def encode(self,str): data = self.u2ent(str) data = data.replace(""&"", ""&amp;"") data = data.replace(""<"", ""&lt;"") data = data.replace(""\"""", ""&quot;"") data = data.replace("">"", ""&gt;"") return data",2
144,Python,html encode string,https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/browser/referenceanalysis.py#L85-L88,def toPdf(self): html = safe_unicode(self.template()).encode('utf-8') pdf_data = createPdf(html) return pdf_data,1
157,Python,html encode string,https://github.com/noahmorrison/chevron/blob/78f1a384eddef16906732d8db66deea6d37049b7/chevron/renderer.py#L34-L47,"def _html_escape(string): """"""HTML escape all of these "" & < >"""""" html_codes = { '""': '&quot;', '<': '&lt;', '>': '&gt;', } # & must be handled first string = string.replace('&', '&amp;') for char in html_codes: string = string.replace(char, html_codes[char]) return string",1
1190,Python,html encode string,https://github.com/wdecoster/nanoplotter/blob/80908dd1be585f450da5a66989de9de4d544ec85/nanoplotter/plot.py#L18-L24,def encode(self): if self.html: return self.html elif self.fig: return self.encode2() else: return self.encode1(),1
1459,Python,html encode string,https://github.com/jeffa/HTML-Auto-python/blob/e0d24335d86c21b6278f776f3d3416c90e438ab9/t/05-encode.py#L6-L70,"def test_lower(self): encoder = Encoder() self.assertEqual( '&amp;&lt;&gt;&quot;&apos;', encoder.encode( '&<>""\'' ), 'default chars encoded when chars is nil' ) self.assertEqual( '&amp;&lt;&gt;&quot;&apos;', encoder.encode( '&<>""\'', '' ), 'encodes when chars is empty' ) self.assertEqual( 'h&#101;llo', encoder.encode( 'hello', 'e' ), 'requested chars encoded correctly' ) self.assertEqual( 'hell&#48;', encoder.encode( 'hell0', 0 ), 'zero encodes correctly' ) self.assertEqual( '&amp;b&#97;r', encoder.encode( '&bar', 'a&' ), 'ampersand is not double encoded' ) self.assertEqual( 'hello', encoder.encode( 'hello' ), 'no encodes when default chars is nil' ) self.assertEqual( 'hello', encoder.encode( 'hello', '' ), 'no encodes when default chars is empty' ) deadbeef = chr(222) + chr(173) + chr(190) + chr(239) self.assertEqual( '&THORN;&shy;&frac34;&iuml;', encoder.encode( deadbeef, deadbeef ), 'hex codes encoded correctly' ) self.assertEqual( '&THORN;&shy;&frac34;&iuml;', encoder.encode( deadbeef ), 'hex codes encoded correctly when chars is nil' ) self.assertEqual( '&THORN;&shy;&frac34;&iuml;', encoder.encode( deadbeef, '' ), 'hex codes encoded correctly when chars is empty' )",1
424,Python,how to reverse a string,https://github.com/mclarkk/lifxlan/blob/ead0e3114d6aa2e5e77dab1191c13c16066c32b0/lifxlan/message.py#L126-L130,"def convert_MAC_to_int(addr): reverse_bytes_str = addr.split(':') reverse_bytes_str.reverse() addr_str = """".join(reverse_bytes_str) return int(addr_str, 16)",2
936,Python,how to reverse a string,https://github.com/honzajavorek/redis-collections/blob/07ca8efe88fb128f7dc7319dfa6a26cd39b3776b/redis_collections/lists.py#L482-L498,"def reverse(self): """""" Reverses the items of this collection ""in place"" (only two values are retrieved from Redis at a time). """""" def reverse_trans(pipe): if self.writeback: self._sync_helper(pipe) n = self.__len__(pipe) for i in range(n // 2): left = pipe.lindex(self.key, i) right = pipe.lindex(self.key, n - i - 1) pipe.lset(self.key, i, right) pipe.lset(self.key, n - i - 1, left) self._transaction(reverse_trans)",2
987,Python,how to reverse a string,https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/strings/reverse_words.py#L9-L14,"def reverse_words(string): arr = string.strip().split() # arr is list of words n = len(arr) reverse(arr, 0, n-1) return "" "".join(arr)",2
1800,Python,how to reverse a string,https://github.com/nilp0inter/cpe/blob/670d947472a7652af5149324977b50f9a7af9bcf/cpe/cpe.py#L115-L146,"def _trim(cls, s): """""" Remove trailing colons from the URI back to the first non-colon. :param string s: input URI string :returns: URI string with trailing colons removed :rtype: string TEST: trailing colons necessary >>> s = '1:2::::' >>> CPE._trim(s) '1:2' TEST: trailing colons not necessary >>> s = '1:2:3:4:5:6' >>> CPE._trim(s) '1:2:3:4:5:6' """""" reverse = s[::-1] idx = 0 for i in range(0, len(reverse)): if reverse[i] == "":"": idx += 1 else: break # Return the substring after all trailing colons, # reversed back to its original character order. new_s = reverse[idx: len(reverse)] return new_s[::-1]",2
1993,Python,how to reverse a string,https://github.com/seleniumbase/SeleniumBase/blob/62e5b43ee1f90a9ed923841bdd53b1b38358f43a/seleniumbase/common/encryption.py#L43-L55,"def reverse_shuffle_string(string): if len(string) < 2: return string new_string = """" odd = (len(string) % 2 == 1) part1 = string[:int(len(string) / 2):1] part2 = string[int(len(string) / 2)::1] for c in range(len(part1)): new_string += part2[c] new_string += part1[c] if odd: new_string += part2[-1] return new_string",2
307,Python,how to reverse a string,https://github.com/tehmaze/ipcalc/blob/d436b95d2783347c3e0084d76ec3c52d1f5d2f0b/ipcalc.py#L544-L558,"def to_reverse(self): """"""Convert the IP address to a PTR record. Using the .in-addr.arpa zone for IPv4 and .ip6.arpa for IPv6 addresses. >>> ip = IP('192.0.2.42') >>> print(ip.to_reverse()) 42.2.0.192.in-addr.arpa >>> print(ip.to_ipv6().to_reverse()) 0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.a.2.2.0.0.0.0.c.2.0.0.2.ip6.arpa """""" if self.v == 4: return '.'.join(list(self.dq.split('.')[::-1]) + ['in-addr', 'arpa']) else: return '.'.join(list(self.hex())[::-1] + ['ip6', 'arpa'])",1
596,Python,how to reverse a string,https://github.com/tornadoweb/tornado/blob/b8b481770bcdb333a69afde5cce7eaa449128326/tornado/web.py#L2209-L2222,"def reverse_url(self, name: str, *args: Any) -> str: """"""Returns a URL path for handler named ``name`` The handler must be added to the application as a named `URLSpec`. Args will be substituted for capturing groups in the `URLSpec` regex. They will be converted to strings if necessary, encoded as utf8, and url-escaped. """""" reversed_url = self.default_router.reverse_url(name, *args) if reversed_url is not None: return reversed_url raise KeyError(""%s not found in named urls"" % name)",1
1971,Python,how to reverse a string,https://github.com/mvcisback/py-aiger-bv/blob/855819844c429c35cdd8dc0b134bcd11f7b2fda3/aigerbv/common.py#L179-L184,"def reverse_gate(wordlen, input='x', output='rev(x)'): circ = identity_gate(wordlen, input, output) output_map = frozenset( (k, tuple(reversed(vs))) for k, vs in circ.output_map ) return attr.evolve(circ, output_map=output_map)",1
143,Python,how to reverse a string,https://github.com/SmartTeleMax/iktomi/blob/80bc0f1408d63efe7f5844367d1f6efba44b35f2/iktomi/web/reverse.py#L218-L229,"def _build_url_silent(self, _name, **kwargs): subreverse = self used_args = set() for part in _name.split('.'): if not subreverse._ready and subreverse._need_arguments: used_args |= subreverse.url_arguments subreverse = subreverse(**kwargs) subreverse = getattr(subreverse, part) if not subreverse._ready and subreverse._is_endpoint: used_args |= subreverse.url_arguments subreverse = subreverse(**kwargs) return used_args, subreverse",0
582,Python,how to reverse a string,https://github.com/frnsys/broca/blob/7236dcf54edc0a4a54a55eb93be30800910667e7/broca/similarity/doc/wikipedia.py#L28-L41,"def compute_bridge_similarity(self, vec1, vec2): EWP = 1 - np.multiply(vec1, vec2) # not sure exactly how to sort the EWP vector #EWP = sorted(EWP, reverse=True) EWP = sorted(EWP, reverse=True) k = 10 EWP = EWP[:k] # The paper does not mention using logs but start to get into underflow # issues multiplying so many decimal values lEWP = -1 * np.log(EWP) return 1/np.sum(lEWP)",0
570,Python,how to read the contents of a gz compressed file,https://github.com/EntilZha/PyFunctional/blob/ac04e4a8552b0c464a7f492f7c9862424867b63e/functional/io.py#L115-L122,"def read(self): with gzip.GzipFile(self.path, compresslevel=self.compresslevel) as gz_file: gz_file.read1 = gz_file.read with io.TextIOWrapper(gz_file, encoding=self.encoding, errors=self.errors, newline=self.newline) as file_content: return file_content.read()",3
1010,Python,how to read the contents of a gz compressed file,https://github.com/rfinnie/dsari/blob/cd7b07c30876467393e0ec18f1ff45d86bcb1676/dsari/render.py#L75-L86,"def read_output(filename): if os.path.isfile(filename): with open(filename, 'rb') as f: return f.read().decode('utf-8') elif os.path.isfile('{}.gz'.format(filename)): with gzip.open('{}.gz'.format(filename), 'rb') as f: return f.read().decode('utf-8') elif HAS_LZMA and os.path.isfile('{}.xz'.format(filename)): with open('{}.xz'.format(filename), 'rb') as f: return lzma.LZMADecompressor().decompress(f.read()).decode('utf-8') else: return None",3
1655,Python,how to read the contents of a gz compressed file,https://github.com/onecodex/onecodex/blob/326a0a1af140e3a57ccf31c3c9c5e17a5775c13d/onecodex/lib/upload.py#L225-L246,"def _file_size(file_path, uncompressed=False): """"""Return size of a single file, compressed or uncompressed"""""" _, ext = os.path.splitext(file_path) if uncompressed: if ext in {"".gz"", "".gzip""}: with gzip.GzipFile(file_path, mode=""rb"") as fp: try: fp.seek(0, os.SEEK_END) return fp.tell() except ValueError: # on python2, cannot seek from end and must instead read to end fp.seek(0) while len(fp.read(8192)) != 0: pass return fp.tell() elif ext in {"".bz"", "".bz2"", "".bzip"", "".bzip2""}: with bz2.BZ2File(file_path, mode=""rb"") as fp: fp.seek(0, os.SEEK_END) return fp.tell() return os.path.getsize(file_path)",3
790,Python,how to read the contents of a gz compressed file,https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/utils/gzip_util.py#L74-L106,"def compress_file(fh_, compresslevel=9, chunk_size=1048576): ''' Generator that reads chunk_size bytes at a time from a file/filehandle and yields the compressed result of each read. .. note:: Each chunk is compressed separately. They cannot be stitched together to form a compressed file. This function is designed to break up a file into compressed chunks for transport and decompression/reassembly on a remote host. ''' try: bytes_read = int(chunk_size) if bytes_read != chunk_size: raise ValueError except ValueError: raise ValueError('chunk_size must be an integer') try: while bytes_read == chunk_size: buf = BytesIO() with open_fileobj(buf, 'wb', compresslevel) as ogz: try: bytes_read = ogz.write(fh_.read(chunk_size)) except AttributeError: # Open the file and re-attempt the read fh_ = salt.utils.files.fopen(fh_, 'rb') bytes_read = ogz.write(fh_.read(chunk_size)) yield buf.getvalue() finally: try: fh_.close() except AttributeError: pass",2
1900,Python,how to read the contents of a gz compressed file,https://github.com/lowandrew/OLCTools/blob/88aa90ac85f84d0bbeb03e43c29b0a9d36e4ce2a/spadespipeline/fileprep.py#L28-L47,"def prep(self): while True: sample = self.queue.get() # Don't make the file if it already exists if not os.path.isfile(sample.general.combined): # Iterate through the uncompressed .fastq file(s) for read in sample.general.fastqfiles: # Only decompress if the reads are gzipped if '.gz' in read: with open(sample.general.combined, 'wb') as combined: # Open the .fastq file with gzip with gzip.open(read, 'rb') as fastq: # Read the file contents and write them to the combined file combined.write(fastq.read()) else: with open(sample.general.combined, 'w') as combined: with open(read, 'r') as fastq: # Read in data and write it to file combined.write(fastq.read()) self.queue.task_done()",2
118,Python,how to read the contents of a gz compressed file,https://github.com/sunlightlabs/django-mediasync/blob/aa8ce4cfff757bbdb488463c64c0863cca6a1932/mediasync/__init__.py#L33-L38,"def compress(s): zbuf = cStringIO.StringIO() zfile = gzip.GzipFile(mode='wb', compresslevel=6, fileobj=zbuf) zfile.write(s) zfile.close() return zbuf.getvalue()",1
305,Python,how to read the contents of a gz compressed file,https://github.com/EntilZha/PyFunctional/blob/ac04e4a8552b0c464a7f492f7c9862424867b63e/functional/io.py#L98-L113,"def __iter__(self): if 't' in self.mode: with gzip.GzipFile(self.path, compresslevel=self.compresslevel) as gz_file: gz_file.read1 = gz_file.read with io.TextIOWrapper(gz_file, encoding=self.encoding, errors=self.errors, newline=self.newline) as file_content: for line in file_content: yield line else: with gzip.open(self.path, mode=self.mode, compresslevel=self.compresslevel) as file_content: for line in file_content: yield line",1
383,Python,how to read the contents of a gz compressed file,https://github.com/saltstack/salt/blob/e8541fd6e744ab0df786c0f76102e41631f45d46/salt/utils/gzip_util.py#L43-L51,"def open_fileobj(fileobj, mode='rb', compresslevel=9): if hasattr(gzip.GzipFile, '__enter__'): return gzip.GzipFile( filename='', mode=mode, fileobj=fileobj, compresslevel=compresslevel ) return GzipFile( filename='', mode=mode, fileobj=fileobj, compresslevel=compresslevel )",1
921,Python,how to read the contents of a gz compressed file,https://github.com/gtaylor/django-athumb/blob/69261ace0dff81e33156a54440874456a7b38dfb/athumb/backends/s3boto.py#L143-L150,"def _compress_content(self, content): """"""Gzip a given string."""""" zbuf = StringIO() zfile = GzipFile(mode='wb', compresslevel=6, fileobj=zbuf) zfile.write(content.read()) zfile.close() content.file = zbuf return content",1
30,Python,how to read the contents of a gz compressed file,https://github.com/log2timeline/plaso/blob/9c564698d2da3ffbe23607a3c54c0582ea18a6cc/plaso/cli/tools.py#L227-L239,"def AddLogFileOptions(self, argument_group): """"""Adds the log file option to the argument group. Args: argument_group (argparse._ArgumentGroup): argparse argument group. """""" argument_group.add_argument( '--logfile', '--log_file', '--log-file', action='store', metavar='FILENAME', dest='log_file', type=str, default='', help=( 'Path of the file in which to store log messages, by default ' 'this file will be named: ""{0:s}-YYYYMMDDThhmmss.log.gz"". Note ' 'that the file will be gzip compressed if the extension is ' '"".gz"".').format(self.NAME))",0
131,Python,how to read csv file in an efficient way,https://github.com/sparklingpandas/sparklingpandas/blob/7d549df4348c979042b683c355aa778fc6d3a768/sparklingpandas/pcontext.py#L68-L155,"def read_csv(self, file_path, use_whole_file=False, names=None, skiprows=0, *args, **kwargs): """"""Read a CSV file in and parse it into Pandas DataFrames. By default, the first row from the first partition of that data is parsed and used as the column names for the data from. If no 'names' param is provided we parse the first row of the first partition of data and use it for column names. Parameters ---------- file_path: string Path to input. Any valid file path in Spark works here, eg: 'file:///my/path/in/local/file/system' or 'hdfs:/user/juliet/' use_whole_file: boolean Whether of not to use the whole file. names: list of strings, optional skiprows: integer, optional indicates how many rows of input to skip. This will only be applied to the first partition of the data (so if #skiprows > #row in first partition this will not work). Generally this shouldn't be an issue for small values of skiprows. No other value of header is supported. All additional parameters available in pandas.read_csv() are usable here. Returns ------- A SparklingPandas DataFrame that contains the data from the specified file. """""" def csv_file(partition_number, files): # pylint: disable=unexpected-keyword-arg file_count = 0 for _, contents in files: # Only skip lines on the first file if partition_number == 0 and file_count == 0 and _skiprows > 0: yield pandas.read_csv( sio(contents), *args, header=None, names=mynames, skiprows=_skiprows, **kwargs) else: file_count += 1 yield pandas.read_csv( sio(contents), *args, header=None, names=mynames, **kwargs) def csv_rows(partition_number, rows): # pylint: disable=unexpected-keyword-arg in_str = ""\n"".join(rows) if partition_number == 0: return iter([ pandas.read_csv( sio(in_str), *args, header=None, names=mynames, skiprows=_skiprows, **kwargs)]) else: # could use .iterows instead? return iter([pandas.read_csv(sio(in_str), *args, header=None, names=mynames, **kwargs)]) # If we need to peak at the first partition and determine the column # names mynames = None _skiprows = skiprows if names: mynames = names else: # In the future we could avoid this expensive call. first_line = self.spark_ctx.textFile(file_path).first() frame = pandas.read_csv(sio(first_line), **kwargs) # pylint sees frame as a tuple despite it being a DataFrame mynames = list(frame.columns) _skiprows += 1 # Do the actual load if use_whole_file: return self.from_pandas_rdd( self.spark_ctx.wholeTextFiles(file_path) .mapPartitionsWithIndex(csv_file)) else: return self.from_pandas_rdd( self.spark_ctx.textFile(file_path) .mapPartitionsWithIndex(csv_rows))",3
201,Python,how to read csv file in an efficient way,https://github.com/probcomp/crosscat/blob/4a05bddb06a45f3b7b3e05e095720f16257d1535/src/utils/data_utils.py#L297-L304,"def read_csv(filename, has_header=True): with open(filename) as fh: csv_reader = csv.reader(fh) header = None if has_header: header = csv_reader.next() rows = [row for row in csv_reader] return header, rows",3
344,Python,how to read csv file in an efficient way,https://github.com/lappis-unb/salic-ml/blob/1b3ebc4f8067740999897ccffd9892dc94482a93/src/salicml/utils/read_csv.py#L10-L15,"def read_csv(csv_name, usecols=None): """"""Returns a DataFrame from a .csv file stored in /data/raw/"""""" csv_path = os.path.join(DATA_FOLDER, csv_name) csv = pd.read_csv(csv_path, low_memory=False, usecols=usecols, encoding=""utf-8"") return csv",3
422,Python,how to read csv file in an efficient way,https://github.com/remix/partridge/blob/0ba80fa30035e5e09fd8d7a7bdf1f28b93d53d03/partridge/gtfs.py#L89-L112,"def _read_csv(self, filename: str) -> pd.DataFrame: path = self._pathmap.get(filename) columns = self._config.nodes.get(filename, {}).get(""required_columns"", []) if path is None or os.path.getsize(path) == 0: # The file is missing or empty. Return an empty # DataFrame containing any required columns. return empty_df(columns) # If the file isn't in the zip, return an empty DataFrame. with open(path, ""rb"") as f: encoding = detect_encoding(f) df = pd.read_csv(path, dtype=np.unicode, encoding=encoding, index_col=False) # Strip leading/trailing whitespace from column names df.rename(columns=lambda x: x.strip(), inplace=True) if not df.empty: # Strip leading/trailing whitespace from column values for col in df.columns: df[col] = df[col].str.strip() return df",3
867,Python,how to read csv file in an efficient way,https://github.com/mapillary/mapillary_tools/blob/816785e90c589cae6e8e34a5530ce8417d29591c/mapillary_tools/process_csv.py#L175-L184,"def read_csv(csv_path, delimiter="","", header=False): csv_data = [] with open(csv_path, 'r') as csvfile: csvreader = csv.reader(csvfile, delimiter=delimiter) if header: next(csvreader, None) csv_data = zip(*csvreader) return csv_data",3
196,Python,how to read csv file in an efficient way,https://github.com/openvax/pyensembl/blob/4b995fb72e848206d6fbf11950cf30964cd9b3aa/pyensembl/memory_cache.py#L64-L72,"def _read_csv(self, csv_path): logger.info(""Reading Dataframe from %s"", csv_path) df = pd.read_csv(csv_path) if 'seqname' in df: # by default, Pandas will infer the type as int, # then switch to str when it hits non-numerical # chromosomes. Make sure whole column has the same type df['seqname'] = df['seqname'].map(str) return df",2
240,Python,how to read csv file in an efficient way,https://github.com/dshean/pygeotools/blob/5ac745717c0098d01eb293ff1fe32fd7358c76ab/pygeotools/lib/iolib.py#L604-L626,"def readcsv(fn): """""" Wrapper to read arbitrary csv, check for header Needs some work to be more robust, quickly added for demcoreg sampling """""" import csv #Check first line for header with open(fn, 'r') as f: reader = csv.DictReader(f) hdr = reader.fieldnames #Assume there is a header on first line, check skiprows = 1 if np.all(f.isdigit() for f in hdr): hdr = None skiprows = 0 #Check header for lat/lon/z or x/y/z tags #Should probably do genfromtxt here if header exists and dtype of cols is variable pts = np.loadtxt(fn, delimiter=',', skiprows=skiprows, dtype=None) return pts",2
405,Python,how to read csv file in an efficient way,https://github.com/thespacedoctor/sherlock/blob/2c80fb6fa31b04e7820e6928e3d437a21e692dd3/sherlock/imports/ned_d.py#L177-L274,"def _create_dictionary_of_ned_d( self): """"""create a list of dictionaries containing all the rows in the ned_d catalogue **Return:** - ``dictList`` - a list of dictionaries containing all the rows in the ned_d catalogue .. todo :: - update key arguments values and definitions with defaults - update return values and definitions - update usage examples and text - update docstring text - check sublime snippet exists - clip any useful text to docs mindmap - regenerate the docs and check redendering of this docstring """""" self.log.debug( 'starting the ``_create_dictionary_of_ned_d`` method') count = 0 with open(self.pathToDataFile, 'rb') as csvFile: csvReader = csv.reader( csvFile, dialect='excel', delimiter=',', quotechar='""') totalRows = sum(1 for row in csvReader) csvFile.close() totalCount = totalRows with open(self.pathToDataFile, 'rb') as csvFile: csvReader = csv.reader( csvFile, dialect='excel', delimiter=',', quotechar='""') theseKeys = [] dictList = [] for row in csvReader: if len(theseKeys) == 0: totalRows -= 1 if ""Exclusion Code"" in row and ""Hubble const."" in row: for i in row: if i == ""redshift (z)"": theseKeys.append(""redshift"") elif i == ""Hubble const."": theseKeys.append(""hubble_const"") elif i == ""G"": theseKeys.append(""galaxy_index_id"") elif i == ""err"": theseKeys.append(""dist_mod_err"") elif i == ""D (Mpc)"": theseKeys.append(""dist_mpc"") elif i == ""Date (Yr. - 1980)"": theseKeys.append(""ref_date"") elif i == ""REFCODE"": theseKeys.append(""ref"") elif i == ""Exclusion Code"": theseKeys.append(""dist_in_ned_flag"") elif i == ""Adopted LMC modulus"": theseKeys.append(""lmc_mod"") elif i == ""m-M"": theseKeys.append(""dist_mod"") elif i == ""Notes"": theseKeys.append(""notes"") elif i == ""SN ID"": theseKeys.append(""dist_derived_from_sn"") elif i == ""method"": theseKeys.append(""dist_method"") elif i == ""Galaxy ID"": theseKeys.append(""primary_ned_id"") elif i == ""D"": theseKeys.append(""dist_index_id"") else: theseKeys.append(i) continue if len(theseKeys): count += 1 if count > 1: # Cursor up one line and clear line sys.stdout.write(""\x1b[1A\x1b[2K"") if count > totalCount: count = totalCount percent = (float(count) / float(totalCount)) * 100. print ""%(count)s / %(totalCount)s (%(percent)1.1f%%) rows added to memory"" % locals() rowDict = {} for t, r in zip(theseKeys, row): rowDict[t] = r if t == ""ref_date"": try: rowDict[t] = int(r) + 1980 except: rowDict[t] = None if rowDict[""dist_index_id""] != ""999999"": dictList.append(rowDict) csvFile.close() self.log.debug( 'completed the ``_create_dictionary_of_ned_d`` method') return dictList",2
1741,Python,how to read csv file in an efficient way,https://github.com/PeerAssets/pypeerassets/blob/8927b4a686887f44fe2cd9de777e2c827c948987/pypeerassets/transactions.py#L99-L114,"def serialize(self): from itertools import chain result = Stream() result << self.version.to_bytes(4, 'little') if self.network.tx_timestamp: result << self.timestamp.to_bytes(4, 'little') result << Parser.to_varint(len(self.ins)) # the most efficient way to flatten a list in python result << bytearray(chain.from_iterable(txin.serialize() for txin in self.ins)) result << Parser.to_varint(len(self.outs)) # the most efficient way to flatten a list in python result << bytearray(chain.from_iterable(txout.serialize() for txout in self.outs)) result << self.locktime return result.serialize()",1
523,Python,how to read csv file in an efficient way,https://github.com/meejah/txtorcon/blob/14053b95adf0b4bd9dd9c317bece912a26578a93/txtorcon/socks.py#L119-L129,"def send_data(self, callback): """""" drain all pending data by calling `callback()` on it """""" # a ""for x in self._outgoing_data"" would potentially be more # efficient, but then there's no good way to bubble exceptions # from callback() out without lying about how much data we # processed .. or eat the exceptions in here. while len(self._outgoing_data): data = self._outgoing_data.pop(0) callback(data)",0
245,Python,how to randomly pick a number,https://github.com/HacKanCuBa/passphrase-py/blob/219d6374338ed9a1475b4f09b0d85212376f11e0/passphrase/secrets.py#L63-L84,"def randbelow(num: int) -> int: """"""Return a random int in the range [0,num). Raises ValueError if num <= 0, and TypeError if it's not an integer. >>> randbelow(16) #doctest:+SKIP 13 """""" if not isinstance(num, int): raise TypeError('number must be an integer') if num <= 0: raise ValueError('number must be greater than zero') if num == 1: return 0 # https://github.com/python/cpython/blob/3.6/Lib/random.py#L223 nbits = num.bit_length() # don't use (n-1) here because n can be 1 randnum = random_randint(nbits) # 0 <= randnum < 2**nbits while randnum >= num: randnum = random_randint(nbits) return randnum",3
667,Python,how to randomly pick a number,https://github.com/linnarsson-lab/loompy/blob/62c8373a92b058753baa3a95331fb541f560f599/loompy/color.py#L288-L289,"def random_within(self, r): return self.random.randint(int(r[0]), int(r[1]))",3
941,Python,how to randomly pick a number,https://github.com/bwesterb/sarah/blob/a9e46e875dfff1dc11255d714bb736e5eb697809/src/drv.py#L62-L67,"def pick(self): while True: idx = random.randint(0, len(self.values) - 1) v, p = self.values[idx] if p >= random.uniform(0, 1): return v",3
963,Python,how to randomly pick a number,https://github.com/bwesterb/sarah/blob/a9e46e875dfff1dc11255d714bb736e5eb697809/src/drv.py#L24-L47,"def pick(self): """""" picks a value accoriding to the given density """""" v = random.uniform(0, self.ub) d = self.dist c = self.vc - 1 s = self.vc while True: s = s / 2 if s == 0: break if v <= d[c][1]: c -= s else: c += s # we only need this logic when increasing c while len(d) <= c: s = s / 2 c -= s if s == 0: break # we may have converged from the left, instead of the right if c == len(d) or v <= d[c][1]: c -= 1 return d[c][0]",3
1626,Python,how to randomly pick a number,https://github.com/tensorlayer/tensorlayer/blob/aa9e52e36c7058a7e6fd81d36563ca6850b21956/tensorlayer/utils.py#L515-L539,"def get_random_int(min_v=0, max_v=10, number=5, seed=None): """"""Return a list of random integer by the given range and quantity. Parameters ----------- min_v : number The minimum value. max_v : number The maximum value. number : int Number of value. seed : int or None The seed for random. Examples --------- >>> r = get_random_int(min_v=0, max_v=10, number=5) [10, 2, 3, 3, 7] """""" rnd = random.Random() if seed: rnd = random.Random(seed) # return [random.randint(min,max) for p in range(0, number)] return [rnd.randint(min_v, max_v) for p in range(0, number)]",3
12,Python,how to randomly pick a number,https://github.com/eddiejessup/spatious/blob/b7ae91bec029e85a45a7f303ee184076433723cd/spatious/vector.py#L185-L216,"def sphere_pick_polar(d, n=1, rng=None): """"""Return vectors uniformly picked on the unit sphere. Vectors are in a polar representation. Parameters ---------- d: float The number of dimensions of the space in which the sphere lives. n: integer Number of samples to pick. Returns ------- r: array, shape (n, d) Sample vectors. """""" if rng is None: rng = np.random a = np.empty([n, d]) if d == 1: a[:, 0] = rng.randint(2, size=n) * 2 - 1 elif d == 2: a[:, 0] = 1.0 a[:, 1] = rng.uniform(-np.pi, +np.pi, n) elif d == 3: u, v = rng.uniform(0.0, 1.0, (2, n)) a[:, 0] = 1.0 a[:, 1] = np.arccos(2.0 * v - 1.0) a[:, 2] = 2.0 * np.pi * u else: raise Exception('Invalid vector for polar representation') return a",2
1282,Python,how to randomly pick a number,https://github.com/ManiacalLabs/BiblioPixelAnimations/blob/fba81f6b94f5265272a53f462ef013df1ccdb426/BiblioPixelAnimations/strip/WhiteTwinkle.py#L68-L77,"def pick_led(self, inc): # Pick a random led, if it's off bump it up an even number so it gets brighter idx = random.randrange(0, self.layout.numLEDs) this_led = self.layout.get(idx) r = this_led[0] if random.randrange(0, self._maxLed) < self.density: if r == 0: r += inc self.layout.set(idx, (2, 2, 2))",2
32,Python,how to randomly pick a number,https://github.com/HacKanCuBa/passphrase-py/blob/219d6374338ed9a1475b4f09b0d85212376f11e0/passphrase/passphrase.py#L71-L76,"def randnum_min(self, randnum: int) -> None: if not isinstance(randnum, int): raise TypeError('randnum_min can only be int') if randnum < 0: raise ValueError('randnum_min should be greater than 0') self._randnum_min = randnum",1
876,Python,how to randomly pick a number,https://github.com/numenta/htmresearch/blob/70c096b09a577ea0432c3f3bfff4442d4871b7aa/htmresearch/frameworks/specific_timing/timing_adtm.py#L276-L306,"def tempoAdjust2(self, tempoFactor): """""" Adjust tempo by aggregating active basal cell votes for pre vs. post :param tempoFactor: scaling signal to MC clock from last sequence item :return: adjusted scaling signal """""" late_votes = (len(self.adtm.getNextBasalPredictedCells()) - len(self.apicalIntersect)) * -1 early_votes = len(self.apicalIntersect) votes = late_votes + early_votes print('vote tally', votes) if votes > 0: tempoFactor = tempoFactor * 0.5 print 'speed up' elif votes < 0: tempoFactor = tempoFactor * 2 print 'slow down' elif votes == 0: print 'pick randomly' if random.random() > 0.5: tempoFactor = tempoFactor * 0.5 print 'random pick: speed up' else: tempoFactor = tempoFactor * 2 print 'random pick: slow down' return tempoFactor",1
427,Python,how to randomly pick a number,https://github.com/alvarogzp/telegram-bot-framework/blob/7b597a415c1901901c677976cb13100fc3083107/bot/action/extra/random.py#L35-L47,"def get_help(event): args = [ """", ""start end"", ""option1 {line-break} option2 [{line-break} option 3 ...]"" ] description = ( ""Without arguments, display a random float number in the range \[0, 1).\n\n"" ""Add two integers separated by a space to get a random number in that range,"" "" including both endpoints: \[start, end].\n\n"" ""Put various options, each one in a different line to get one chosen randomly."" ) return CommandUsageMessage.get_usage_message(event.command, args, description)",0
141,Python,how to make the checkbox checked,https://github.com/aloetesting/aloe_webdriver/blob/65d847da4bdc63f9c015cb19d4efdee87df8ffad/aloe_webdriver/__init__.py#L542-L547,"def check_checkbox(self, value): """"""Check the checkbox with label (recommended), name or id."""""" check_box = find_field(world.browser, 'checkbox', value) assert check_box, ""Cannot find checkbox '{}'."".format(value) if not check_box.is_selected(): check_box.click()",3
189,Python,how to make the checkbox checked,https://github.com/Shapeways/coyote_framework/blob/cb29899b984a21d56bf65d0b1d907073948fe16c/coyote_framework/webdriver/webdriverwrapper/WebElementWrapper.py#L655-L660,"def checkbox_check(self, force_check=False): """""" Wrapper to check a checkbox """""" if not self.get_attribute('checked'): self.click(force_click=force_check)",3
488,Python,how to make the checkbox checked,https://github.com/jbm950/pygame_toolbox/blob/3fe32145fc149e4dd0963c30a2b6a4dddd4fac0e/pygame_toolbox/graphics/widgets.py#L141-L162,"def __call__(self, *arg): if self.status: # If the checkbox was previously checked, set the status # to 0 and turn the whole box back to white. self.status = 0 self.image.fill((255, 255, 255)) # Draw the checkbox on the external surface ptg.Button.set_position(self, self.position, self.midpoint, self.surface) else: # If the checkbox was previously unchecked, set the status # to 1 and draw a check in the center of the checkbox. self.status = 1 if self.checktype == 'r': self.draw_rect_check() elif self.checktype == 'c': self.draw_circle_check() # Draw the new image on the external surface ptg.Button.set_position(self, self.position, self.midpoint, self.surface)",3
797,Python,how to make the checkbox checked,https://github.com/twosigma/beakerx/blob/404de61ed627d9daaf6b77eb4859e7cb6f37413f/beakerx/beakerx/easyform/easyform.py#L112-L117,"def addCheckBox(self, *args, **kwargs): checkbox = BeakerxCheckbox(description=self.getDescription(args, kwargs)) checkbox.value = getValue(kwargs, 'value', False) self.children += (checkbox,) self.components[checkbox.description] = checkbox return checkbox",3
1537,Python,how to make the checkbox checked,https://github.com/aloetesting/aloe_webdriver/blob/65d847da4bdc63f9c015cb19d4efdee87df8ffad/aloe_webdriver/__init__.py#L564-L568,"def assert_checked_checkbox(self, value): """"""Assert the checkbox with label (recommended), name or id is checked."""""" check_box = find_field(world.browser, 'checkbox', value) assert check_box, ""Cannot find checkbox '{}'."".format(value) assert check_box.is_selected(), ""Check box should be selected.""",3
1609,Python,how to make the checkbox checked,https://github.com/Shapeways/coyote_framework/blob/cb29899b984a21d56bf65d0b1d907073948fe16c/coyote_framework/webdriver/webdriverwrapper/WebElementWrapper.py#L662-L667,"def checkbox_uncheck(self, force_check=False): """""" Wrapper to uncheck a checkbox """""" if self.get_attribute('checked'): self.click(force_click=force_check)",3
1662,Python,how to make the checkbox checked,https://github.com/bbangert/lettuce_webdriver/blob/d11f8531c43bb7150c316e0dc4ccd083617becf7/lettuce_webdriver/webdriver.py#L337-L341,"def check_checkbox(step, value): with AssertContextManager(step): check_box = find_field(world.browser, 'checkbox', value) if not check_box.is_selected(): check_box.click()",3
839,Python,how to make the checkbox checked,https://github.com/nion-software/nionswift/blob/d43693eaf057b8683b9638e575000f055fede452/nion/swift/Facade.py#L612-L615,"def create_check_box_widget(self, text=None): check_box_widget = CheckBoxWidget(self.__ui) check_box_widget.text = text return check_box_widget",2
903,Python,how to make the checkbox checked,https://github.com/DLR-RM/RAFCON/blob/24942ef1a904531f49ab8830a1dbb604441be498/source/rafcon/gui/utils/dialog.py#L233-L234,"def get_checkbox_state_by_name(self, checkbox_text): return [checkbox.get_active() for checkbox in self.checkboxes if checkbox.get_label() == checkbox_text]",1
1771,Python,how to make the checkbox checked,https://github.com/PySimpleGUI/PySimpleGUI/blob/08184197f5bd4580ab5e5aca28bdda30f87b86fc/PySimpleGUI27.py#L486-L493,def CheckboxHandler(self): if self.Key is not None: self.ParentForm.LastButtonClicked = self.Key else: self.ParentForm.LastButtonClicked = '' self.ParentForm.FormRemainedOpen = True if self.ParentForm.CurrentlyRunningMainloop: self.ParentForm.TKroot.quit(),1
153,Python,how to get html of website,https://github.com/dwillis/python-espncricinfo/blob/96469e39f309e28586fcec40cc6a20b2fddacff7/espncricinfo/player.py#L44-L50,"def get_html(self): r = requests.get(self.url) if r.status_code == 404: raise PlayerNotFoundError else: soup = BeautifulSoup(r.text, 'html.parser') return soup.find(""div"", class_=""pnl490M"")",3
208,Python,how to get html of website,https://github.com/sirfoga/pyhal/blob/4394d8a1f7e45bea28a255ec390f4962ee64d33a/hal/internet/web.py#L171-L180,"def get_html_source(self): """"""Gets source page of url :return: HTML source """""" req = urllib.request.Request(self.url) req.add_header(""user-agent"", random.choice(USER_AGENTS)) req_text = urllib.request.urlopen(req).read() self.source = str(req_text) self.soup = BeautifulSoup(self.source, ""html.parser"") return self.source",3
1736,Python,how to get html of website,https://github.com/dariosky/wfcli/blob/87a9ed30dbd456f801135a55099f0541b0614ccb/wfcli/wfapi.py#L123-L134,"def update_website(self, website): self.connect() website = self.server.update_website( self.session_id, website['name'], website['ip'], website['https'], website['subdomains'], website['certificate'], *website['website_apps'] ) return website",3
1965,Python,how to get html of website,https://github.com/Netflix-Skunkworks/cloudaux/blob/c4b0870c3ac68b1c69e71d33cf78b6a8bdf437ea/cloudaux/orchestration/aws/s3.py#L160-L178,"def get_website(bucket_name, **conn): try: result = get_bucket_website(Bucket=bucket_name, **conn) except ClientError as e: if ""NoSuchWebsiteConfiguration"" not in str(e): raise e return None website = {} if result.get(""IndexDocument""): website[""IndexDocument""] = result[""IndexDocument""] if result.get(""RoutingRules""): website[""RoutingRules""] = result[""RoutingRules""] if result.get(""RedirectAllRequestsTo""): website[""RedirectAllRequestsTo""] = result[""RedirectAllRequestsTo""] if result.get(""ErrorDocument""): website[""ErrorDocument""] = result[""ErrorDocument""] return website",3
1724,Python,how to get html of website,https://github.com/GibbsConsulting/jupyter-plotly-dash/blob/19e3898372ddf7c1d20292eae1ea0df9e0808fe2/jupyter_plotly_dash/dash_wrapper.py#L104-L120,"def _repr_html_(self): url = self.get_app_root_url() da_id = self.session_id() comm = locate_jpd_comm(da_id, self, url[1:-1]) external = self.add_external_link and '<hr/><a href=""{url}"" target=""_new"">Open in new window</a>'.format(url=url) or """" fb = 'frameborder=""%i""' %(self.frame and 1 or 0) iframe = '''<div> <iframe src=""%(url)s"" width=%(width)s height=%(height)s %(frame)s></iframe> %(external)s for %(url)s </div>''' %{'url' : url, #'local_url' : local_url, 'da_id' : da_id, 'external' : external, 'width' : self.width, 'height' : self.height, 'frame': fb,} return iframe def callback(self, *args, **kwargs):",2
1152,Python,how to get html of website,https://github.com/Azure/azure-sdk-for-python/blob/d7306fde32f60a293a7567678692bdad31e4b667/azure-servicemanagement-legacy/azure/servicemanagement/websitemanagementservice.py#L103-L114,"def get_site(self, webspace_name, website_name): ''' List the web sites defined on this webspace. webspace_name: The name of the webspace. website_name: The name of the website. ''' return self._perform_get(self._get_sites_details_path(webspace_name, website_name), Site)",1
745,Python,how to get html of website,https://github.com/pyrogram/pyrogram/blob/e7258a341ba905cfa86264c22040654db732ec1c/examples/inline_queries.py#L16-L51,"def answer(client, inline_query): inline_query.answer( results=[ InlineQueryResultArticle( id=uuid4(), title=""Installation"", input_message_content=InputTextMessageContent( ""Here's how to install **Pyrogram**"" ), url=""https://docs.pyrogram.ml/start/Installation"", description=""How to install Pyrogram"", thumb_url=""https://i.imgur.com/JyxrStE.png"", reply_markup=InlineKeyboardMarkup( [ [InlineKeyboardButton(""Open website"", url=""https://docs.pyrogram.ml/start/Installation"")] ] ) ), InlineQueryResultArticle( id=uuid4(), title=""Usage"", input_message_content=InputTextMessageContent( ""Here's how to use **Pyrogram**"" ), url=""https://docs.pyrogram.ml/start/Usage"", description=""How to use Pyrogram"", thumb_url=""https://i.imgur.com/JyxrStE.png"", reply_markup=InlineKeyboardMarkup( [ [InlineKeyboardButton(""Open website"", url=""https://docs.pyrogram.ml/start/Usage"")] ] ) ) ], cache_time=1 )",0
753,Python,how to get html of website,https://github.com/inorton/junit2html/blob/73ff9d84c41b60148e86ce597ef605a0f1976d4b/junit2htmlreport/parser.py#L561-L574,"def html(self): """""" Render the test suite as a HTML report with links to errors first. :return: """""" page = self.get_html_head() page += ""<body><h1>Test Report</h1>"" page += self.toc() for suite in self.suites: page += suite.html() page += ""</body></html>"" return page",0
1318,Python,how to get html of website,https://github.com/audreyr/alotofeffort/blob/06deca82a70fa9896496fd44c8c6f24707396c50/alotofeffort/main.py#L9-L30,"def main(): """""" Entry point for the package, as defined in setup.py. """""" # Log info and above to console logging.basicConfig( format='%(levelname)s: %(message)s', level=logging.INFO) # Get command line input/output arguments msg = 'Instantly deploy static HTML sites to S3 at the command line.' parser = argparse.ArgumentParser(description=msg) parser.add_argument( 'www_dir', help='Directory containing the HTML files for your website.' ) parser.add_argument( 'bucket_name', help='Name of S3 bucket to deploy to, e.g. mybucket.' ) args = parser.parse_args() # Deploy the site to S3! deploy(args.www_dir, args.bucket_name)",0
1437,Python,how to get html of website,https://github.com/bitesofcode/projex/blob/d31743ec456a41428709968ab11a2cf6c6c76247/projex/init.py#L368-L384,"def website(app=None, mode='home', subcontext='UserGuide'): """""" Returns the website location for projex software. :param app | <str> || None mode | <str> (home, docs, blog, dev) :return <str> """""" base_url = WEBSITES.get(mode, '') if app and base_url: opts = {'app': app, 'base_url': base_url} base_url = SUBCONTEXT_MAP.get((mode, subcontext), base_url) base_url %= opts return base_url",0
1057,Python,how to get database table name,https://github.com/spulec/moto/blob/4a286c4bc288933bb023396e2784a6fdbb966bc9/moto/glue/models.py#L48-L53,"def get_table(self, database_name, table_name): database = self.get_database(database_name) try: return database.tables[table_name] except KeyError: raise TableNotFoundException(table_name)",3
1675,Python,how to get database table name,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/hive_hooks.py#L578-L591,"def get_table(self, table_name, db='default'): """"""Get a metastore table object >>> hh = HiveMetastoreHook() >>> t = hh.get_table(db='airflow', table_name='static_babynames') >>> t.tableName 'static_babynames' >>> [col.name for col in t.sd.cols] ['state', 'year', 'name', 'gender', 'num'] """""" if db == 'default' and '.' in table_name: db, table_name = table_name.split('.')[:2] with self.metastore as client: return client.get_table(dbname=db, tbl_name=table_name)",3
1827,Python,how to get database table name,https://github.com/spulec/moto/blob/4a286c4bc288933bb023396e2784a6fdbb966bc9/moto/glue/models.py#L55-L57,"def get_tables(self, database_name): database = self.get_database(database_name) return [table for table_name, table in database.tables.items()]",3
280,Python,how to get database table name,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_glue_catalog_hook.py#L139-L152,"def get_table_location(self, database_name, table_name): """""" Get the physical location of the table :param database_name: Name of hive database (schema) @table belongs to :type database_name: str :param table_name: Name of hive table :type table_name: str :return: str """""" table = self.get_table(database_name, table_name) return table['StorageDescriptor']['Location']",2
916,Python,how to get database table name,https://github.com/spulec/moto/blob/4a286c4bc288933bb023396e2784a6fdbb966bc9/moto/glue/responses.py#L36-L41,"def get_table(self): database_name = self.parameters.get('DatabaseName') table_name = self.parameters.get('Name') table = self.glue_backend.get_table(database_name, table_name) return json.dumps({'Table': table.as_dict()})",2
1009,Python,how to get database table name,https://github.com/firstprayer/monsql/blob/6285c15b574c8664046eae2edfeb548c7b173efd/monsql/wrapper_postgresql.py#L48-L50,"def get_table_obj(self, name): table = PostgreSQLTable(db=self.db, name=name, mode=self.mode) return table",2
502,Python,how to get database table name,https://github.com/simonw/datasette/blob/11b352b4d52fd02a422776edebb14f12e4994d3b/datasette/app.py#L521-L527,"def table_metadata(self, database, table): ""Fetch table-specific metadata."" return (self.metadata(""databases"") or {}).get(database, {}).get( ""tables"", {} ).get( table, {} )",1
1402,Python,how to get database table name,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_glue_catalog_hook.py#L120-L137,"def get_table(self, database_name, table_name): """""" Get the information of the table :param database_name: Name of hive database (schema) @table belongs to :type database_name: str :param table_name: Name of hive table :type table_name: str :rtype: dict >>> hook = AwsGlueCatalogHook() >>> r = hook.get_table('db', 'table_foo') >>> r['Name'] = 'table_foo' """""" result = self.get_conn().get_table(DatabaseName=database_name, Name=table_name) return result['Table']",1
742,Python,how to get database table name,https://github.com/wdbm/pyprel/blob/c1253ea3f8c60a2f5493a0d5a61ca3c84df7c21d/pyprel_examples_database.py#L54-L115,"def main(options): filename_database = options[""--database""] name_table = options[""--table""] print(""\npyprel database examples\n"") if os.path.exists(filename_database): print(""create database {database}"".format( database = filename_database )) create_database(filename = ""database.db"") print(""access database {filename}"".format( filename = filename_database )) database = dataset.connect( ""sqlite:///{filename_database}"".format( filename_database = filename_database ) ) table = database[name_table] print(""add data to database"") table.insert(dict( name = ""Legolas Greenleaf"", age = 2000, country = ""Mirkwood"", uuid4 = str(uuid.uuid4()) )) table.insert(dict( name = ""Cody Rapol"", age = 30, country = ""USA"", activity = ""DDR"", uuid4 = str(uuid.uuid4()) )) print( """""" database tables:\n{tables} \ntable {table} columns:\n{columns} \ntable {table} row one:\n{row} """""".format( tables = database.tables, table = name_table, columns = database[name_table].columns, row = [entry for entry in table.find(id = ""1"")] ) ) print(""table {table} printout:\n"".format( table = name_table )) print( pyprel.Table( contents = pyprel.table_dataset_database_table( table = database[name_table] ) ) )",0
1480,Python,how to get database table name,https://github.com/synw/dataswim/blob/4a4a53f80daa7cd8e8409d76a19ce07296269da2/dataswim/db/infos.py#L56-L78,"def table(self, name: str): """""" Display info about a table: number of rows and columns :param name: name of the table :type name: str :example: ``tables = ds.table(""mytable"")`` """""" if self._check_db() == False: return try: res = self.getall(name) except Exception as e: self.err(e, self.table, ""Can not get records from database"") return if res is None: self.warning(""Table"", name, ""does not contain any record"") return num = len(res) self.info(num, ""rows"") self.info(""Fields:"", "", "".join(list(res)))",0
27,Python,how to get current date,https://github.com/Open-ET/openet-core-beta/blob/f2b81ccf87bf7e7fe1b9f3dd1d4081d0ec7852db/openet/core/utils.py#L82-L95,"def date_0utc(date): """"""Get the 0 UTC date for a date Parameters ---------- date : ee.Date Returns ------- ee.Date """""" return ee.Date.fromYMD(date.get('year'), date.get('month'), date.get('day'))",3
401,Python,how to get current date,https://github.com/airbus-cert/mispy/blob/6d523d6f134d2bd38ec8264be74e73b68403da65/mispy/misp.py#L595-L605,"def date(self): """""" Getter/setter for the date member. The setter can take a string or a :meth:`datetime.datetime` and will do the appropriate transformation. """""" if self._date: return self._date return datetime.datetime.now().strftime('%Y-%m-%d')",3
901,Python,how to get current date,https://github.com/ctuning/ck/blob/7e009814e975f8742790d3106340088a46223714/ck/kernel.py#L4351-L4377,"def get_current_date_time(i): """""" Input: {} Output: { return - return code = 0 array - array with date and time iso_datetime - date and time in ISO format } """""" import datetime a={} now1=datetime.datetime.now() now=now1.timetuple() a['date_year']=now[0] a['date_month']=now[1] a['date_day']=now[2] a['time_hour']=now[3] a['time_minute']=now[4] a['time_second']=now[5] return {'return':0, 'array':a, 'iso_datetime':now1.isoformat()}",3
1075,Python,how to get current date,https://github.com/mitsei/dlkit/blob/445f968a175d61c8d92c0f617a3c17dc1dc7c584/dlkit/json_/osid/markers.py#L309-L318,"def is_effective(self): """"""Tests if the current date is within the start end end dates inclusive. return: (boolean) - ``true`` if this is effective, ``false`` otherwise *compliance: mandatory -- This method must be implemented.* """""" now = DateTime.utcnow() return self.get_start_date() <= now and self.get_end_date() >= now",2
1205,Python,how to get current date,https://github.com/crazy-canux/arguspy/blob/e9486b5df61978a990d56bf43de35f3a4cdefcc3/scripts/check_wmi_sh.py#L226-L238,"def __get_current_datetime(self): """"""Get current datetime for every file."""""" self.wql_time = ""SELECT LocalDateTime FROM Win32_OperatingSystem"" self.current_time = self.query(self.wql_time) # [{'LocalDateTime': '20160824161431.977000+480'}]' self.current_time_string = str( self.current_time[0].get('LocalDateTime').split('.')[0]) # '20160824161431' self.current_time_format = datetime.datetime.strptime( self.current_time_string, '%Y%m%d%H%M%S') # param: datetime.datetime(2016, 8, 24, 16, 14, 31) -> type: # datetime.datetime return self.current_time_format",2
1324,Python,how to get current date,https://github.com/caktus/django-timepiece/blob/52515dec027664890efbc535429e1ba1ee152f40/timepiece/entries/views.py#L44-L55,"def get_dates(self): today = datetime.date.today() day = today if 'week_start' in self.request.GET: param = self.request.GET.get('week_start') try: day = datetime.datetime.strptime(param, '%Y-%m-%d').date() except: pass week_start = utils.get_week_start(day) week_end = week_start + relativedelta(days=6) return today, week_start, week_end",2
1939,Python,how to get current date,https://github.com/ricobl/django-importer/blob/6967adfa7a286be7aaf59d3f33c6637270bd9df6/sample_project/tasks/importers.py#L64-L88,"def parse_date(self, item, field_name, source_name): """""" Converts the date in the format: Thu 03. As only the day is provided, tries to find the best match based on the current date, considering that dates are on the past. """""" # Get the current date now = datetime.now().date() # Get the date from the source val = self.get_value(item, source_name) week_day, day = val.split() day = int(day) # If the current date is minor than the item date # go back one month if now.day < day: if now.month == 1: now = now.replace(month=12, year=now.year-1) else: now = now.replace(month=now.month-1) # Finally, replace the source day in the current date # and return now = now.replace(day=day) return now",2
801,Python,how to get current date,https://github.com/alexhayes/django-toolkit/blob/b64106392fad596defc915b8235fe6e1d0013b5b/django_toolkit/views.py#L343-L353,"def get_date(self): """""" Return (date_list, items, extra_context) for this request. """""" year = self.get_year() month = self.get_month() day = self.get_day() return _date_from_string(year, self.get_year_format(), month, self.get_month_format(), day, self.get_day_format())",1
1336,Python,how to get current date,https://github.com/iloob/python-periods/blob/8988373522907d72c0ee5896c2ffbb573a8500d9/periods/week.py#L21-L30,"def __repr__(self): if self.get_start_date().year != self.get_end_date().year: if self.week == 1: return ""%sW1"" % self.get_end_date().year elif self.week > 51: return ""%sW%s"" % (self.get_start_date().year, self.get_start_date().isocalendar()[1]) return ""%sW%s"" % (self.get_start_date().year, self.get_start_date().isocalendar()[1])",0
317,Python,how to extract zip file recursively,https://github.com/bokeh/bokeh/blob/dc8cf49e4e4302fd38537ad089ece81fbcca4737/examples/app/stocks/download_sample_data.py#L27-L61,"def extract_zip(zip_name, exclude_term=None): """"""Extracts a zip file to its containing directory."""""" zip_dir = os.path.dirname(os.path.abspath(zip_name)) try: with zipfile.ZipFile(zip_name) as z: # write each zipped file out if it isn't a directory files = [zip_file for zip_file in z.namelist() if not zip_file.endswith('/')] print('Extracting %i files from %r.' % (len(files), zip_name)) for zip_file in files: # remove any provided extra directory term from zip file if exclude_term: dest_file = zip_file.replace(exclude_term, '') else: dest_file = zip_file dest_file = os.path.normpath(os.path.join(zip_dir, dest_file)) dest_dir = os.path.dirname(dest_file) # make directory if it does not exist if not os.path.isdir(dest_dir): os.makedirs(dest_dir) # read file from zip, then write to new directory data = z.read(zip_file) with open(dest_file, 'wb') as f: f.write(encode_utf8(data)) except zipfile.error as e: print(""Bad zipfile (%r): %s"" % (zip_name, e)) raise e",3
319,Python,how to extract zip file recursively,https://github.com/buildinspace/peru/blob/76e4012c6c34e85fb53a4c6d85f4ac3633d93f77/peru/resources/plugins/curl/curl_plugin.py#L117-L137,"def extract_zip(archive_path, dest): with zipfile.ZipFile(archive_path) as z: validate_filenames(z.namelist()) z.extractall(dest) # Set file permissions. Tar does this by default, but with zip we need # to do it ourselves. for info in z.filelist: if not info.filename.endswith('/'): # This is how to get file permissions out of a zip archive, # according to http://stackoverflow.com/q/434641/823869 and # http://bugs.python.org/file34873/issue15795_cleaned.patch. mode = (info.external_attr >> 16) & 0o777 # Don't copy the whole mode, just set the executable bit. Two # reasons for this. 1) This is all going to end up in a git # tree, which only records the executable bit anyway. 2) Zip's # support for Unix file modes is nonstandard, so the mode field # is often zero and could be garbage. Mistakenly setting a file # executable isn't a big deal, but e.g. removing read # permissions would cause an error. if mode & stat.S_IXUSR: os.chmod(os.path.join(dest, info.filename), 0o755)",3
744,Python,how to extract zip file recursively,https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/helpers/utils.py#L43-L65,"def extract_zipdir(zip_file): """""" Extract contents of zip file into subfolder in parent directory. Parameters ---------- zip_file : str Path to zip file Returns ------- str : folder where the zip was extracted """""" if not os.path.exists(zip_file): raise ValueError('{} does not exist'.format(zip_file)) directory = os.path.dirname(zip_file) filename = os.path.basename(zip_file) dirpath = os.path.join(directory, filename.replace('.zip', '')) with zipfile.ZipFile(zip_file, 'r', zipfile.ZIP_DEFLATED) as zipf: zipf.extractall(dirpath) return dirpath",3
1425,Python,how to extract zip file recursively,https://github.com/samuelcolvin/grablib/blob/2fca8a3950f29fb2a97a7bd75c0839060a91cedf/grablib/download.py#L125-L152,"def _extract_zip(self, url, content, value): zipinmemory = IO(content) zcopied = 0 with zipfile.ZipFile(zipinmemory) as zipf: progress_logger.debug('%d files in zip archive', len(zipf.namelist())) for filepath in zipf.namelist(): if filepath.endswith('/'): continue regex_pattern, targets = None, None for r, t in value.items(): if re.match(r, filepath): regex_pattern, targets = r, t break if regex_pattern is None: progress_logger.debug('""%s"" no target found', filepath) elif targets is None: progress_logger.debug('""%s"" skipping (regex: ""%s"")', filepath, regex_pattern) else: if isinstance(targets, str): targets = [targets] for target in targets: new_path = self._file_path(filepath, target, regex=regex_pattern) progress_logger.debug('""%s"" ➤ ""%s"" (regex: ""%s"")', filepath, new_path.relative_to(self.download_root), regex_pattern) self._write(new_path, zipf.read(filepath), url) zcopied += 1 return zcopied",3
1546,Python,how to extract zip file recursively,https://github.com/jedie/DragonPy/blob/6659e5b5133aab26979a498ee7453495773a4f6c/dragonpy/components/rom.py#L101-L134,"def extract_zip(self): assert self.FILE_COUNT>0 try: with zipfile.ZipFile(self.archive_path, ""r"") as zip: namelist = zip.namelist() print(""namelist():"", namelist) if len(namelist) != self.FILE_COUNT: msg = ( ""Wrong archive content?!?"" "" There exists %i files, but it should exist %i."" ""Existing names are: %r"" ) % (len(namelist), self.FILE_COUNT, namelist) log.error(msg) raise RuntimeError(msg) for filename in namelist: content = zip.read(filename) dst = self.file_rename(filename) out_filename=os.path.join(self.ROM_PATH, dst) with open(out_filename, ""wb"") as f: f.write(content) if dst == filename: print(""%r extracted"" % out_filename) else: print(""%r extracted to %r"" % (filename, out_filename)) self.post_processing(out_filename) except BadZipFile as err: msg = ""Error extracting archive %r: %s"" % (self.archive_path, err) log.error(msg) raise BadZipFile(msg)",3
1576,Python,how to extract zip file recursively,https://github.com/jedie/DragonPy/blob/6659e5b5133aab26979a498ee7453495773a4f6c/dragonpy/Simple6809/Simple6809_rom.py#L40-L66,"def extract_zip(self): assert self.FILE_COUNT>0 try: with zipfile.ZipFile(self.archive_path, ""r"") as zip: namelist = zip.namelist() print(""namelist():"", namelist) if namelist != self.ARCHIVE_NAMES: msg = ( ""Wrong archive content?!?"" "" namelist should be: %r"" ) % self.ARCHIVE_NAMES log.error(msg) raise RuntimeError(msg) zip.extractall(path=self.ROM_PATH) except BadZipFile as err: msg = ""Error extracting archive %r: %s"" % (self.archive_path, err) log.error(msg) raise BadZipFile(msg) hex2bin( src=os.path.join(self.ROM_PATH, ""ExBasROM.hex""), dst=self.rom_path, verbose=False )",3
121,Python,how to extract zip file recursively,https://github.com/ToucanToco/toucan-data-sdk/blob/c3ca874e1b64f4bdcc2edda750a72d45d1561d8a/toucan_data_sdk/sdk.py#L173-L191,"def extract_zip(zip_file_path): """""" Returns: dict: Dict[str, DataFrame] """""" dfs = {} with zipfile.ZipFile(zip_file_path, mode='r') as z_file: names = z_file.namelist() for name in names: content = z_file.read(name) _, tmp_file_path = tempfile.mkstemp() try: with open(tmp_file_path, 'wb') as tmp_file: tmp_file.write(content) dfs[name] = joblib.load(tmp_file_path) finally: shutil.rmtree(tmp_file_path, ignore_errors=True) return dfs",2
710,Python,how to extract zip file recursively,https://github.com/demosdemon/format-pipfile/blob/f95162c49d8fc13153080ddb11ac5a5dcd4d2e7c/ci/appveyor-download.py#L95-L102,"def unpack_zipfile(filename): """"""Unpack a zipfile, using the names in the zip."""""" with open(filename, ""rb"") as fzip: z = zipfile.ZipFile(fzip) for name in z.namelist(): print(("" extracting {}"".format(name))) ensure_dirs(name) z.extract(name)",2
1797,Python,how to extract zip file recursively,https://github.com/jedie/DragonPy/blob/6659e5b5133aab26979a498ee7453495773a4f6c/dragonpy/Multicomp6809/Multicomp6809_rom.py#L39-L54,"def extract_zip(self): assert self.FILE_COUNT>0 try: with zipfile.ZipFile(self.archive_path, ""r"") as zip: content = zip.read(""ROMS/6809/EXT_BASIC_NO_USING.hex"") out_filename=os.path.join(self.ROM_PATH, ""EXT_BASIC_NO_USING.hex"") with open(out_filename, ""wb"") as f: f.write(content) print(""%r extracted"" % out_filename) self.post_processing(out_filename) except BadZipFile as err: msg = ""Error extracting archive %r: %s"" % (self.archive_path, err) log.error(msg) raise BadZipFile(msg)",1
139,Python,how to extract zip file recursively,https://github.com/pantsbuild/pex/blob/87b2129d860250d3b9edce75b9cb62f9789ee521/pex/common.py#L90-L95,"def _chmod(self, info, path): # This magic works to extract perm bits from the 32 bit external file attributes field for # unix-created zip files, for the layout, see: # https://www.forensicswiki.org/wiki/ZIP#External_file_attributes attr = info.external_attr >> 16 os.chmod(path, attr)",0
886,Python,how to empty array,https://github.com/deshima-dev/decode/blob/e789e174cd316e7ec8bc55be7009ad35baced3c0/decode/core/array/functions.py#L109-L121,"def empty(shape, dtype=None, **kwargs): """"""Create an array of given shape and type, without initializing entries. Args: shape (sequence of ints): 2D shape of the array. dtype (data-type, optional): Desired data-type for the array. kwargs (optional): Other arguments of the array (*coords, attrs, and name). Returns: array (decode.array): Decode array without initializing entries. """""" data = np.empty(shape, dtype) return dc.array(data, **kwargs)",3
1052,Python,how to empty array,https://github.com/openfisca/openfisca-core/blob/92ce9396e29ae5d9bac5ea604cfce88517c6b35c/openfisca_core/variables.py#L427-L433,"def default_array(self, array_size): array = np.empty(array_size, dtype = self.dtype) if self.value_type == Enum: array.fill(self.default_value.index) return EnumArray(array, self.possible_values) array.fill(self.default_value) return array",3
1833,Python,how to empty array,https://github.com/scikit-hep/uproot/blob/fc406827e36ed87cfb1062806e118f53fd3a3b0a/uproot/interp/numerical.py#L340-L341,"def empty(self): return self.awkward.numpy.empty((0, self.numbytes), dtype=self.todtype)",3
58,Python,how to empty array,https://github.com/jepegit/cellpy/blob/9f4a84cdd11f72cfa02cda8c2d7b5174abbb7370/cellpy/readers/core.py#L261-L266,def no_data(self): try: empty = self.dfdata.empty except AttributeError: empty = True return empty,2
2008,Python,how to empty array,https://github.com/mattjj/pybasicbayes/blob/76aef00f011415cc5c858cd1a101f3aab971a62d/pybasicbayes/distributions/gaussian.py#L1017-L1019,"def _empty_stats(self): return np.array([0.,np.zeros_like(self.mu_0),np.zeros_like(self.mu_0)], dtype=np.object)",2
739,Python,how to empty array,https://github.com/rainwoodman/sharedmem/blob/b23e59c1ed0e28f7b6c96c17a04d55c700e06e3a/sharedmem/sharedmem.py#L785-L791,"def empty_like(array, dtype=None): """""" Create a shared memory array from the shape of array. """""" array = numpy.asarray(array) if dtype is None: dtype = array.dtype return anonymousmemmap(array.shape, dtype)",1
1922,Python,how to empty array,https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/generic.py#L1878-L1924,"def empty(self): """""" Indicator whether DataFrame is empty. True if DataFrame is entirely empty (no items), meaning any of the axes are of length 0. Returns ------- bool If DataFrame is empty, return True, if not return False. See Also -------- Series.dropna DataFrame.dropna Notes ----- If DataFrame contains only NaNs, it is still not considered empty. See the example below. Examples -------- An example of an actual empty DataFrame. Notice the index is empty: >>> df_empty = pd.DataFrame({'A' : []}) >>> df_empty Empty DataFrame Columns: [A] Index: [] >>> df_empty.empty True If we only have NaNs in our DataFrame, it is not considered empty! We will need to drop the NaNs to make the DataFrame empty: >>> df = pd.DataFrame({'A' : [np.nan]}) >>> df A 0 NaN >>> df.empty False >>> df.dropna().empty True """""" return any(len(self._get_axis(a)) == 0 for a in self._AXIS_ORDERS)",1
10,Python,how to empty array,https://github.com/scikit-hep/awkward-array/blob/1f878c4e11a4548fd977e230ce93eb5534db73a2/awkward/array/virtual.py#L93-L97,"def empty_like(self, **overrides): if isinstance(self.array, self.numpy.ndarray): return self.numpy.empty_like(array) else: return self.array.empty_like(**overrides)",0
1215,Python,how to empty array,https://github.com/openclimatedata/pymagicc/blob/d896014832cf458d1e95e5878fd6d5961f3e2e05/pymagicc/io.py#L1652-L1696,"def _write_header(self, output): header_lines = [] header_lines.append(""{}"".format(len(self.data_block))) variables = self._get_df_header_row(""variable"") variables = convert_magicc7_to_openscm_variables(variables, inverse=True) variables = [v.replace(""_EMIS"", """") for v in variables] regions = self._get_df_header_row(""region"") regions = convert_magicc_to_openscm_regions(regions, inverse=True) special_scen_code = get_special_scen_code(regions=regions, emissions=variables) header_lines.append(""{}"".format(special_scen_code)) # for a scen file, the convention is (although all these lines are # actually ignored by source so could be anything): # - line 3 is name # - line 4 is description # - line 5 is notes (other notes lines go at the end) # - line 6 is empty header_lines.append(""NAME - need better solution for how to control this"") header_lines.append( ""DESCRIPTION - need better solution for how to control this"" ) header_lines.append(""NOTES - need better solution for how to control this"") header_lines.append("""") header_lines.append( ""OTHER NOTES - need better solution for how to control this"" ) header_lines.append( ""OTHER NOTES - need better solution for how to control this"" ) header_lines.append( ""OTHER NOTES - need better solution for how to control this"" ) header_lines.append( ""OTHER NOTES - need better solution for how to control this"" ) output.write(self._newline_char.join(header_lines)) output.write(self._newline_char) return output",0
1511,Python,how to determine a string is a valid word,https://github.com/phoopy/phoopy-console/blob/d38ec0eb952e79239699a0f855c07437a34024b0/phoopy/console/helper/string_helper.py#L41-L43,"def get_most_used_words(words, stopwords): valid_words = [word.lower() for word in words if StringHelper.is_valid_word(word, stopwords)] return dict(Counter(valid_words).most_common(5))",3
1944,Python,how to determine a string is a valid word,https://github.com/Ezhil-Language-Foundation/open-tamil/blob/b7556e88878d29bbc6c944ee17cdd3f75b8ea9f0/solthiruthi/datastore.py#L340-L358,"def isWord(self,word,ret_ref_trie=False): # see if @word is present in the current Trie; return True or False letters = utf8.get_letters(word) wLen = len(letters) ref_trie = self.trie ref_word_limits = self.word_limits for itr,letter in enumerate(letters): idx = self.getidx( letter ) #print(idx, letter) if itr == (wLen-1): break if not ref_trie[idx][1]: return False #this branch of Trie did not exist ref_trie = ref_trie[idx][1] ref_word_limits = ref_word_limits[idx][1] if ret_ref_trie: return ref_word_limits[idx][0],ref_trie,ref_word_limits return ref_word_limits[idx][0]",3
655,Python,how to determine a string is a valid word,https://github.com/thunlp/THULAC-Python/blob/3f1f126cd92c3d2aebdf4ab4850de3c9428a3b66/thulac/manage/TimeWord.py#L110-L117,"def isHttpWord(self, word): if(len(word) < 5): return False else: if(word[0] == ord('h') and word[1] == ord('t') and word[2] == ord('t') and word[3] == ord('p')): return True else: return False",2
1196,Python,how to determine a string is a valid word,https://github.com/SpotlightData/preprocessing/blob/180c6472bc2642afbd7a1ece08d0b0d14968a708/preprocessing/spellcheck.py#L91-L101,"def find_word_prob(word_string, word_total=sum(WORD_DISTRIBUTION.values())): ''' Finds the relative probability of the word appearing given context of a base corpus. Returns this probability value as a float instance. ''' if word_string is None: return 0 elif isinstance(word_string, str): return WORD_DISTRIBUTION[word_string] / word_total else: raise InputError(""string or none type variable not passed as argument to find_word_prob"")",2
1884,Python,how to determine a string is a valid word,https://github.com/SpotlightData/preprocessing/blob/180c6472bc2642afbd7a1ece08d0b0d14968a708/preprocessing/spellcheck.py#L18-L28,"def correct_word(word_string): ''' Finds all valid one and two letter corrections for word_string, returning the word with the highest relative probability as type str. ''' if word_string is None: return """" elif isinstance(word_string, str): return max(find_candidates(word_string), key=find_word_prob) else: raise InputError(""string or none type variable not passed as argument to correct_word"")",2
425,Python,how to determine a string is a valid word,https://github.com/O365/python-o365/blob/02a71cf3775cc6a3c042e003365d6a07c8c75a73/O365/utils/utils.py#L879-L907,"def _parse_filter_word(self, word): """""" Converts the word parameter into the correct format """""" if isinstance(word, str): word = ""'{}'"".format(word) elif isinstance(word, dt.date): if isinstance(word, dt.datetime): if word.tzinfo is None: # if it's a naive datetime, localize the datetime. word = self.protocol.timezone.localize( word) # localize datetime into local tz if word.tzinfo != pytz.utc: word = word.astimezone( pytz.utc) # transform local datetime to utc if '/' in self._attribute: # TODO: this is a fix for the case when the parameter # filtered is a string instead a dateTimeOffset # but checking the '/' is not correct, but it will # differentiate for now the case on events: # start/dateTime (date is a string here) from # the case on other dates such as # receivedDateTime (date is a dateTimeOffset) word = ""'{}'"".format( word.isoformat()) # convert datetime to isoformat. else: word = ""{}"".format( word.isoformat()) # convert datetime to isoformat elif isinstance(word, bool): word = str(word).lower() return word",1
746,Python,how to determine a string is a valid word,https://github.com/nickmckay/LiPD-utilities/blob/5dab6bbeffc5effd68e3a6beaca6b76aa928e860/Python/lipd/noaa_lpd.py#L559-L575,"def __camel_case(word): """""" Convert underscore naming into camel case naming :param str word: :return str: """""" word = word.lower() if '_' in word: split_word = word.split('_') else: split_word = word.split() if len(split_word) > 0: for i, word in enumerate(split_word): if i > 0: split_word[i] = word.title() strings = ''.join(split_word) return strings",1
1228,Python,how to determine a string is a valid word,https://github.com/benedictpaten/sonLib/blob/1decb75bb439b70721ec776f685ce98e25217d26/bioio.py#L636-L641,"def padWord(word, length=25): if len(word) > length: return word[:length] if len(word) < length: return word + "" ""*(length-len(word)) return word",1
1613,Python,how to determine a string is a valid word,https://github.com/a-tal/nagaram/blob/2edcb0ef8cb569ebd1c398be826472b4831d6110/nagaram/scrabble.py#L136-L184,"def valid_scrabble_word(word): """"""Checks if the input word could be played with a full bag of tiles. Returns: True or false """""" letters_in_bag = { ""a"": 9, ""b"": 2, ""c"": 2, ""d"": 4, ""e"": 12, ""f"": 2, ""g"": 3, ""h"": 2, ""i"": 9, ""j"": 1, ""k"": 1, ""l"": 4, ""m"": 2, ""n"": 6, ""o"": 8, ""p"": 2, ""q"": 1, ""r"": 6, ""s"": 4, ""t"": 6, ""u"": 4, ""v"": 2, ""w"": 2, ""x"": 1, ""y"": 2, ""z"": 1, ""_"": 2, } for letter in word: if letter == ""?"": continue try: letters_in_bag[letter] -= 1 except KeyError: return False if letters_in_bag[letter] < 0: letters_in_bag[""_""] -= 1 if letters_in_bag[""_""] < 0: return False return True",1
265,Python,how to determine a string is a valid word,https://github.com/deepmipt/DeepPavlov/blob/f3e4a69a3764d25d2f5bad4f1f1aebc872b00f9c/deeppavlov/models/preprocessors/capitalization.py#L75-L108,"def process_word(word: str, to_lower: bool = False, append_case: Optional[str] = None) -> Tuple[str]: """"""Converts word to a tuple of symbols, optionally converts it to lowercase and adds capitalization label. Args: word: input word to_lower: whether to lowercase append_case: whether to add case mark ('<FIRST_UPPER>' for first capital and '<ALL_UPPER>' for all caps) Returns: a preprocessed word """""" if all(x.isupper() for x in word) and len(word) > 1: uppercase = ""<ALL_UPPER>"" elif word[0].isupper(): uppercase = ""<FIRST_UPPER>"" else: uppercase = None if to_lower: word = word.lower() if word.isdigit(): answer = [""<DIGIT>""] elif word.startswith(""http://"") or word.startswith(""www.""): answer = [""<HTTP>""] else: answer = list(word) if to_lower and uppercase is not None: if append_case == ""first"": answer = [uppercase] + answer elif append_case == ""last"": answer = answer + [uppercase] return tuple(answer)",0
72,Python,how to check if a checkbox is checked,https://github.com/Shapeways/coyote_framework/blob/cb29899b984a21d56bf65d0b1d907073948fe16c/coyote_framework/webdriver/webdriverwrapper/WebElementWrapper.py#L655-L660,"def checkbox_check(self, force_check=False): """""" Wrapper to check a checkbox """""" if not self.get_attribute('checked'): self.click(force_click=force_check)",3
114,Python,how to check if a checkbox is checked,https://github.com/aloetesting/aloe_webdriver/blob/65d847da4bdc63f9c015cb19d4efdee87df8ffad/aloe_webdriver/__init__.py#L564-L568,"def assert_checked_checkbox(self, value): """"""Assert the checkbox with label (recommended), name or id is checked."""""" check_box = find_field(world.browser, 'checkbox', value) assert check_box, ""Cannot find checkbox '{}'."".format(value) assert check_box.is_selected(), ""Check box should be selected.""",3
997,Python,how to check if a checkbox is checked,https://github.com/bbangert/lettuce_webdriver/blob/d11f8531c43bb7150c316e0dc4ccd083617becf7/lettuce_webdriver/webdriver.py#L359-L361,"def assert_not_checked_checkbox(step, value): check_box = find_field(world.browser, 'checkbox', value) assert_true(step, not check_box.is_selected())",3
1405,Python,how to check if a checkbox is checked,https://github.com/jbm950/pygame_toolbox/blob/3fe32145fc149e4dd0963c30a2b6a4dddd4fac0e/pygame_toolbox/graphics/widgets.py#L141-L162,"def __call__(self, *arg): if self.status: # If the checkbox was previously checked, set the status # to 0 and turn the whole box back to white. self.status = 0 self.image.fill((255, 255, 255)) # Draw the checkbox on the external surface ptg.Button.set_position(self, self.position, self.midpoint, self.surface) else: # If the checkbox was previously unchecked, set the status # to 1 and draw a check in the center of the checkbox. self.status = 1 if self.checktype == 'r': self.draw_rect_check() elif self.checktype == 'c': self.draw_circle_check() # Draw the new image on the external surface ptg.Button.set_position(self, self.position, self.midpoint, self.surface)",3
1456,Python,how to check if a checkbox is checked,https://github.com/aloetesting/aloe_webdriver/blob/65d847da4bdc63f9c015cb19d4efdee87df8ffad/aloe_webdriver/__init__.py#L542-L547,"def check_checkbox(self, value): """"""Check the checkbox with label (recommended), name or id."""""" check_box = find_field(world.browser, 'checkbox', value) assert check_box, ""Cannot find checkbox '{}'."".format(value) if not check_box.is_selected(): check_box.click()",3
136,Python,how to check if a checkbox is checked,https://github.com/bbangert/lettuce_webdriver/blob/d11f8531c43bb7150c316e0dc4ccd083617becf7/lettuce_webdriver/webdriver.py#L337-L341,"def check_checkbox(step, value): with AssertContextManager(step): check_box = find_field(world.browser, 'checkbox', value) if not check_box.is_selected(): check_box.click()",2
202,Python,how to check if a checkbox is checked,https://github.com/DLR-RM/RAFCON/blob/24942ef1a904531f49ab8830a1dbb604441be498/source/rafcon/gui/utils/dialog.py#L233-L234,"def get_checkbox_state_by_name(self, checkbox_text): return [checkbox.get_active() for checkbox in self.checkboxes if checkbox.get_label() == checkbox_text]",2
1003,Python,how to check if a checkbox is checked,https://github.com/MechanicalSoup/MechanicalSoup/blob/027a270febf5bcda6a75db60ea9838d631370f4b/mechanicalsoup/form.py#L99-L146,"def set_checkbox(self, data, uncheck_other_boxes=True): """"""Set the *checked*-attribute of input elements of type ""checkbox"" specified by ``data`` (i.e. check boxes). :param data: Dict of ``{name: value, ...}``. In the family of checkboxes whose *name*-attribute is ``name``, check the box whose *value*-attribute is ``value``. All boxes in the family can be checked (unchecked) if ``value`` is True (False). To check multiple specific boxes, let ``value`` be a tuple or list. :param uncheck_other_boxes: If True (default), before checking any boxes specified by ``data``, uncheck the entire checkbox family. Consider setting to False if some boxes are checked by default when the HTML is served. """""" for (name, value) in data.items(): # Case-insensitive search for type=checkbox checkboxes = self.find_by_type(""input"", ""checkbox"", {'name': name}) if not checkboxes: raise InvalidFormMethod(""No input checkbox named "" + name) # uncheck if requested if uncheck_other_boxes: self.uncheck_all(name) # Wrap individual values (e.g. int, str) in a 1-element tuple. if not isinstance(value, list) and not isinstance(value, tuple): value = (value,) # Check or uncheck one or more boxes for choice in value: choice_str = str(choice) # Allow for example literal numbers for checkbox in checkboxes: if checkbox.attrs.get(""value"", ""on"") == choice_str: checkbox[""checked""] = """" break # Allow specifying True or False to check/uncheck elif choice is True: checkbox[""checked""] = """" break elif choice is False: if ""checked"" in checkbox.attrs: del checkbox.attrs[""checked""] break else: raise LinkNotFoundError( ""No input checkbox named %s with choice %s"" % (name, choice) )",2
252,Python,how to check if a checkbox is checked,https://github.com/PySimpleGUI/PySimpleGUI/blob/08184197f5bd4580ab5e5aca28bdda30f87b86fc/PySimpleGUI27.py#L486-L493,def CheckboxHandler(self): if self.Key is not None: self.ParentForm.LastButtonClicked = self.Key else: self.ParentForm.LastButtonClicked = '' self.ParentForm.FormRemainedOpen = True if self.ParentForm.CurrentlyRunningMainloop: self.ParentForm.TKroot.quit(),1
1705,Python,how to check if a checkbox is checked,https://github.com/InQuest/python-sandboxapi/blob/9bad73f453e25d7d23e7b4b1ae927f44a35a5bc3/sandboxapi/joe.py#L37-L51,"def check(self, item_id): """"""Check if an analysis is complete. :type item_id: str :param item_id: File ID to check. :rtype: bool :return: Boolean indicating if a report is done or not. """""" try: return self.jbx.info(item_id).get('status').lower() == 'finished' except jbxapi.JoeException: return False return False",1
34,Python,heatmap from 3d coordinates,https://github.com/jwkvam/plotlywrapper/blob/762b42912e824fecb1212c186900f2ebdd0ab12b/plotlywrapper.py#L832-L850,"def heatmap(z, x=None, y=None, colorscale='Viridis'): """"""Create a heatmap. Parameters ---------- z : TODO x : TODO, optional y : TODO, optional colorscale : TODO, optional Returns ------- Chart """""" z = np.atleast_1d(z) data = [go.Heatmap(z=z, x=x, y=y, colorscale=colorscale)] return Chart(data=data)",3
177,Python,heatmap from 3d coordinates,https://github.com/tensorpack/tensorpack/blob/d7a13cb74c9066bc791d7aafc3b744b60ee79a9f/examples/CaffeModels/load-cpm.py#L27-L32,"def colorize(img, heatmap): """""" img: bgr, [0,255] heatmap: [0,1] """""" heatmap = viz.intensity_to_rgb(heatmap, cmap='jet')[:, :, ::-1] return img * 0.5 + heatmap * 0.5",3
381,Python,heatmap from 3d coordinates,https://github.com/flo-compbio/genometools/blob/dd962bb26d60a0f14ca14d8c9a4dd75768962c7d/genometools/expression/matrix.py#L295-L319,"def get_figure(self, heatmap_kw=None, **kwargs): """"""Generate a plotly figure showing the matrix as a heatmap. This is a shortcut for ``ExpMatrix.get_heatmap(...).get_figure(...)``. See :func:`ExpHeatmap.get_figure` for keyword arguments. Parameters ---------- heatmap_kw : dict or None If not None, dictionary containing keyword arguments to be passed to the `ExpHeatmap` constructor. Returns ------- `plotly.graph_objs.Figure` The plotly figure. """""" if heatmap_kw is not None: assert isinstance(heatmap_kw, dict) if heatmap_kw is None: heatmap_kw = {} return self.get_heatmap(**heatmap_kw).get_figure(**kwargs)",3
450,Python,heatmap from 3d coordinates,https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/heatmaps.py#L110-L158,"def draw(self, size=None, cmap=""jet""): """""" Render the heatmaps as RGB images. Parameters ---------- size : None or float or iterable of int or iterable of float, optional Size of the rendered RGB image as ``(height, width)``. See :func:`imgaug.imgaug.imresize_single_image` for details. If set to None, no resizing is performed and the size of the heatmaps array is used. cmap : str or None, optional Color map of ``matplotlib`` to use in order to convert the heatmaps to RGB images. If set to None, no color map will be used and the heatmaps will be converted to simple intensity maps. Returns ------- heatmaps_drawn : list of (H,W,3) ndarray Rendered heatmaps. One per heatmap array channel. Dtype is uint8. """""" heatmaps_uint8 = self.to_uint8() heatmaps_drawn = [] for c in sm.xrange(heatmaps_uint8.shape[2]): # c:c+1 here, because the additional axis is needed by imresize_single_image heatmap_c = heatmaps_uint8[..., c:c+1] if size is not None: heatmap_c_rs = ia.imresize_single_image(heatmap_c, size, interpolation=""nearest"") else: heatmap_c_rs = heatmap_c heatmap_c_rs = np.squeeze(heatmap_c_rs).astype(np.float32) / 255.0 if cmap is not None: # import only when necessary (faster startup; optional dependency; less fragile -- see issue #225) import matplotlib.pyplot as plt cmap_func = plt.get_cmap(cmap) heatmap_cmapped = cmap_func(heatmap_c_rs) heatmap_cmapped = np.delete(heatmap_cmapped, 3, 2) else: heatmap_cmapped = np.tile(heatmap_c_rs[..., np.newaxis], (1, 1, 3)) heatmap_cmapped = np.clip(heatmap_cmapped * 255, 0, 255).astype(np.uint8) heatmaps_drawn.append(heatmap_cmapped) return heatmaps_drawn",3
1124,Python,heatmap from 3d coordinates,https://github.com/kgori/treeCl/blob/fed624b3db1c19cc07175ca04e3eda6905a8d305/treeCl/plotter.py#L249-L274,"def heatmap(self, partition=None, cmap=CM.Blues): """""" Plots a visual representation of a distance matrix """""" if isinstance(self.dm, DistanceMatrix): length = self.dm.values.shape[0] else: length = self.dm.shape[0] datamax = float(np.abs(self.dm).max()) fig = plt.figure() ax = fig.add_subplot(111) ticks_at = [0, 0.5 * datamax, datamax] if partition: sorting = flatten_list(partition.get_membership()) self.dm = self.dm.reorder(sorting) cax = ax.imshow( self.dm.values, interpolation='nearest', origin='lower', extent=[0., length, 0., length], vmin=0, vmax=datamax, cmap=cmap, ) cbar = fig.colorbar(cax, ticks=ticks_at, format='%1.2g') cbar.set_label('Distance') return fig",3
1301,Python,heatmap from 3d coordinates,https://github.com/jwkvam/plotlywrapper/blob/762b42912e824fecb1212c186900f2ebdd0ab12b/doc/figures.py#L43-L45,"def heatmap2(): x = np.arange(5) pw.heatmap(z=np.arange(25), x=np.tile(x, 5), y=x.repeat(5)).save('fig_heatmap2.html', **options)",2
1370,Python,heatmap from 3d coordinates,https://github.com/widdowquinn/pyani/blob/2b24ec971401e04024bba896e4011984fe3f53f0/pyani/pyani_graphics.py#L219-L228,"def get_mpl_heatmap_axes(dfr, fig, heatmap_gs): """"""Return axis for Matplotlib heatmap."""""" # Create heatmap axis heatmap_axes = fig.add_subplot(heatmap_gs[1, 1]) heatmap_axes.set_xticks(np.linspace(0, dfr.shape[0] - 1, dfr.shape[0])) heatmap_axes.set_yticks(np.linspace(0, dfr.shape[0] - 1, dfr.shape[0])) heatmap_axes.grid(False) heatmap_axes.xaxis.tick_bottom() heatmap_axes.yaxis.tick_right() return heatmap_axes",2
1798,Python,heatmap from 3d coordinates,https://github.com/kevinsprong23/aperture/blob/d0420fef3b25d8afc0e5ddcfb6fe5f0ff42b9799/aperture/heatmaps.py#L123-L144,"def heatmap(x, y, step=None, min_pt=None, max_pt=None, colormap='Blues', alpha=1, grid=False, colorbar=True, scale='lin', vmax='auto', vmin='auto', crop=True): """""" function to take vectors x and y and hist them """""" (x_vec, y_vec, hist_matrix) = calc_2d_hist(x, y, step, min_pt, max_pt) # simple in this case because it is positive counts if scale == 'log': for row in hist_matrix: for i, el in enumerate(row): row[i] = 0 if row[i] == 0 else log10(row[i]) # plot fig = plt.figure() init_heatmap(x_vec, y_vec, hist_matrix, fig, colormap=colormap, alpha=alpha, grid=grid, colorbar=colorbar, vmax=vmax, vmin=vmin, crop=crop) return fig",2
86,Python,heatmap from 3d coordinates,https://github.com/kgori/treeCl/blob/fed624b3db1c19cc07175ca04e3eda6905a8d305/treeCl/plotter.py#L31-L82,"def heatmap(dm, partition=None, cmap=CM.Blues, fontsize=10): """""" heatmap(dm, partition=None, cmap=CM.Blues, fontsize=10) Produce a 2D plot of the distance matrix, with values encoded by coloured cells. Args: partition: treeCl.Partition object - if supplied, will reorder rows and columns of the distance matrix to reflect the groups defined by the partition cmap: matplotlib colourmap object - the colour palette to use fontsize: int or None - sets the size of the locus lab Returns: matplotlib plottable object """""" assert isinstance(dm, DistanceMatrix) datamax = float(np.abs(dm.values).max()) length = dm.shape[0] if partition: sorting = np.array(flatten_list(partition.get_membership())) new_dm = dm.reorder(dm.df.columns[sorting]) else: new_dm = dm fig = plt.figure() ax = fig.add_subplot(111) ax.xaxis.tick_top() ax.grid(False) tick_positions = np.array(list(range(length))) + 0.5 if fontsize is not None: ax.set_yticks(tick_positions) ax.set_xticks(tick_positions) ax.set_xticklabels(new_dm.df.columns, rotation=90, fontsize=fontsize, ha='center') ax.set_yticklabels(new_dm.df.index, fontsize=fontsize, va='center') cbar_ticks_at = [0, 0.5 * datamax, datamax] cax = ax.imshow( new_dm.values, interpolation='nearest', extent=[0., length, length, 0.], vmin=0, vmax=datamax, cmap=cmap, ) cbar = fig.colorbar(cax, ticks=cbar_ticks_at, format='%1.2g') cbar.set_label('Distance') return fig",1
497,Python,heatmap from 3d coordinates,https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/graphics/camera.py#L181-L194,"def _get_projection_matrix(self): # Convert from homogeneous 3d coordinates to # 2D coordinates fov = self.fov*np.pi/180.0 top = np.tan(fov * 0.5)*self.z_near bottom = -top left = self.aspectratio * bottom right = self.aspectratio * top return clip_matrix(left, right, bottom, top, self.z_near, self.z_far, perspective=True)",1
117,Python,hash set for counting distinct elements,https://github.com/pricingassistant/mongokat/blob/61eaf4bc1c4cc359c6f9592ec97b9a04d9561411/mongokat/collection.py#L136-L137,"def distinct(self, *args, **kwargs): return self._collection_with_options(kwargs).distinct(*args, **kwargs)",2
199,Python,hash set for counting distinct elements,https://github.com/merenlab/illumina-utils/blob/246d0611f976471783b83d2aba309b0cb57210f6/IlluminaUtils/lib/fastalib.py#L111-L128,"def init_unique_hash(self): while self.next_regular(): hash = hashlib.sha1(self.seq.upper().encode('utf-8')).hexdigest() if hash in self.unique_hash_dict: self.unique_hash_dict[hash]['ids'].append(self.id) self.unique_hash_dict[hash]['count'] += 1 else: self.unique_hash_dict[hash] = {'id': self.id, 'ids': [self.id], 'seq': self.seq, 'count': 1} self.unique_hash_list = [i[1] for i in sorted([(self.unique_hash_dict[hash]['count'], hash)\ for hash in self.unique_hash_dict], reverse=True)] self.total_unique = len(self.unique_hash_dict) self.reset()",2
1161,Python,hash set for counting distinct elements,https://github.com/cggh/scikit-allel/blob/3c979a57a100240ba959dd13f98839349530f215/allel/model/ndarray.py#L2510-L2520,"def distinct_counts(self): """"""Return counts for each distinct haplotype."""""" # hash the haplotypes k = [hash(self.values[:, i].tobytes()) for i in range(self.shape[1])] # count and sort # noinspection PyArgumentList counts = sorted(collections.Counter(k).values(), reverse=True) return np.asarray(counts)",2
1913,Python,hash set for counting distinct elements,https://github.com/mlenzen/collections-extended/blob/ee9e86f6bbef442dbebcb3a5970642c5c969e2cf/collections_extended/bags.py#L89-L96,"def _set_count(self, elem, count): if count < 0: raise ValueError self._size += count - self.count(elem) if count == 0: self._dict.pop(elem, None) else: self._dict[elem] = count",2
482,Python,hash set for counting distinct elements,https://github.com/openstates/billy/blob/5fc795347f12a949e410a8cfad0c911ea6bced67/billy/models/base.py#L414-L416,"def distinct(self, *args, **kwargs): return CursorWrapper( self.cursor.distinct(*args, **kwargs), self.instance)",1
559,Python,hash set for counting distinct elements,https://github.com/firstprayer/monsql/blob/6285c15b574c8664046eae2edfeb548c7b173efd/monsql/queryset.py#L110-L117,"def distinct(self): """""" Only return distinct row. Return a new query set with distinct mark """""" new_query_set = self.clone() new_query_set.query.distinct = True return new_query_set",1
1165,Python,hash set for counting distinct elements,https://github.com/cggh/scikit-allel/blob/3c979a57a100240ba959dd13f98839349530f215/allel/model/ndarray.py#L2492-L2508,"def distinct(self): """"""Return sets of indices for each distinct haplotype."""""" # setup collection d = collections.defaultdict(set) # iterate over haplotypes for i in range(self.shape[1]): # hash the haplotype k = hash(self.values[:, i].tobytes()) # collect d[k].add(i) # extract sets, sorted by most common return sorted(d.values(), key=len, reverse=True)",1
105,Python,hash set for counting distinct elements,https://github.com/openvax/sercol/blob/e66a8e8c3c0b21e53eb8f73be4d23409fab311ae/sercol/collection.py#L74-L96,"def clone_with_new_elements( self, new_elements, drop_keywords=set([]), rename_dict={}, extra_kwargs={}): """""" Create another Collection of the same class and with same state but possibly different entries. Extra parameters to control which keyword arguments get passed to the initializer are necessary since derived classes have different constructors than the base class. """""" kwargs = dict( elements=new_elements, distinct=self.distinct, sort_key=self.sort_key, sources=self.sources) for name in drop_keywords: kwargs.pop(name) for old_name, new_name in rename_dict.items(): kwargs[new_name] = kwargs.pop(old_name) kwargs.update(extra_kwargs) return self.__class__(**kwargs)",0
685,Python,hash set for counting distinct elements,https://github.com/emc-openstack/storops/blob/24b4b13bf065c0ef0538dd0b5ebb8f25d24176bd/storops/vnx/resource/disk.py#L141-L166,"def same_disks(self, count=2): """""" filter self to the required number of disks with same size and type Select the disks with the same type and same size. If not enough disks available, set self to empty. :param count: number of disks to retrieve :return: disk list """""" ret = self if len(self) > 0: type_counter = Counter(self.drive_type) drive_type, counts = type_counter.most_common()[0] self.set_drive_type(drive_type) if len(self) > 0: size_counter = Counter(self.capacity) size, counts = size_counter.most_common()[0] self.set_capacity(size) if len(self) >= count: indices = self.index[:count] self.set_indices(indices) else: self.set_indices('N/A') return ret",0
944,Python,hash set for counting distinct elements,https://github.com/ibis-project/ibis/blob/1e39a5fd9ef088b45c155e8a5f541767ee8ef2e7/ibis/expr/api.py#L3265-L3282,"def _table_union(left, right, distinct=False): """""" Form the table set union of two table expressions having identical schemas. Parameters ---------- right : TableExpr distinct : boolean, default False Only union distinct rows not occurring in the calling table (this can be very expensive, be careful) Returns ------- union : TableExpr """""" op = ops.Union(left, right, distinct=distinct) return op.to_expr()",0
573,Python,group by count,https://github.com/shazow/unstdlib.py/blob/e0632fe165cfbfdb5a7e4bc7b412c9d6f2ebad83/unstdlib/standard/list_.py#L16-L36,"def groupby_count(i, key=None, force_keys=None): """""" Aggregate iterator values into buckets based on how frequently the values appear. Example:: >>> list(groupby_count([1, 1, 1, 2, 3])) [(1, 3), (2, 1), (3, 1)] """""" counter = defaultdict(lambda: 0) if not key: key = lambda o: o for k in i: counter[key(k)] += 1 if force_keys: for k in force_keys: counter[k] += 0 return counter.items()",3
641,Python,group by count,https://github.com/adamjaso/pyauto/blob/b11da69fb21a49241f5ad75dac48d9d369c6279b/ouidb/pyauto/ouidb/config.py#L43-L50,"def get_top_entries(self): query = 'select count(vendor_name) count, vendor_name from mac_vendor group by vendor_name having count > 10 order by count;' try: db = self.get_db() results = db.execute(query) return [i for i in results] finally: db.close()",3
732,Python,group by count,https://github.com/letuananh/chirptext/blob/ce60b47257b272a587c8703ea1f86cd1a45553a7/chirptext/leutile.py#L178-L184,"def group_by_count(self): d = OrderedDict() for item, count in self.most_common(): if count not in d: d[count] = [] d[count].append(item) return d.items()",3
1020,Python,group by count,https://github.com/rueckstiess/mtools/blob/a6a22910c3569c0c8a3908660ca218a4557e4249/mtools/mplotqueries/plottypes/base_type.py#L79-L91,"def group(self): """"""(re-)group all logevents by the given group."""""" if hasattr(self, 'group_by'): group_by = self.group_by else: group_by = self.default_group_by if self.args['group'] is not None: group_by = self.args['group'] self.groups = Grouping(self.logevents, group_by) self.groups.move_items(None, 'others') self.groups.sort_by_size(group_limit=self.args['group_limit'], discard_others=self.args['no_others'])",2
1327,Python,group by count,https://github.com/Koed00/django-q/blob/c84fd11a67c9a47d821786dfcdc189bb258c6f54/django_q/models.py#L53-L55,"def group_count(self, failures=False): if self.group: return self.get_group_count(self.group, failures)",2
1331,Python,group by count,https://github.com/hcpl/xkbgroup/blob/fcf4709a3c8221e0cdf62c09e5cccda232b0104c/xkbgroup/core.py#L242-L258,"def groups_count(self): """"""Number of all groups (get-only). :getter: Returns number of all groups :type: int """""" if self._keyboard_description.contents.ctrls is not None: return self._keyboard_description.contents.ctrls.contents.num_groups else: groups_source = self._groups_source groups_count = 0 while (groups_count < XkbNumKbdGroups and groups_source[groups_count] != None_): groups_count += 1 return groups_count",1
1448,Python,group by count,https://github.com/realitix/vulkan/blob/07285387092aaa61d2d71fa2913d60a73f022cbe/vulkan/_vulkan.py#L6130-L6135,"def vkCmdDispatch(commandBuffer ,groupCountX ,groupCountY ,groupCountZ ,): result = _callApi(lib.vkCmdDispatch, commandBuffer,groupCountX,groupCountY,groupCountZ)",1
873,Python,group by count,https://github.com/pandas-dev/pandas/blob/9feb3ad92cc0397a04b665803a49299ee7aa1037/pandas/core/groupby/generic.py#L1183-L1196,"def count(self): """""" Compute count of group, excluding missing values """""" ids, _, ngroups = self.grouper.group_info val = self.obj.get_values() mask = (ids != -1) & ~isna(val) ids = ensure_platform_int(ids) minlength = ngroups or 0 out = np.bincount(ids[mask], minlength=minlength) return Series(out, index=self.grouper.result_index, name=self._selection_name, dtype='int64')",0
1148,Python,group by count,https://github.com/Koed00/django-q/blob/c84fd11a67c9a47d821786dfcdc189bb258c6f54/django_q/models.py#L48-L51,"def get_group_count(group_id, failures=False): if failures: return Failure.objects.filter(group=group_id).count() return Task.objects.filter(group=group_id).count()",0
1240,Python,group by count,https://github.com/facetoe/zenpy/blob/34c54c7e408b9ed01604ddf8b3422204c8bf31ea/zenpy/lib/api_objects/__init__.py#L1283-L1286,"def group(self, group): if group: self.group_id = group.id self._group = group",0
53,Python,get the description of a http status code,https://github.com/hardbyte/python-can/blob/cdc5254d96072df7739263623f3e920628a7d214/can/interfaces/socketcan/utils.py#L70-L90,"def error_code_to_str(code): """""" Converts a given error code (errno) to a useful and human readable string. :param int code: a possibly invalid/unknown error code :rtype: str :returns: a string explaining and containing the given error code, or a string explaining that the errorcode is unknown if that is the case """""" try: name = errno.errorcode[code] except KeyError: name = ""UNKNOWN"" try: description = os.strerror(code) except ValueError: description = ""no description available"" return ""{} (errno {}): {}"".format(name, code, description)",3
687,Python,get the description of a http status code,https://github.com/vecnet/vecnet.simulation/blob/3a4b3df7b12418c6fa8a7d9cd49656a1c031fc0e/vecnet/simulation/sim_status.py#L47-L54,"def get_description(status_code): """""" Get the description for a status code. """""" description = _descriptions.get(status_code) if description is None: description = 'code = %s (no description)' % str(status_code) return description",3
1344,Python,get the description of a http status code,https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/http_code.py#L157-L204,"def get(self): """""" Return the HTTP code status. :return: The matched and formatted status code. :rtype: str|int|None """""" if PyFunceble.HTTP_CODE[""active""]: # The http status code extraction is activated. # We get the http status code. http_code = self._access() # We initiate a variable which will save the list of allowed # http status code. list_of_valid_http_code = [] for codes in [ PyFunceble.HTTP_CODE[""list""][""up""], PyFunceble.HTTP_CODE[""list""][""potentially_down""], PyFunceble.HTTP_CODE[""list""][""potentially_up""], ]: # We loop throught the list of http status code. # We extend the list of valid with the currently read # codes. list_of_valid_http_code.extend(codes) if http_code not in list_of_valid_http_code or http_code is None: # * The extracted http code is not in the list of valid http code. # or # * The extracted http code is equal to `None`. # We return 3 star in order to mention that we were not eable to extract # the http status code. return ""*"" * 3 # * The extracted http code is in the list of valid http code. # or # * The extracted http code is not equal to `None`. # We return the extracted http status code. return http_code # The http status code extraction is activated. # We return None. return None",3
1481,Python,get the description of a http status code,https://github.com/mar10/wsgidav/blob/cec0d84222fc24bea01be1cea91729001963f172/wsgidav/dav_error.py#L264-L274,"def get_http_status_string(v): """"""Return HTTP response string, e.g. 204 -> ('204 No Content'). The return string always includes descriptive text, to satisfy Apache mod_dav. `v`: status code or DAVError """""" code = get_http_status_code(v) try: return ERROR_DESCRIPTIONS[code] except KeyError: return ""{} Status"".format(code)",3
1781,Python,get the description of a http status code,https://github.com/funilrys/PyFunceble/blob/cdf69cbde120199171f7158e1c33635753e6e2f5/PyFunceble/url.py#L75-L125,"def get(cls): # pragma: no cover """""" Execute the logic behind the URL handling. :return: The status of the URL. :rtype: str """""" if Check().is_url_valid() or PyFunceble.CONFIGURATION[""local""]: # * The url is valid. # or # * We are testing in/for a local or private network. if ""current_test_data"" in PyFunceble.INTERN: PyFunceble.INTERN[""current_test_data""][""url_syntax_validation""] = True # We initiate the HTTP status code. PyFunceble.INTERN.update({""http_code"": HTTPCode().get()}) # We initiate the list of active status code. active_list = [] active_list.extend(PyFunceble.HTTP_CODE[""list""][""potentially_up""]) active_list.extend(PyFunceble.HTTP_CODE[""list""][""up""]) # We initiate the list of inactive status code. inactive_list = [] inactive_list.extend(PyFunceble.HTTP_CODE[""list""][""potentially_down""]) inactive_list.append(""*"" * 3) if PyFunceble.INTERN[""http_code""] in active_list: # The extracted HTTP status code is in the list of active list. # We handle and return the up status. return URLStatus(PyFunceble.STATUS[""official""][""up""]).handle() if PyFunceble.INTERN[""http_code""] in inactive_list: # The extracted HTTP status code is in the list of inactive list. # We handle and return the down status. return URLStatus(PyFunceble.STATUS[""official""][""down""]).handle() # The extracted HTTP status code is not in the list of active nor invalid list. if ""current_test_data"" in PyFunceble.INTERN: # The end-user want more information whith his test. # We update the url_syntax_validation index. PyFunceble.INTERN[""current_test_data""][""url_syntax_validation""] = False # We handle and return the invalid down status. return URLStatus(PyFunceble.STATUS[""official""][""invalid""]).handle()",3
35,Python,get the description of a http status code,https://github.com/DenisCarriere/geocoder/blob/39b9999ec70e61da9fa52fe9fe82a261ad70fa8b/geocoder/opencage.py#L409-L415,"def _catch_errors(self, json_response): status = json_response.get('status') if status and status.get('code') != 200: self.status_code = status.get('code') self.error = status.get('message') return self.error",2
715,Python,get the description of a http status code,https://github.com/TestInABox/stackInABox/blob/63ee457401e9a88d987f85f513eb512dcb12d984/stackinabox/util/requests_mock/core.py#L63-L75,"def get_reason_for_status(status_code): """"""Lookup the HTTP reason text for a given status code. :param status_code: int - HTTP status code :returns: string - HTTP reason text """""" if status_code in requests.status_codes.codes: return requests.status_codes._codes[status_code][0].replace('_', ' ') else: return 'Unknown status code - {0}'.format(status_code)",2
937,Python,get the description of a http status code,https://github.com/cloudendpoints/endpoints-python/blob/00dd7c7a52a9ee39d5923191c2604b8eafdb3f24/endpoints/errors.py#L239-L253,"def _get_status_code(self, http_status): """"""Get the HTTP status code from an HTTP status string. Args: http_status: A string containing a HTTP status code and reason. Returns: An integer with the status code number from http_status. """""" try: return int(http_status.split(' ', 1)[0]) except TypeError: _logger.warning('Unable to find status code in HTTP status %r.', http_status) return 500",2
1248,Python,get the description of a http status code,https://github.com/ronaldguillen/wave/blob/20bb979c917f7634d8257992e6d449dc751256a9/wave/renderers.py#L568-L571,"def get_description(self, view, status_code): if status_code in (status.HTTP_401_UNAUTHORIZED, status.HTTP_403_FORBIDDEN): return '' return view.get_view_description(html=True)",2
1486,Python,get the description of a http status code,https://github.com/ewdurbin/community/blob/f6b80e215a88508e3b07c2f4996ede6edea2c8e3/community/app.py#L36-L42,"def compute_status(): status_codes = [] for name, health_url in DEPENDENCIES.items(): status_codes.append(get_status(health_url)[0]) if max(status_codes) == 500: return ('UNHEALTHY', max(status_codes)) return ('HEALTHY', max(status_codes))",2
1652,Python,get name of enumerated value,https://github.com/jborean93/smbprotocol/blob/d8eb00fbc824f97d0f4946e3f768c5e6c723499a/smbprotocol/structure.py#L775-L785,"def _to_string(self): enum_name = None value = self._get_calculated_value(self.value) for enum, enum_value in vars(self.enum_type).items(): if value == enum_value: enum_name = enum break if enum_name is None: return ""(%d) UNKNOWN_ENUM"" % value else: return ""(%d) %s"" % (value, enum_name)",3
2071,Python,get name of enumerated value,https://github.com/apple/turicreate/blob/74514c3f99e25b46f22c6e02977fe3da69221c2e/src/external/coremltools_wrap/coremltools/deps/protobuf/python/google/protobuf/descriptor.py#L321-L337,"def EnumValueName(self, enum, value): """"""Returns the string name of an enum value. This is just a small helper method to simplify a common operation. Args: enum: string name of the Enum. value: int, value of the enum. Returns: string name of the enum value. Raises: KeyError if either the Enum doesn't exist or the value is not a valid value for the enum. """""" return self.enum_types_by_name[enum].values_by_number[value].name",3
541,Python,get name of enumerated value,https://github.com/mbedmicro/pyOCD/blob/41a174718a9739f3cbe785c2ba21cb7fd1310c6f/pyocd/debug/svd/parser.py#L82-L88,"def _parse_enumerated_value(self, enumerated_value_node): return SVDEnumeratedValue( name=_get_text(enumerated_value_node, 'name'), description=_get_text(enumerated_value_node, 'description'), value=_get_int(enumerated_value_node, 'value'), is_default=_get_int(enumerated_value_node, 'isDefault') )",2
1025,Python,get name of enumerated value,https://github.com/pkkid/python-plexapi/blob/9efbde96441c2bfbf410eacfb46e811e108e8bbc/plexapi/settings.py#L132-L139,"def _getEnumValues(self, data): """""" Returns a list of dictionary of valis value for this setting. """""" enumstr = data.attrib.get('enumValues') if not enumstr: return None if ':' in enumstr: return {self._cast(k): v for k, v in [kv.split(':') for kv in enumstr.split('|')]} return enumstr.split('|')",2
1332,Python,get name of enumerated value,https://github.com/ibelie/typy/blob/3616845fb91459aacd8df6bf82c5d91f4542bee7/typy/Proto.py#L266-L274,"def Enum(name, *fields): obj = TypeObject() obj.isEnum = True obj.__name__ = name obj.__enum__ = {} for a, p in fields: obj.__enum__[p] = TypeObject() obj.__enum__[p].name = a return obj",2
1570,Python,get name of enumerated value,https://github.com/crytic/slither/blob/04c147f7e50223c6af458ca430befae747ccd259/slither/core/declarations/contract.py#L431-L439,"def get_enum_from_name(self, enum_name): """""" Return an enum from a name Args: enum_name (str): name of the enum Returns: Enum """""" return next((e for e in self.enums if e.name == enum_name), None)",2
489,Python,get name of enumerated value,https://github.com/google/apitools/blob/f3745a7ea535aa0e88b0650c16479b696d6fd446/apitools/base/protorpclite/messages.py#L1786-L1809,"def validate_default_element(self, value): """"""Validate default element of Enum field. Enum fields allow for delayed resolution of default values when the type of the field has not been resolved. The default value of a field may be a string or an integer. If the Enum type of the field has been resolved, the default value is validated against that type. Args: value: Value to validate. Raises: ValidationError if value is not expected message type. """""" if isinstance(value, (six.string_types, six.integer_types)): # Validation of the value does not happen for delayed resolution # enumerated types. Ignore if type is not yet resolved. if self.__type: self.__type(value) return value return super(EnumField, self).validate_default_element(value)",1
2027,Python,get name of enumerated value,https://github.com/ChristianTremblay/BAC0/blob/8d95b065ea068524a08f5b0c34322ebeeba95d06/BAC0/core/devices/Points.py#L734-L744,"def enumValue(self): """""" returns: (str) Enum state value """""" try: return self.properties.units_state[int(self.lastValue) - 1] except IndexError: value = ""unknown"" except ValueError: value = ""NaN"" return value",1
46,Python,get name of enumerated value,https://github.com/kahowell/sdl2-cffi/blob/8e51d97c7c19ca17d8c2994f54b18e8563310270/sdl2/_cffi.py#L155-L159,"def sanitize_enum(self, enum): for name, enumeratorlist in enum.children(): for name, enumerator in enumeratorlist.children(): enumerator.value = c_ast.Constant('dummy', '...') return enum",0
525,Python,get name of enumerated value,https://github.com/iotile/coretools/blob/2d794f5f1346b841b0dcd16c9d284e9bf2f3c6ec/iotilecore/iotile/core/utilities/schema_verify/enum_verify.py#L16-L32,"def verify(self, obj): """"""Verify that the object conforms to this verifier's schema. Args: obj (object): A python object to verify Raises: ValidationError: If there is a problem verifying the object, a ValidationError is thrown with at least the reason key set indicating the reason for the lack of validation. """""" if obj not in self.options: raise ValidationError(""Object is not in list of enumerated options"", reason='not in list of enumerated options', object=obj, options=self.options) return obj",0
224,Python,get inner html,https://github.com/miyakogi/wdom/blob/a21bcd23e94baceee71161829f6897bee3fd39c1/wdom/web_node.py#L305-L311,"def innerHTML(self, html: str) -> None: # type: ignore """"""Set innerHTML both on this node and related browser node."""""" df = self._parse_html(html) if self.connected: self._set_inner_html_web(df.html) self._empty() self._append_child(df)",3
460,Python,get inner html,https://github.com/maxpowel/scrapium/blob/bc12c425aa5978f953a87d05920ba0f61a00409c/scrapium/scrapium.py#L140-L142,"def get_html(self, url): r = self.get(url) return self.html(r.text)",3
1054,Python,get inner html,https://github.com/miyakogi/wdom/blob/a21bcd23e94baceee71161829f6897bee3fd39c1/wdom/tag.py#L155-L159,"def innerHTML(self) -> str: """"""Get innerHTML of the inner node."""""" if self._inner_element: return self._inner_element.innerHTML return super().innerHTML",3
1061,Python,get inner html,https://github.com/miyakogi/wdom/blob/a21bcd23e94baceee71161829f6897bee3fd39c1/wdom/tag.py#L148-L152,"def html(self) -> str: """"""Get whole html representation of this node."""""" if self._inner_element: return self.start_tag + self._inner_element.html + self.end_tag return super().html",3
930,Python,get inner html,https://github.com/KelSolaar/Umbra/blob/66f45f08d9d723787f1191989f8b0dda84b412ce/umbra/reporter.py#L468-L477,"def __set_html(self, html=None): """""" Sets the html content in the View using given body. :param html: Html content. :type html: unicode """""" self.__html = self.__get_html(html) self.__view.setHtml(self.__html)",2
750,Python,get inner html,https://github.com/zenwalker/python-xmltag/blob/5ba900753d939b0f3811c88b0f95ebbbdecd1727/xmltag/nodes.py#L45-L59,"def render(self): indent = self.doc.indent inner = self.content or '' if not self.safe: inner = escape(inner, quote=False) inner += ''.join([n.render() for n in self.child_nodes]) html = self.doc.render_tag(self.tag_name, inner, self.attrs) if indent: pretty_html = '\n' + (indent * self.level) + html if self._is_last(): pretty_html += '\n' + indent * (self.level - 1) html = pretty_html return html",1
819,Python,get inner html,https://github.com/JohnVinyard/zounds/blob/337b3f98753d09eaab1c72dcd37bb852a3fa5ac6/zounds/ui/baseapp.py#L82-L95,"def _get_html(self, html_filename): path, fn = os.path.split(__file__) with open(os.path.join(path, html_filename)) as f: html = f.read() to_replace = [(m.groupdict()['filename'], html[m.start(): m.end()]) for m in self.SCRIPT_TAG.finditer(html)] for filename, tag in to_replace: with open(os.path.join(path, filename)) as scriptfile: html = html.replace( tag, '<script>{}</script>'.format(scriptfile.read())) return html",1
1408,Python,get inner html,https://github.com/tensorflow/lucid/blob/d1a1e2e4fd4be61b89b8cba20dc425a5ae34576e/lucid/scratch/web/svelte.py#L43-L68,"def SvelteComponent(name, path): """"""Display svelte components in iPython. Args: name: name of svelte component (must match component filename when built) path: path to compile svelte .js file or source svelte .html file. (If html file, we try to call svelte and build the file.) Returns: A function mapping data to a rendered svelte component in ipython. """""" if path[-3:] == "".js"": js_path = path elif path[-5:] == "".html"": print(""Trying to build svelte component from html..."") js_path = build_svelte(path) js_content = read(js_path, mode='r') def inner(data): id_str = js_id(name) html = _template \ .replace(""$js"", js_content) \ .replace(""$name"", name) \ .replace(""$data"", json.dumps(data)) \ .replace(""$id"", id_str) _display_html(html) return inner",1
1508,Python,get inner html,https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/rnc_web.py#L408-L416,"def get_png_img_html(blob: Union[bytes, memoryview], extra_html_class: str = None) -> str: """""" Converts a PNG blob to an HTML IMG tag with embedded data. """""" return """"""<img {}src=""{}"" />"""""".format( 'class=""{}"" '.format(extra_html_class) if extra_html_class else """", get_png_data_url(blob) )",1
1234,Python,get inner html,https://github.com/limodou/uliweb/blob/34472f25e4bc0b954a35346672f94e84ef18b076/uliweb/form/widgets.py#L22-L24,def html(self): return ''.join([self.pre_html() % self.kwargs] + [self.to_html()] + [self.post_html() % self.kwargs]),0
8,Python,get executable path,https://github.com/sarugaku/virtenv/blob/fc42a9d8dc9f1821d3893899df78e08a081f6ca3/virtenv_cli.py#L20-L28,"def which(name): for p in os.environ['PATH'].split(os.pathsep): exe = os.path.join(p, name) if is_executable(exe): return os.path.abspath(exe) for ext in [''] + os.environ.get('PATHEXT', '').split(os.pathsep): exe = '{}{}'.format(exe, ext.lower()) if is_executable(exe): return os.path.abspath(exe)",3
68,Python,get executable path,https://github.com/ethereum/pyethereum/blob/b704a5c6577863edc539a1ec3d2620a443b950fb/ethereum/tools/_solidity.py#L24-L43,"def get_compiler_path(): """""" Return the path to the solc compiler. This funtion will search for the solc binary in the $PATH and return the path of the first executable occurence. """""" # If the user provides a specific solc binary let's use that given_binary = os.environ.get('SOLC_BINARY') if given_binary: return given_binary for path in os.getenv('PATH', '').split(os.pathsep): path = path.strip('""') executable_path = os.path.join(path, BINARY) if os.path.isfile(executable_path) and os.access( executable_path, os.X_OK): return executable_path return None",3
282,Python,get executable path,https://github.com/costastf/locationsharinglib/blob/dcd74b0cdb59b951345df84987238763e50ef282/_CI/library/core_library.py#L186-L206,"def get_binary_path(executable, logging_level='INFO'): """"""Gets the software name and returns the path of the binary."""""" if sys.platform == 'win32': if executable == 'start': return executable executable = executable + '.exe' if executable in os.listdir('.'): binary = os.path.join(os.getcwd(), executable) else: binary = next((os.path.join(path, executable) for path in os.environ['PATH'].split(os.pathsep) if os.path.isfile(os.path.join(path, executable))), None) else: venv_parent = get_venv_parent_path() venv_bin_path = os.path.join(venv_parent, '.venv', 'bin') if not venv_bin_path in os.environ.get('PATH'): if logging_level == 'DEBUG': print(f'Adding path {venv_bin_path} to environment PATH variable') os.environ['PATH'] = os.pathsep.join([os.environ['PATH'], venv_bin_path]) binary = shutil.which(executable) return binary if binary else None",3
367,Python,get executable path,https://github.com/pypa/pipenv/blob/cae8d76c210b9777e90aab76e9c4b0e53bb19cde/pipenv/utils.py#L1300-L1316,"def find_windows_executable(bin_path, exe_name): """"""Given an executable name, search the given location for an executable"""""" requested_path = get_windows_path(bin_path, exe_name) if os.path.isfile(requested_path): return requested_path try: pathext = os.environ[""PATHEXT""] except KeyError: pass else: for ext in pathext.split(os.pathsep): path = get_windows_path(bin_path, exe_name + ext.strip().lower()) if os.path.isfile(path): return path return find_executable(exe_name)",3
533,Python,get executable path,https://github.com/nikcub/paths/blob/2200b85273d07d7a3c8b15ceb3b03cbb5c11439a/paths/__init__.py#L85-L107,"def find_executable(executable, path=None): """"""Tries to find 'executable' in the directories listed in 'path'. A string listing directories separated by 'os.pathsep'; defaults to os.environ['PATH']. Returns the complete filename or None if not found. """""" if path is None: path = os.environ['PATH'] paths = path.split(os.pathsep) base, ext = os.path.splitext(executable) if (sys.platform == 'win32' or os.name == 'os2') and (ext != '.exe'): executable = executable + '.exe' if not os.path.isfile(executable): for p in paths: f = os.path.join(p, executable) if os.path.isfile(f): # the file exists, we have a shot at spawn working return f return None else: return executable",3
768,Python,get executable path,https://github.com/shapiromatron/bmds/blob/395c6ce84ad82876fd9fa4a89a3497fb61616de0/bmds/models/base.py#L173-L177,"def get_exe_path(cls): """""" Return the full path to the executable. """""" return os.path.abspath(os.path.join(ROOT, cls.bmds_version_dir, cls.exe + "".exe""))",3
1548,Python,get executable path,https://github.com/kgori/treeCl/blob/fed624b3db1c19cc07175ca04e3eda6905a8d305/treeCl/wrappers/abstract_wrapper.py#L161-L176,"def _search_for_executable(self, executable): """""" Search for file give in ""executable"". If it is not found, we try the environment PATH. Returns either the absolute path to the found executable, or None if the executable couldn't be found. """""" if os.path.isfile(executable): return os.path.abspath(executable) else: envpath = os.getenv('PATH') if envpath is None: return for path in envpath.split(os.pathsep): exe = os.path.join(path, executable) if os.path.isfile(exe): return os.path.abspath(exe)",3
1658,Python,get executable path,https://github.com/etcher-be/elib_run/blob/c9d8ba9f067ab90c5baa27375a92b23f1b97cdde/elib_run/_find_exe.py#L18-L62,"def find_executable(executable: str, *paths: str) -> typing.Optional[Path]: """""" Based on: https://gist.github.com/4368898 Public domain code by anatoly techtonik <techtonik@gmail.com> Programmatic equivalent to Linux `which` and Windows `where` Find if ´executable´ can be run. Looks for it in 'path' (string that lists directories separated by 'os.pathsep'; defaults to os.environ['PATH']). Checks for all executable extensions. Returns full path or None if no command is found. Args: executable: executable name to look for paths: root paths to examine (defaults to system PATH) Returns: executable path as string or None """""" if not executable.endswith('.exe'): executable = f'{executable}.exe' if executable in _KNOWN_EXECUTABLES: return _KNOWN_EXECUTABLES[executable] output = f'{executable}' if not paths: path = os.environ['PATH'] paths = tuple([str(Path(sys.exec_prefix, 'Scripts').absolute())] + path.split(os.pathsep)) executable_path = Path(executable).absolute() if not executable_path.is_file(): for path_ in paths: executable_path = Path(path_, executable).absolute() if executable_path.is_file(): break else: _LOGGER.error('%s -> not found', output) return None _KNOWN_EXECUTABLES[executable] = executable_path _LOGGER.info('%s -> %s', output, str(executable_path)) return executable_path",3
1912,Python,get executable path,https://github.com/ga4gh/ga4gh-common/blob/ea1b562dce5bf088ac4577b838cfac7745f08346/ga4gh/common/utils.py#L30-L40,"def getPathOfExecutable(executable): """""" Returns the full path of the executable, or None if the executable can not be found. """""" exe_paths = os.environ['PATH'].split(':') for exe_path in exe_paths: exe_file = os.path.join(exe_path, executable) if os.path.isfile(exe_file) and os.access(exe_file, os.X_OK): return exe_file return None",3
259,Python,get executable path,https://github.com/calmjs/calmjs/blob/b9b407c2b6a7662da64bccba93bb8d92e7a5fafd/src/calmjs/utils.py#L128-L173,"def which(cmd, mode=os.F_OK | os.X_OK, path=None): """""" Given cmd, check where it is on PATH. Loosely based on the version in python 3.3. """""" if os.path.dirname(cmd): if os.path.isfile(cmd) and os.access(cmd, mode): return cmd if path is None: path = os.environ.get('PATH', defpath) if not path: return None paths = path.split(pathsep) if sys.platform == 'win32': # oh boy if curdir not in paths: paths = [curdir] + paths # also need to check the fileexts... pathext = os.environ.get('PATHEXT', '').split(pathsep) if any(cmd.lower().endswith(ext.lower()) for ext in pathext): files = [cmd] else: files = [cmd + ext for ext in pathext] else: # sanity files = [cmd] seen = set() for p in paths: normpath = normcase(p) if normpath in seen: continue seen.add(normpath) for f in files: fn = os.path.join(p, f) if os.path.isfile(fn) and os.access(fn, mode): return fn return None",1
80,Python,get current process id,https://github.com/djtaylor/python-lsbinit/blob/a41fc551226f61ac2bf1b8b0f3f5395db85e75a2/lsbinit/pid.py#L43-L78,"def ps(self): """""" Get the process information from the system PS command. """""" # Get the process ID pid = self.get() # Parent / child processes parent = None children = [] # If the process is running if pid: proc = Popen(['ps', '-ef'], stdout=PIPE) for _line in proc.stdout.readlines(): line = self.unicode(_line.rstrip()) # Get the current PID / parent PID this_pid, this_parent = self._ps_extract_pid(line) try: # If scanning a child process if int(pid) == int(this_parent): children.append('{}; [{}]'.format(this_pid.rstrip(), re.sub(' +', ' ', line))) # If scanning the parent process if int(pid) == int(this_pid): parent = re.sub(' +', ' ', line) # Ignore value errors except ValueError: continue # Return the parent PID and any children processes return (parent, children)",3
389,Python,get current process id,https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/gurumate-2.8.6-py2.7.egg/gurumate/linux2/procs.py#L43-L46,def get_pid(PROCNAME): for proc in psutil.process_iter(): if proc.name == PROCNAME: return proc.pid,3
532,Python,get current process id,https://github.com/Pajinek/vhm/blob/e323e99855fd5c40fd61fba87c2646a1165505ed/tools/vhmlib/manager.py#L103-L111,"def get_pid(self, id): f = open(self.config[id][""pidfile""]) try: pid = int(f.read().strip()) except ValueError: print ""Wrong PID format (int %s)"" % self.config[id][""pidfile""] sys.exit(1) f.close() return pid",3
1385,Python,get current process id,https://github.com/Jaymon/prom/blob/b7ad2c259eca198da03e1e4bc7d95014c168c361/prom/query.py#L1458-L1464,"def process_id(self): ret = """" if thread: f = getattr(os, 'getpid', None) if f: ret = str(f()) return ret",3
1420,Python,get current process id,https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/pydevd_attach_to_process/winappdbg/thread.py#L204-L226,"def get_pid(self): """""" @rtype: int @return: Parent process global ID. @raise WindowsError: An error occured when calling a Win32 API function. @raise RuntimeError: The parent process ID can't be found. """""" if self.dwProcessId is None: if self.__process is not None: # Infinite loop if self.__process is None self.dwProcessId = self.get_process().get_pid() else: try: # I wish this had been implemented before Vista... # XXX TODO find the real ntdll call under this api hThread = self.get_handle( win32.THREAD_QUERY_LIMITED_INFORMATION) self.dwProcessId = win32.GetProcessIdOfThread(hThread) except AttributeError: # This method really sucks :P self.dwProcessId = self.__get_pid_by_scanning() return self.dwProcessId",3
632,Python,get current process id,https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/cwl/workflow.py#L421-L437,"def _create_variable(orig_v, step, variables): """"""Create a new output variable, potentially over-writing existing or creating new. """""" # get current variable, and convert to be the output of our process step try: v = _get_variable(orig_v[""id""], variables) except ValueError: v = copy.deepcopy(orig_v) if not isinstance(v[""id""], six.string_types): v[""id""] = _get_string_vid(v[""id""]) for key, val in orig_v.items(): if key not in [""id"", ""type""]: v[key] = val if orig_v.get(""type"") != ""null"": v[""type""] = orig_v[""type""] v[""id""] = ""%s/%s"" % (step.name, get_base_id(v[""id""])) return v",1
1076,Python,get current process id,https://github.com/jobec/rfc5424-logging-handler/blob/9c4f669c5e54cf382936cd950e2204caeb6d05f0/rfc5424logging/handler.py#L277-L281,"def get_procid(self, record): procid = getattr(record, 'procid', self.procid) if procid is None or procid == '': procid = getattr(record, 'process', NILVALUE) return self.filter_printusascii(str(procid))",1
36,Python,get current process id,https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/pydevd_attach_to_process/winappdbg/interactive.py#L669-L676,"def get_process_id_from_prefix(self): if self.cmdprefix: pid = self.input_process(self.cmdprefix) else: if self.lastEvent is None: raise CmdError(""no current process set"") pid = self.lastEvent.get_pid() return pid",0
601,Python,get current process id,https://github.com/helixyte/everest/blob/70c9b93c3061db5cb62428349d18b8fb8566411b/everest/representers/xml.py#L440-L450,def get_id(self): # FIXME: This will not work with ID strings that happen to be # convertible to an int. id_str = self.get('id') try: id_val = int(id_str) except ValueError: id_val = id_str except TypeError: # Happens if the id string is None. id_val = id_str return id_val,0
577,Python,get current observable value,https://github.com/jor-/util/blob/0eb0be84430f88885f4d48335596ca8881f85587/util/observable/decorator.py#L114-L143,"def _set_observable(self, observable_name, new_value): ## check old value if self._has_value(observable_name): old_value = self.new_value(observable_name) ## set only if different value must_set = np.any(new_value != old_value) if must_set: ## if values are observable_names with observers call associated observers of sub observable_names if isinstance(old_value, Observable) and isinstance(new_value, Observable): ## copy observer old_value.copy_observers_to(new_value) ## notify old observer old_value._notify_observers(observable_name=None, include_everything_observers=True) for observable_name in old_value._observers.keys(): old_has_value = old_value._has_value(observable_name) new_has_value = new_value._has_value(observable_name) if old_has_value != new_has_value or (old_has_value and new_has_value and np.any(old_value._get_value(observable_name) != new_value._get_value(observable_name))): old_value._notify_observers(observable_name=observable_name, include_everything_observers=False) else: must_set = True ## set new observable_name value and call observer if must_set: self._set_value(observable_name, new_value) self._notify_observers(observable_name)",3
1496,Python,get current observable value,https://github.com/Qiskit/qiskit-terra/blob/d4f58d903bc96341b816f7c35df936d6421267d1/qiskit/quantum_info/analyzation/make_observable.py#L13-L37,"def make_dict_observable(matrix_observable): """"""Convert an observable in matrix form to dictionary form. Takes in a diagonal observable as a matrix and converts it to a dictionary form. Can also handle a list sorted of the diagonal elements. Args: matrix_observable (list): The observable to be converted to dictionary form. Can be a matrix or just an ordered list of observed values Returns: Dict: A dictionary with all observable states as keys, and corresponding values being the observed value for that state """""" dict_observable = {} observable = np.array(matrix_observable) observable_size = len(observable) observable_bits = int(np.ceil(np.log2(observable_size))) binary_formater = '0{}b'.format(observable_bits) if observable.ndim == 2: observable = observable.diagonal() for state_no in range(observable_size): state_str = format(state_no, binary_formater) dict_observable[state_str] = observable[state_no] return dict_observable",2
17,Python,get current observable value,https://github.com/fredericklussier/ObservablePy/blob/fd7926a0568621f80b1d567d18f199976f1fa4e8/observablePy/ObservableStore.py#L72-L84,"def add(self, observableElement): """""" add an observable element :param str observableElement: the name of the observable element :raises RuntimeError: if element name already exist in the store """""" if observableElement not in self._observables: self._observables.append(observableElement) else: raise RuntimeError( ""{0} is already an observable element"" .format(observableElement))",1
644,Python,get current observable value,https://github.com/timofurrer/observable/blob/a6a764efaf9408a334bdb1ddf4327d9dbc4b8eaa/observable/property.py#L70-L95,"def _trigger_event( self, holder: T.Any, alt_name: str, action: str, *event_args: T.Any ) -> None: """"""Triggers an event on the associated Observable object. The Holder is the object this property is a member of, alt_name is used as the event name when self.event is not set, action is prepended to the event name and event_args are passed through to the registered event handlers."""""" if isinstance(self.observable, Observable): observable = self.observable elif isinstance(self.observable, str): observable = getattr(holder, self.observable) elif isinstance(holder, Observable): observable = holder else: raise TypeError( ""This ObservableProperty is no member of an Observable "" ""object. Specify where to find the Observable object for "" ""triggering events with the observable keyword argument "" ""when initializing the ObservableProperty."" ) name = alt_name if self.event is None else self.event event = ""{}_{}"".format(action, name) observable.trigger(event, *event_args)",1
892,Python,get current observable value,https://github.com/Nekmo/simple-monitor-alert/blob/11d6dbd3c0b3b9a210d6435208066f5636f1f44e/monitors/web.py#L91-L101,"def _sma_observable(observable_name, expected, function=None, param=None, value=None): param = param or os.environ.get(observable_name) if not param: return groups = parse_param(param) if not isinstance(param, list) else param function = function or globals()['test_' + observable_name] value = value or function(*groups) name = function.__name__.replace('test_', '').replace('_', ' ').capitalize() print('{}.name = ""{} {}""'.format(observable_name, name, groups[1])) print('{}.value = {}'.format(observable_name, value)) print('{}.expected = {}'.format(observable_name, expected))",1
1107,Python,get current observable value,https://github.com/Nekmo/simple-monitor-alert/blob/11d6dbd3c0b3b9a210d6435208066f5636f1f44e/simple_monitor_alert/sma.py#L69-L75,"def get_monitor_observables(self, name): try: lines = self.items(name) except NoSectionError: return [] lines = [ItemLine(key, value) for key, value in lines] return get_observables_from_lines(lines)",1
1296,Python,get current observable value,https://github.com/RPi-Distro/python-gpiozero/blob/7b67374fd0c8c4fde5586d9bad9531f076db9c0c/gpiozero/mixins.py#L539-L546,"def value(self): if not self.partial: self.full.wait() try: return self.average(self.queue) except (ZeroDivisionError, ValueError): # No data == inactive value return 0.0",1
1488,Python,get current observable value,https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/adapters/acquirefielddefaults.py#L44-L57,"def __call__(self, field): acquire = getattr(field, 'acquire', True) if acquire: fieldname = getattr(field, 'acquire_fieldname', field.getName()) current = self.context while hasattr(current, 'aq_parent'): current = current.aq_parent if IPloneSiteRoot.providedBy(current): break if fieldname in current.Schema()._names: value = current.Schema()[fieldname].get(current) if value is not None: return value",1
1836,Python,get current observable value,https://github.com/timofurrer/observable/blob/a6a764efaf9408a334bdb1ddf4327d9dbc4b8eaa/observable/property.py#L102-L108,"def create_with( cls, event: str = None, observable: T.Union[str, Observable] = None ) -> T.Callable[..., ""ObservableProperty""]: """"""Creates a partial application of ObservableProperty with event and observable preset."""""" return functools.partial(cls, event=event, observable=observable)",1
2066,Python,get current observable value,https://github.com/ARMmbed/mbed-cloud-sdk-python/blob/c0af86fb2cdd4dc7ed26f236139241067d293509/examples/connect/set-resource.py#L23-L42,"def _main(): api = ConnectAPI() # calling start_notifications is required for getting/setting resource synchronously api.start_notifications() devices = api.list_connected_devices().data if not devices: raise Exception(""No connected devices registered. Aborting"") # Synchronously get the initial/current value of the resource value = api.get_resource_value(devices[0].id, WRITEABLE_RESOURCE) print(""Current value: %r"" % (value,)) # Set Resource value. Resource needs to have type == ""writable_resource"" api.set_resource_value(device_id=devices[0].id, resource_path=WRITEABLE_RESOURCE, resource_value='10') # Synchronously get the current value of the resource value = api.get_resource_value(devices[0].id, WRITEABLE_RESOURCE) print(""Current value: %r"" % (value,))",1
340,Python,get current ip address,https://github.com/UpCloudLtd/upcloud-python-api/blob/954b0ad7c4b932b2be31a95d88975f6b0eeac8ed/upcloud_api/cloud_manager/ip_address_mixin.py#L16-L23,"def get_ip(self, address): """""" Get an IPAddress object with the IP address (string) from the API. e.g manager.get_ip('80.69.175.210') """""" res = self.get_request('/ip_address/' + address) return IPAddress(cloud_manager=self, **res['ip_address'])",3
1037,Python,get current ip address,https://github.com/kencochrane/django-defender/blob/e3e547dbb83235e0d564a6d64652c7df00412ff2/defender/utils.py#L41-L50,"def get_ip(request): """""" get the ip address from the request """""" if config.BEHIND_REVERSE_PROXY: ip_address = request.META.get(config.REVERSE_PROXY_HEADER, '') ip_address = ip_address.split("","", 1)[0].strip() if ip_address == '': ip_address = get_ip_address_from_request(request) else: ip_address = get_ip_address_from_request(request) return ip_address",3
1545,Python,get current ip address,https://github.com/emc-openstack/storops/blob/24b4b13bf065c0ef0538dd0b5ebb8f25d24176bd/storops/vnx/resource/mover.py#L203-L208,def get_ip(self): if self._ip is not None: ret = self._ip else: ret = self.ip_addr return ret,3
1711,Python,get current ip address,https://github.com/danielfrg/datasciencebox/blob/6b7aa642c6616a46547035fcb815acc1de605a6f/datasciencebox/core/cloud/instance.py#L80-L83,def get_ip(self): if self._ip is None: self._ip = self.fetch_ip() return self._ip,3
951,Python,get current ip address,https://github.com/dlancer/django-crispy-contact-form/blob/3d422556add5aea3607344a034779c214f84da04/contact_form/helpers.py#L8-L19,"def get_user_ip(request): """"""Return user ip :param request: Django request object :return: user ip """""" ip = get_real_ip(request) if ip is None: ip = get_ip(request) if ip is None: ip = '127.0.0.1' return ip",2
1844,Python,get current ip address,https://github.com/fy0/slim/blob/9951a910750888dbe7dd3e98acae9c40efae0689/slim/base/view.py#L128-L136,"async def get_ip(self) -> Union[IPv4Address, IPv6Address]: """""" get ip address of client :return: """""" xff = await self.get_x_forwarded_for() if xff: return xff[0] ip_addr = self._request.transport.get_extra_info('peername')[0] return ip_address(ip_addr)",2
1065,Python,get current ip address,https://github.com/softlayer/softlayer-python/blob/9f181be08cc3668353b05a6de0cb324f52cff6fa/SoftLayer/CLI/vpn/ipsec/detail.py#L63-L84,"def _get_address_translations_table(address_translations): """"""Yields a formatted table to print address translations. :param List[dict] address_translations: List of address translations. :return Table: Formatted for address translation output. """""" table = formatting.Table(['id', 'static IP address', 'static IP address id', 'remote IP address', 'remote IP address id', 'note']) for address_translation in address_translations: table.add_row([address_translation.get('id', ''), address_translation.get('internalIpAddressRecord', {}) .get('ipAddress', ''), address_translation.get('internalIpAddressId', ''), address_translation.get('customerIpAddressRecord', {}) .get('ipAddress', ''), address_translation.get('customerIpAddressId', ''), address_translation.get('notes', '')]) return table",1
1723,Python,get current ip address,https://github.com/istresearch/scrapy-cluster/blob/13aaed2349af5d792d6bcbfcadc5563158aeb599/crawler/crawling/distributed_scheduler.py#L275-L298,"def update_ipaddress(self): ''' Updates the scheduler so it knows its own ip address ''' # assign local ip in case of exception self.old_ip = self.my_ip self.my_ip = '127.0.0.1' try: obj = urllib.request.urlopen(settings.get('PUBLIC_IP_URL', 'http://ip.42.pl/raw')) results = self.ip_regex.findall(obj.read()) if len(results) > 0: self.my_ip = results[0] else: raise IOError(""Could not get valid IP Address"") obj.close() self.logger.debug(""Current public ip: {ip}"".format(ip=self.my_ip)) except IOError: self.logger.error(""Could not reach out to get public ip"") pass if self.old_ip != self.my_ip: self.logger.info(""Changed Public IP: {old} -> {new}"".format( old=self.old_ip, new=self.my_ip))",1
13,Python,get current ip address,https://github.com/rshipp/python-dshield/blob/1b003d0dfac0bc2ee8b86ca5f1a44b765b8cc6e0/dshield.py#L57-L74,"def ip(ip_address, return_format=None): """"""Returns a summary of the information our database holds for a particular IP address (similar to /ipinfo.html). In the returned data: Count: (also reports or records) total number of packets blocked from this IP. Attacks: (also targets) number of unique destination IP addresses for these packets. :param ip_address: a valid IP address """""" response = _get('ip/{address}'.format(address=ip_address), return_format) if 'bad IP address' in str(response): raise Error('Bad IP address, {address}'.format(address=ip_address)) else: return response",0
160,Python,get current ip address,https://github.com/douban/pymesos/blob/c8096d3d510648649963b7cead01d4d06b5dcf26/pymesos/scheduler.py#L473-L490,"def onNewMasterDetectedMessage(self, data): master = None try: if isinstance(data, six.binary_type): data = data.decode('utf-8') parsed = json.loads(data) if parsed and ""address"" in parsed: ip = parsed[""address""].get(""ip"") port = parsed[""address""].get(""port"") if ip and port: master = ""%s:%s"" % (ip, port) except Exception: logger.exception(""No JSON content, probably connecting "" ""to older Mesos version."") if master: self.change_master(master)",0
236,Python,get all parents of xml node,https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/mo_graphs/graph.py#L58-L65,"def get_edges(self, node): return [ (node, child) for child in self.get_children(node) ] + [ (parent, node) for parent in self.get_parents(node) ]",3
1567,Python,get all parents of xml node,https://github.com/fishtown-analytics/dbt/blob/aa4f771df28b307af0cf9fe2fc24432f10a8236b/core/dbt/graph/selector.py#L317-L344,"def get_ancestor_ephemeral_nodes(self, selected_nodes): node_names = {} for node_id in selected_nodes: if node_id not in self.manifest.nodes: continue node = self.manifest.nodes[node_id] # sources don't have ancestors and this results in a silly select() if node.resource_type == NodeType.Source: continue node_names[node_id] = node.name include_spec = [ '+{}'.format(node_names[node]) for node in selected_nodes if node in node_names ] if not include_spec: return set() all_ancestors = self.select_nodes(self.linker.graph, include_spec, []) res = [] for ancestor in all_ancestors: ancestor_node = self.manifest.nodes.get(ancestor, None) if ancestor_node and self.is_ephemeral_model(ancestor_node): res.append(ancestor) return set(res)",3
1931,Python,get all parents of xml node,https://github.com/emc-openstack/storops/blob/24b4b13bf065c0ef0538dd0b5ebb8f25d24176bd/storops/vnx/resource/nfs_share.py#L38-L50,"def get_xml_node(self): xb = xmlapi.XmlBuilder() ret = [] if self.access_hosts is not None: ret.append(xb.list_elements('AccessHosts', self.access_hosts)) if self.rw_hosts is not None: ret.append(xb.list_elements('RwHosts', self.rw_hosts)) if self.ro_hosts is not None: ret.append(xb.list_elements('RoHosts', self.ro_hosts)) if self.root_hosts is not None: ret.append(xb.list_elements('RootHosts', self.root_hosts)) return ret",3
249,Python,get all parents of xml node,https://github.com/mfitzp/biocyc/blob/2fe81971687e4dcf1fcf869af0e7b3549be535b1/biocyc/biocyc.py#L566-L571,"def _import_parents_from_xml(self, xml): parents = xml.iterfind('parent') for p in parents: for o in p: # Store a tuple of orgid, identifier self._parents.append( o.attrib['frameid'] ) #( o.attrib['orgid'], ) )",2
680,Python,get all parents of xml node,https://github.com/inveniosoftware-contrib/json-merger/blob/adc6d372da018427e1db7b92424d3471e01a4118/json_merger/contrib/inspirehep/match.py#L199-L207,"def _find(self, node): root = node while root in self.parents: root = self.parents[root] while node in self.parents: prev_node = node node = self.parents[node] self.parents[prev_node] = root return root",2
966,Python,get all parents of xml node,https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/mo_graphs/tree_graph.py#L39-L44,"def get_parents(self, node): parent = self.parents.get(node) if parent == None: return set() else: return {parent}",2
1406,Python,get all parents of xml node,https://github.com/incf-nidash/nidmresults/blob/438f7cce6abc4a4379b629bd76f4d427891e033f/nidmresults/owl/owl_reader.py#L85-L93,"def get_nidm_parent(self, term): # Find direct nidm parent of 'term' parents = self.get_direct_parents(term) for parent in parents: if not self.is_external_namespace(parent): return parent return None",2
847,Python,get all parents of xml node,https://github.com/klahnakoski/pyLibrary/blob/fa2dcbc48fda8d26999baef400e9a98149e0b982/mo_graphs/algorithms.py#L115-L202,"def dominator_tree(graph): """""" RETURN DOMINATOR FOREST THERE ARE TWO TREES, ""ROOTS"" and ""LOOPS"" ROOTS HAVE NO PARENTS LOOPS ARE NODES THAT ARE A MEMBER OF A CYCLE THAT HAS NO EXTRNAL PARENT roots = dominator_tree(graph).get_children(ROOTS) """""" todo = Queue() done = set() dominator = Tree(None) nodes = list(graph.nodes) while True: # FIGURE OUT NET ITEM TO WORK ON if todo: node = todo.pop() elif nodes: node = nodes.pop() if len(nodes) % 1000 == 0: Log.note(""{{num}} nodes remaining"", num=len(nodes)) else: break if node in done: continue parents = graph.get_parents(node) - {node} if not parents: # node WITHOUT parents IS A ROOT done.add(node) dominator.add_edge(Edge(ROOTS, node)) continue not_done = parents - done if not_done: # THERE ARE MORE parents TO DO FIRST more_todo = not_done - todo if not more_todo: # ALL PARENTS ARE PART OF A CYCLE, MAKE node A ROOT done.add(node) dominator.add_edge(Edge(LOOPS, node)) else: # DO THE PARENTS BEFORE node todo.push(node) for p in more_todo: todo.push(p) continue # WE CAN GET THE DOMINATORS FOR ALL parents if len(parents) == 1: # SHORTCUT dominator.add_edge(Edge(list(parents)[0], node)) done.add(node) continue paths_from_roots = [ list(reversed(dominator.get_path_to_root(p))) for p in parents ] if any(p[0] is ROOTS for p in paths_from_roots): # THIS OBJECT CAN BE REACHED FROM A ROOT, IGNORE PATHS FROM LOOPS paths_from_roots = [p for p in paths_from_roots if p[0] is ROOTS] if len(paths_from_roots) == 1: # SHORTCUT dom = paths_from_roots[0][-1] dominator.add_edge(Edge(dom, node)) done.add(node) continue # FIND COMMON PATH FROM root num_paths = len(paths_from_roots) for i, x in enumerate(zip_longest(*paths_from_roots)): if x.count(x[0]) != num_paths: dom = paths_from_roots[0][i-1] if dom is LOOPS: # CAN BE REACHED FROM MORE THAN ONE LOOP, PICK ONE TO BLAME dom = paths_from_roots[0][-1] break else: # ALL PATHS IDENTICAL dom = paths_from_roots[0][-1] dominator.add_edge(Edge(dom, node)) done.add(node) return dominator",1
1139,Python,get all parents of xml node,https://github.com/tanghaibao/goatools/blob/407682e573a108864a79031f8ca19ee3bf377626/goatools/obo_parser.py#L205-L211,"def get_all_parents(self): """"""Return all parent GO IDs."""""" all_parents = set() for parent in self.parents: all_parents.add(parent.item_id) all_parents |= parent.get_all_parents() return all_parents",1
347,Python,get all parents of xml node,https://github.com/pgmpy/pgmpy/blob/9381a66aba3c3871d3ccd00672b148d17d63239e/pgmpy/estimators/MLE.py#L84-L139,"def estimate_cpd(self, node): """""" Method to estimate the CPD for a given variable. Parameters ---------- node: int, string (any hashable python object) The name of the variable for which the CPD is to be estimated. Returns ------- CPD: TabularCPD Examples -------- >>> import pandas as pd >>> from pgmpy.models import BayesianModel >>> from pgmpy.estimators import MaximumLikelihoodEstimator >>> data = pd.DataFrame(data={'A': [0, 0, 1], 'B': [0, 1, 0], 'C': [1, 1, 0]}) >>> model = BayesianModel([('A', 'C'), ('B', 'C')]) >>> cpd_A = MaximumLikelihoodEstimator(model, data).estimate_cpd('A') >>> print(cpd_A) ╒══════╤══════════╕ │ A(0) │ 0.666667 │ ├──────┼──────────┤ │ A(1) │ 0.333333 │ ╘══════╧══════════╛ >>> cpd_C = MaximumLikelihoodEstimator(model, data).estimate_cpd('C') >>> print(cpd_C) ╒══════╤══════╤══════╤══════╤══════╕ │ A │ A(0) │ A(0) │ A(1) │ A(1) │ ├──────┼──────┼──────┼──────┼──────┤ │ B │ B(0) │ B(1) │ B(0) │ B(1) │ ├──────┼──────┼──────┼──────┼──────┤ │ C(0) │ 0.0 │ 0.0 │ 1.0 │ 0.5 │ ├──────┼──────┼──────┼──────┼──────┤ │ C(1) │ 1.0 │ 1.0 │ 0.0 │ 0.5 │ ╘══════╧══════╧══════╧══════╧══════╛ """""" state_counts = self.state_counts(node) # if a column contains only `0`s (no states observed for some configuration # of parents' states) fill that column uniformly instead state_counts.ix[:, (state_counts == 0).all()] = 1 parents = sorted(self.model.get_parents(node)) parents_cardinalities = [len(self.state_names[parent]) for parent in parents] node_cardinality = len(self.state_names[node]) cpd = TabularCPD(node, node_cardinality, np.array(state_counts), evidence=parents, evidence_card=parents_cardinalities, state_names=self.state_names) cpd.normalize() return cpd",0
330,Python,fuzzy match ranking,https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/filtering/filt_obj.py#L319-L344,"def fuzzmatch(self, fuzzkey, multi=False): """""" Identify a filter by fuzzy string matching. Partial ('fuzzy') matching performed by `fuzzywuzzy.fuzzy.ratio` Parameters ---------- fuzzkey : str A string that partially matches one filter name more than the others. Returns ------- The name of the most closely matched filter. : str """""" keys, ratios = np.array([(f, seqm(None, fuzzkey, f).ratio()) for f in self.components.keys()]).T mratio = max(ratios) if multi: return keys[ratios == mratio] else: if sum(ratios == mratio) == 1: return keys[ratios == mratio][0] else: raise ValueError(""\nThe filter key provided ('{:}') matches two or more filter names equally well:\n"".format(fuzzkey) + ', '.join(keys[ratios == mratio]) + ""\nPlease be more specific!"")",3
881,Python,fuzzy match ranking,https://github.com/hearsaycorp/normalize/blob/8b36522ddca6d41b434580bd848f3bdaa7a999c8/normalize/diff.py#L547-L588,"def _fuzzy_match(set_a, set_b): seen = dict() scores = list() # Yes, this is O(n.m), but python's equality operator is # fast for hashable types. for a_pk_seq, b_pk_seq in product(set_a, set_b): a_pk, a_seq = a_pk_seq b_pk, b_seq = b_pk_seq if (a_pk, b_pk) in seen: if seen[a_pk, b_pk][0]: score = list(seen[a_pk, b_pk]) scores.append(score + [a_pk_seq, b_pk_seq]) else: match = 0 common = min((len(a_pk), len(b_pk))) no_match = max((len(a_pk), len(b_pk))) - common for i in range(0, common): if a_pk[i] == b_pk[i]: if not _nested_falsy(a_pk[i]): match += 1 else: no_match += 1 seen[a_pk, b_pk] = (match, no_match) if match: scores.append([match, no_match, a_pk_seq, b_pk_seq]) remaining_a = set(set_a) remaining_b = set(set_b) for match, no_match, a_pk_seq, b_pk_seq in sorted( scores, key=lambda x: x[0] - x[1], reverse=True, ): if a_pk_seq in remaining_a and b_pk_seq in remaining_b: remaining_a.remove(a_pk_seq) remaining_b.remove(b_pk_seq) yield a_pk_seq, b_pk_seq if not remaining_a or not remaining_b: break",3
496,Python,fuzzy match ranking,https://github.com/CalebBell/fluids/blob/57f556752e039f1d3e5a822f408c184783db2828/fluids/friction.py#L60-L71,"def fuzzy_match(name, strings): global fuzzy_match_fun if fuzzy_match_fun is not None: return fuzzy_match_fun(name, strings) try: from fuzzywuzzy import process, fuzz fuzzy_match_fun = lambda name, strings: process.extractOne(name, strings, scorer=fuzz.partial_ratio)[0] except ImportError: # pragma: no cover import difflib fuzzy_match_fun = lambda name, strings: difflib.get_close_matches(name, strings, n=1, cutoff=0)[0] return fuzzy_match_fun(name, strings)",2
1564,Python,fuzzy match ranking,https://github.com/toejough/pimento/blob/cdb00a93976733aa5521f8504152cedeedfc711a/pimento/__init__.py#L298-L311,"def _exact_match(response, matches, insensitive, fuzzy): ''' returns an exact match, if it exists, given parameters for the match ''' for match in matches: if response == match: return match elif insensitive and response.lower() == match.lower(): return match elif fuzzy and _exact_fuzzy_match(response, match, insensitive): return match else: return None",2
656,Python,fuzzy match ranking,https://github.com/coleifer/peewee/blob/ea9403b01acb039adb3a2472186d795c796b77a0/playhouse/sqlite_ext.py#L1136-L1169,"def rank(raw_match_info, *raw_weights): # Handle match_info called w/default args 'pcx' - based on the example rank # function http://sqlite.org/fts3.html#appendix_a match_info = _parse_match_info(raw_match_info) score = 0.0 p, c = match_info[:2] weights = get_weights(c, raw_weights) # matchinfo X value corresponds to, for each phrase in the search query, a # list of 3 values for each column in the search table. # So if we have a two-phrase search query and three columns of data, the # following would be the layout: # p0 : c0=[0, 1, 2], c1=[3, 4, 5], c2=[6, 7, 8] # p1 : c0=[9, 10, 11], c1=[12, 13, 14], c2=[15, 16, 17] for phrase_num in range(p): phrase_info_idx = 2 + (phrase_num * c * 3) for col_num in range(c): weight = weights[col_num] if not weight: continue col_idx = phrase_info_idx + (col_num * 3) # The idea is that we count the number of times the phrase appears # in this column of the current row, compared to how many times it # appears in this column across all rows. The ratio of these values # provides a rough way to score based on ""high value"" terms. row_hits = match_info[col_idx] all_rows_hits = match_info[col_idx + 1] if row_hits > 0: score += weight * (float(row_hits) / all_rows_hits) return -score",1
1863,Python,fuzzy match ranking,https://github.com/azraq27/gini/blob/3c2b5265d096d606b303bfe25ac9adb74b8cee14/gini/matching.py#L63-L68,"def best_item_from_list(item,options,fuzzy=90,fname_match=True,fuzzy_fragment=None,guess=False): '''Returns just the best item, or ``None``''' match = best_match_from_list(item,options,fuzzy,fname_match,fuzzy_fragment,guess) if match: return match[0] return None",1
2040,Python,fuzzy match ranking,https://github.com/PrefPy/prefpy/blob/f395ba3782f05684fa5de0cece387a6da9391d02/prefpy/egmm_mixpl.py#L69-L90,"def Dictionarize(rankings, m): rankcnt = {} #print(""1"",rankings[1]) for ranking in rankings: #print(""2"",ranking) flag = 0 l = len(ranking) if len(set(ranking)) < l: print(""Orders with duplicate alternatives are ignored!"") continue for i in range(l): if ranking[i] >= m or ranking[i] < 0: flag = 1 if flag == 1: print(""Alternative index out of range! Ranking ignored!"") continue key = rank2str(ranking) if key in rankcnt: rankcnt[key] += 1 else: rankcnt[key] = 1 return rankcnt",1
382,Python,fuzzy match ranking,https://github.com/locationlabs/mockredis/blob/fd4e3117066ff0c24e86ebca007853a8092e3254/mockredis/client.py#L1193-L1196,"def zrank(self, name, value): zset = self._get_zset(name, ""ZRANK"") return zset.rank(self._encode(value)) if zset else None",0
711,Python,fuzzy match ranking,https://github.com/mbodenhamer/syn/blob/aeaa3ad8a49bac8f50cf89b6f1fe97ad43d1d258/syn/base_utils/logic.py#L59-L61,"def fuzzy_xor(a, b): return fuzzy_and(fuzzy_or(a, b), fuzzy_not(fuzzy_equiv(a, b)))",0
1017,Python,fuzzy match ranking,https://github.com/sosreport/sos/blob/2ebc04da53dc871c8dd5243567afa4f8592dca29/sos/policies/__init__.py#L713-L724,"def find_preset(self, preset): """"""Find a preset profile matching the specified preset string. :param preset: a string containing a preset profile name. :returns: a matching PresetProfile. """""" # FIXME: allow fuzzy matching? for match in self.presets.keys(): if match == preset: return self.presets[match] return None",0
542,Python,format date,https://github.com/stestagg/dateformat/blob/4743f5dabf1eaf66524247328c76cfd3a05d0daf/benchmark/benchmark_parsing.py#L22-L24,"def format_date_list(dates): format = dateformat.DateFormat(""YYYY-MM-DD hh:mm:ss"") return [format.format(date) for date in dates]",3
634,Python,format date,https://github.com/geex-arts/django-jet/blob/64d5379f8a2278408694ce7913bf25f26035b855/jet/dashboard/dashboard_modules/google_analytics.py#L267-L279,"def format_grouped_date(self, data, group): date = self.get_grouped_date(data, group) if group == 'week': date = u'%s — %s' % ( (date - datetime.timedelta(days=6)).strftime('%d.%m'), date.strftime('%d.%m') ) elif group == 'month': date = date.strftime('%b, %Y') else: date = formats.date_format(date, 'DATE_FORMAT') return date",3
747,Python,format date,https://github.com/LandRegistry/lr-utils/blob/811c9e5c11678a04ee203fa55a7c75080f4f9d89/lrutils/templatefilters/template_filters.py#L10-L12,"def dateformat(value, format='%-d %B %Y'): new_date = parser.parse(value, dayfirst=True) return new_date.strftime(format)",3
864,Python,format date,https://github.com/saschpe/rapport/blob/ccceb8f84bd7e8add88ab5e137cdab6424aa4683/rapport/util.py#L55-L69,"def datetime_from_iso8601(date): """"""Small helper that parses ISO-8601 date dates. >>> datetime_from_iso8601(""2013-04-10T12:52:39"") datetime.datetime(2013, 4, 10, 12, 52, 39) >>> datetime_from_iso8601(""2013-01-07T12:55:19.257"") datetime.datetime(2013, 1, 7, 12, 55, 19, 257000) """""" format = ISO8610_FORMAT if date.endswith(""Z""): date = date[:-1] # Date date is UTC if re.match("".*\.\d+"", date): # Date includes microseconds format = ISO8610_FORMAT_MICROSECONDS return datetime.datetime.strptime(date, format)",3
1212,Python,format date,https://github.com/BernardFW/bernard/blob/9c55703e5ffe5717c9fa39793df59dbfa5b4c5ab/src/bernard/i18n/_formatter.py#L80-L85,"def format_date(self, value, format_): """""" Format the date using Babel """""" date_ = make_date(value) return dates.format_date(date_, format_, locale=self.lang)",3
1471,Python,format date,https://github.com/ofek/pypinfo/blob/48d56e690d7667ae5854752c3a2dc07e321d5637/pypinfo/core.py#L65-L70,"def format_date(date, timestamp_format): try: date = DATE_ADD.format(int(date)) except ValueError: date = timestamp_format.format(date) return date",3
1475,Python,format date,https://github.com/abilian/abilian-core/blob/0a71275bf108c3d51e13ca9e093c0249235351e3/abilian/web/admin/panels/audit.py#L29-L33,"def format_date_for_input(date): date_fmt = get_locale().date_formats[""short""].pattern # force numerical months date_fmt = date_fmt.replace(""MMMM"", ""MM"").replace(""MMM"", ""MM"") return format_date(date, date_fmt)",3
2001,Python,format date,https://github.com/binux/pyspider/blob/3fccfabe2b057b7a56d4a4c79dc0dd6cd2239fe9/pyspider/libs/utils.py#L72-L130,"def format_date(date, gmt_offset=0, relative=True, shorter=False, full_format=False): """"""Formats the given date (which should be GMT). By default, we return a relative time (e.g., ""2 minutes ago""). You can return an absolute date string with ``relative=False``. You can force a full format date (""July 10, 1980"") with ``full_format=True``. This method is primarily intended for dates in the past. For dates in the future, we fall back to full format. From tornado """""" if not date: return '-' if isinstance(date, float) or isinstance(date, int): date = datetime.datetime.utcfromtimestamp(date) now = datetime.datetime.utcnow() if date > now: if relative and (date - now).seconds < 60: # Due to click skew, things are some things slightly # in the future. Round timestamps in the immediate # future down to now in relative mode. date = now else: # Otherwise, future dates always use the full format. full_format = True local_date = date - datetime.timedelta(minutes=gmt_offset) local_now = now - datetime.timedelta(minutes=gmt_offset) local_yesterday = local_now - datetime.timedelta(hours=24) difference = now - date seconds = difference.seconds days = difference.days format = None if not full_format: ret_, fff_format = fix_full_format(days, seconds, relative, shorter, local_date, local_yesterday) format = fff_format if ret_: return format else: format = format if format is None: format = ""%(month_name)s %(day)s, %(year)s"" if shorter else \ ""%(month_name)s %(day)s, %(year)s at %(time)s"" str_time = ""%d:%02d"" % (local_date.hour, local_date.minute) return format % { ""month_name"": local_date.strftime('%b'), ""weekday"": local_date.strftime('%A'), ""day"": str(local_date.day), ""year"": str(local_date.year), ""month"": local_date.month, ""time"": str_time }",3
47,Python,format date,https://github.com/EconForge/dolo/blob/d91ddf148b009bf79852d9aec70f3a1877e0f79a/dolo/compiler/symbolic.py#L10-L17,"def std_tsymbol(tsymbol): s, date = tsymbol if date == 0: return '_{}_'.format(s) elif date <= 0: return '_{}_m{}_'.format(s, str(-date)) elif date >= 0: return '_{}__{}_'.format(s, str(date))",2
485,Python,format date,https://github.com/stestagg/dateformat/blob/4743f5dabf1eaf66524247328c76cfd3a05d0daf/benchmark/benchmark_formatting.py#L29-L32,"def format_dateformat(dates): format = dateformat.DateFormat(""YYYY-MM-DD hh:mm:ss"") for date in dates: assert isinstance(format.format(date), str)",2
504,Python,finding time elapsed using a timer,https://github.com/AFriemann/simple_tools/blob/27d0f838c23309ebfc8afb59511220c6f8bb42fe/simple_tools/contextmanagers/time.py#L16-L20,"def timer(name): startTime = time.time() yield elapsedTime = time.time() - startTime print('[{}] finished in {} ms'.format(name, int(elapsedTime * 1000)))",3
642,Python,finding time elapsed using a timer,https://github.com/alexmojaki/littleutils/blob/1132d2d2782b05741a907d1281cd8c001f1d1d9d/littleutils/__init__.py#L716-L739,"def timer(description='Operation', log=None): """""" Simple context manager which logs (if log is provided) or prints the time taken in seconds for the block to complete. >>> with timer(): ... sleep(0.1) # doctest:+ELLIPSIS Operation took 0.1... seconds >>> with timer('Sleeping'): ... sleep(0.2) # doctest:+ELLIPSIS Sleeping took 0.2... seconds >>> with timer(description='Doing', log=PrintingLogger()): ... sleep(0.3) # doctest:+ELLIPSIS Doing took 0.3... seconds """""" start = time() yield elapsed = time() - start message = '%s took %s seconds' % (description, elapsed) (print if log is None else log.info)(message)",3
2004,Python,finding time elapsed using a timer,https://github.com/djf604/chunky-pipes/blob/cd5b2a31ded28ab949da6190854228f1c8897882/chunkypipes/util/decorators/__init__.py#L6-L14,"def timed(func): @functools.wraps(func) def timer(*args, **kwargs): start_time = time() result = func(*args, **kwargs) elapsed_time = str(timedelta(seconds=int(time() - start_time))) print('Elapsed time is {}'.format(elapsed_time)) return result return timer",3
209,Python,finding time elapsed using a timer,https://github.com/sony/nnabla/blob/aaf3d33b7cbb38f2a03aa754178ba8f7c8481320/python/benchmark/function/function_benchmark.py#L224-L234,"def _calc_benchmark_stat(self, f): timer = Timer() i = 0 while True: f() i += 1 if i >= self.min_run: _, elapsed = timer.lap() if elapsed > self.min_time: break return BenchmarkStat(elapsed / i, i)",2
329,Python,finding time elapsed using a timer,https://github.com/fermiPy/fermipy/blob/9df5e7e3728307fd58c5bba36fd86783c39fbad4/fermipy/timing.py#L14-L21,"def elapsed_time(self): """"""Get the elapsed time."""""" # Timer is running if self._t0 is not None: return self._time + self._get_time() else: return self._time",2
433,Python,finding time elapsed using a timer,https://github.com/DIPSAS/SwarmManagement/blob/c9ef1165b240c145d42e2d363925c8200fc19f43/SwarmManagement/SwarmTools.py#L170-L180,"def TimeoutCounter(secTimeout): startTime = time.time() elapsedTime = time.time() - startTime timeLeft = secTimeout - int(elapsedTime) printedTime = timeLeft while elapsedTime < secTimeout: timeLeft = secTimeout - int(elapsedTime) if timeLeft < printedTime: printedTime = timeLeft print(""Restarting Swarm in %d seconds"" % printedTime) elapsedTime = time.time() - startTime",2
510,Python,finding time elapsed using a timer,https://github.com/ianlini/bistiming/blob/46a78ec647723c3516fc4fc73f2619ab41f647f2/examples/stopwatch_examples.py#L107-L119,"def cumulative_elapsed_time_example(): print(""[cumulative_elapsed_time_example] use python logging module with different log level"") timer = Stopwatch(""Waiting"") with timer: sleep(1) sleep(1) with timer: sleep(1) timer.log_elapsed_time(prefix=""timer.log_elapsed_time(): "") # 0:00:01.... print(""timer.get_elapsed_time():"", timer.get_elapsed_time()) # 0:00:01.... print(""timer.split_elapsed_time:"", timer.split_elapsed_time) # [datetime.timedelta(seconds=1), datetime.timedelta(seconds=1)] print(""timer.get_cumulative_elapsed_time():"", timer.get_cumulative_elapsed_time()) # 0:00:02....",2
774,Python,finding time elapsed using a timer,https://github.com/andrewramsay/sk8-drivers/blob/67347a71762fb421f5ae65a595def5c7879e8b0c/pysk8/calibration/sk8_calibration_gui.py#L49-L57,"def update(self): # on every timer tick, record a gyro sample and exit if required time has elapsed elapsed = int(100 * ((time.time() - self.started_at) / self.GYRO_BIAS_TIME)) self.progressBar.setValue(elapsed) self.samples.append(self.imu.gyro) if time.time() - self.started_at > self.GYRO_BIAS_TIME: self.accept() QtWidgets.QMessageBox.information(self, 'Gyro calibration', 'Calibration finished')",2
1315,Python,finding time elapsed using a timer,https://github.com/indygreg/python-zstandard/blob/74fa5904c3e7df67a4260344bf919356a181487e/bench.py#L32-L71,"def timer(fn, miniter=3, minwall=3.0): """"""Runs fn() multiple times and returns the results. Runs for at least ``miniter`` iterations and ``minwall`` wall time. """""" results = [] count = 0 # Ideally a monotonic clock, but doesn't matter too much. wall_begin = time.time() while True: wstart = time.time() start = os.times() fn() end = os.times() wend = time.time() count += 1 user = end[0] - start[0] system = end[1] - start[1] cpu = user + system wall = wend - wstart results.append((cpu, user, system, wall)) # Ensure we run at least ``miniter`` times. if count < miniter: continue # And for ``minwall`` seconds. elapsed = wend - wall_begin if elapsed < minwall: continue break return results",2
1108,Python,finding time elapsed using a timer,https://github.com/buildbot/buildbot/blob/5df3cfae6d760557d99156633c32b1822a1e130c/master/buildbot/process/metrics.py#L74-L76,"def __init__(self, timer, elapsed): self.timer = timer self.elapsed = elapsed",1
1236,Python,find int in string,https://github.com/fdb/aufmachen/blob/f2986a0cf087ac53969f82b84d872e3f1c6986f4/aufmachen/websites/immoweb.py#L115-L130,"def find_number(regex, s): """"""Find a number using a given regular expression. If the string cannot be found, returns None. The regex should contain one matching group, as only the result of the first group is returned. The group should only contain numeric characters ([0-9]+). s - The string to search. regex - A string containing the regular expression. Returns an integer or None. """""" result = find_string(regex, s) if result is None: return None return int(result)",3
278,Python,find int in string,https://github.com/acutesoftware/AIKIF/blob/fcf1582dc5f884b9a4fa7c6e20e9de9d94d21d03/aikif/core_data.py#L311-L318,"def find(self, txt): result = [] for e in self.table: print('find(self, txt) e = ', e) if txt in str(e): result.append(e) #print(e) return result",1
659,Python,find int in string,https://github.com/nickolas360/librecaptcha/blob/bd247082fd98118be220f326e5cd162e6d926655/librecaptcha/recaptcha.py#L354-L380,"def find_challenge_goal(self, id, raw=False): start = 0 matching_strings = [] def try_find(): nonlocal start index = self.js_strings.index(id, start) for i in range(FIND_GOAL_SEARCH_DISTANCE): next_str = self.js_strings[index + i + 1] if re.search(r""\bselect all\b"", next_str, re.I): matching_strings.append((i, index, next_str)) start = index + FIND_GOAL_SEARCH_DISTANCE + 1 try: while True: try_find() except (ValueError, IndexError): pass try: goal = min(matching_strings)[2] except ValueError: return None, None raw = goal plain = raw.replace(""<strong>"", """").replace(""</strong>"", """") return raw, plain",1
898,Python,find int in string,https://github.com/klahnakoski/mo-logs/blob/0971277ac9caf28a755b766b70621916957d4fea/mo_logs/strings.py#L547-L554,"def find_first(value, find_arr, start=0): i = len(value) for f in find_arr: temp = value.find(f, start) if temp == -1: continue i = min(i, temp) if i == len(value): return -1 return i",1
955,Python,find int in string,https://github.com/klahnakoski/mo-logs/blob/0971277ac9caf28a755b766b70621916957d4fea/mo_logs/strings.py#L294-L315,"def find(value, find, start=0): """""" Return index of `find` in `value` beginning at `start` :param value: :param find: :param start: :return: If NOT found, return the length of `value` string """""" l = len(value) if is_list(find): m = l for f in find: i = value.find(f, start) if i == -1: continue m = min(m, i) return m else: i = value.find(find, start) if i == -1: return l return i",1
1981,Python,find int in string,https://github.com/hubo1016/vlcp/blob/239055229ec93a99cc7e15208075724ccf543bd1/vlcp/service/sdn/plugins/networkvxlandriver.py#L217-L233,"def _isavaliablevni(vnirange,allocated,vni): find = False for start,end in vnirange: if start <= int(vni) <= end: find = True break if find: if str(vni) not in allocated: find = True else: find = False else: find = False return find",1
135,Python,find int in string,https://github.com/lionheart/django-pyodbc/blob/46adda7b0bfabfa2640f72592c6f6f407f78b363/django_pyodbc/compiler.py#L107-L111,"def _break(s, find): """"""Break a string s into the part before the substring to find, and the part including and after the substring."""""" i = s.find(find) return s[:i], s[i:]",0
262,Python,find int in string,https://github.com/enkore/i3pystatus/blob/14cfde967cecf79b40e223e35a04600f4c875af7/i3pystatus/core/util.py#L80-L84,"def get(self, find_id): find_id = int(find_id) for module in self: if id(module) == find_id: return module",0
293,Python,find int in string,https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_latex.py#L890-L896,"def is_substr(find, strlist): if len(strlist) < 1 and len(find) < 1: return False for i in range(len(strlist)): if find not in strlist[i]: return False return True",0
462,Python,find int in string,https://github.com/mlperf/training/blob/1c6ae725a81d15437a2b2df05cac0673fde5c3a4/single_stage_detector/ssd/utils.py#L655-L689,"def _parse_xml(self, xml_file): # print(xml_file) root = ElementTree.ElementTree(file=xml_file) img_name = root.find(""filename"").text # Get basename base_name = Path(img_name).resolve().stem if base_name not in self.filter: return [] img_size = ( int(root.find(""size"").find(""height"").text), int(root.find(""size"").find(""width"").text), int(root.find(""size"").find(""depth"").text),) tmp_data = [] for obj in root.findall(""object""): # extract xmin, ymin, xmax, ymax difficult = obj.find(""difficult"").text if difficult == ""1"" and not self.difficult: continue bbox = ( int(obj.find(""bndbox"").find(""xmin"").text), int(obj.find(""bndbox"").find(""ymin"").text), int(obj.find(""bndbox"").find(""xmax"").text), int(obj.find(""bndbox"").find(""ymax"").text),) bbox_label = obj.find(""name"").text if bbox_label in self.label_map: bbox_label = self.label_map[bbox_label] else: self.label_num += 1 self.label_map[bbox_label] = self.label_num bbox_label = self.label_num tmp_data.append((bbox, bbox_label)) return (img_name, img_size, tmp_data)",0
1676,Python,filter array,https://github.com/brechtm/rinohtype/blob/40a63c4e5ad7550f62b6860f1812cb67cafb9dc7/src/rinoh/backend/pdf/filter.py#L459-L463,def params(self): if not any(filter.params for filter in self): return None else: return Array(filter.params or Null() for filter in self),3
526,Python,filter array,https://github.com/PiotrDabkowski/Js2Py/blob/c0fa43f5679cf91ca8986c5747fcb07a433dc584/js2py/internals/prototypes/jsarray.py#L382-L398,"def filter(this, args): array = to_object(this, args.space) callbackfn = get_arg(args, 0) arr_len = js_arr_length(array) if not is_callable(callbackfn): raise MakeError('TypeError', 'callbackfn must be a function') _this = get_arg(args, 1) k = 0 res = [] while k < arr_len: if array.has_property(unicode(k)): kValue = array.get(unicode(k)) if to_boolean( callbackfn.call(_this, (kValue, float(k), array))): res.append(kValue) k += 1 return args.space.ConstructArray(res)",2
1199,Python,filter array,https://github.com/ccxt/ccxt/blob/23062efd7a5892c79b370c9d951c03cf8c0ddf23/python/ccxt/base/exchange.py#L692-L693,"def filterBy(self, array, key, value=None): return Exchange.filter_by(array, key, value)",2
1252,Python,filter array,https://github.com/jim-easterbrook/python-gphoto2/blob/345e0f4457442ecd1aa4dc2ce8f8272c0910f91e/examples/cam-conf-view-gui.py#L265-L274,"def get_json_filters(args): jsonfilters = [] if hasattr(args, 'include_names_json'): splitnames = args.include_names_json.split("","") for iname in splitnames: splitnameeq = iname.split(""="") # 1-element array if = not present; 2-element array if present splitnameeq = list(filter(None, splitnameeq)) jsonfilters.append(splitnameeq) jsonfilters = list(filter(None, jsonfilters)) return jsonfilters",2
37,Python,filter array,https://github.com/kwikteam/phy/blob/7e9313dc364304b7d2bd03b92938347343703003/phy/traces/filter.py#L28-L34,"def apply_filter(x, filter=None, axis=0): """"""Apply a filter to an array."""""" x = _as_array(x) if x.shape[axis] == 0: return x b, a = filter return signal.filtfilt(b, a, x, axis=axis)",1
39,Python,filter array,https://github.com/jopohl/urh/blob/2eb33b125c8407964cd1092843cde5010eb88aae/src/urh/controller/widgets/SignalFrame.py#L31-L33,"def perform_filter(result_array: Array, data, f_low, f_high, filter_bw): result_array = np.frombuffer(result_array.get_obj(), dtype=np.complex64) result_array[:] = Filter.apply_bandpass_filter(data, f_low, f_high, filter_bw=filter_bw)",1
1160,Python,filter array,https://github.com/tango-controls/pytango/blob/9cf78c517c9cdc1081ff6d080a9646a740cc1d36/tango/device_proxy.py#L704-L744,"def __DeviceProxy__get_property_list(self, filter, array=None): """""" get_property_list(self, filter, array=None) -> obj Get the list of property names for the device. The parameter filter allows the user to filter the returned name list. The wildcard character is '*'. Only one wildcard character is allowed in the filter parameter. Parameters : - filter[in] : (str) the filter wildcard - array[out] : (sequence obj or None) (optional, default is None) an array to be filled with the property names. If None a new list will be created internally with the values. Return : the given array filled with the property names (or a new list if array is None) Throws : NonDbDevice, WrongNameSyntax, ConnectionFailed (with database), CommunicationFailed (with database) DevFailed from database device New in PyTango 7.0.0 """""" if array is None: new_array = StdStringVector() self._get_property_list(filter, new_array) return new_array if isinstance(array, StdStringVector): self._get_property_list(filter, array) return array elif isinstance(array, collections_abc.Sequence): new_array = StdStringVector() self._get_property_list(filter, new_array) StdStringVector_2_seq(new_array, array) return array raise TypeError('array must be a mutable sequence<string>')",1
417,Python,filter array,https://github.com/Yipit/pyeqs/blob/2e385c0a5d113af0e20be4d9393add2aabdd9565/pyeqs/query_builder.py#L41-L50,"def _build_filtered_query(self, f, operator): """""" Create the root of the filter tree """""" self._filtered = True if isinstance(f, Filter): filter_object = f else: filter_object = Filter(operator).filter(f) self._filter_dsl = filter_object",0
461,Python,filter array,https://github.com/odlgroup/odl/blob/b8443f6aca90e191ba36c91d32253c5a36249a6c/odl/tomo/analytic/filtered_back_projection.py#L313-L474,"def fbp_filter_op(ray_trafo, padding=True, filter_type='Ram-Lak', frequency_scaling=1.0): """"""Create a filter operator for FBP from a `RayTransform`. Parameters ---------- ray_trafo : `RayTransform` The ray transform (forward operator) whose approximate inverse should be computed. Its geometry has to be any of the following `Parallel2dGeometry` : Exact reconstruction `Parallel3dAxisGeometry` : Exact reconstruction `FanBeamGeometry` : Approximate reconstruction, correct in limit of fan angle = 0. Only flat detectors are supported (det_curvature_radius is None). `ConeFlatGeometry`, pitch = 0 (circular) : Approximate reconstruction, correct in the limit of fan angle = 0 and cone angle = 0. `ConeFlatGeometry`, pitch > 0 (helical) : Very approximate unless a `tam_danielson_window` is used. Accurate with the window. Other geometries: Not supported padding : bool, optional If the data space should be zero padded. Without padding, the data may be corrupted due to the circular convolution used. Using padding makes the algorithm slower. filter_type : optional The type of filter to be used. The predefined options are, in approximate order from most noise senstive to least noise sensitive: ``'Ram-Lak'``, ``'Shepp-Logan'``, ``'Cosine'``, ``'Hamming'`` and ``'Hann'``. A callable can also be provided. It must take an array of values in [0, 1] and return the filter for these frequencies. frequency_scaling : float, optional Relative cutoff frequency for the filter. The normalized frequencies are rescaled so that they fit into the range [0, frequency_scaling]. Any frequency above ``frequency_scaling`` is set to zero. Returns ------- filter_op : `Operator` Filtering operator for FBP based on ``ray_trafo``. See Also -------- tam_danielson_window : Windowing for helical data """""" impl = 'pyfftw' if PYFFTW_AVAILABLE else 'numpy' alen = ray_trafo.geometry.motion_params.length if ray_trafo.domain.ndim == 2: # Define ramp filter def fourier_filter(x): abs_freq = np.abs(x[1]) norm_freq = abs_freq / np.max(abs_freq) filt = _fbp_filter(norm_freq, filter_type, frequency_scaling) scaling = 1 / (2 * alen) return filt * np.max(abs_freq) * scaling # Define (padded) fourier transform if padding: # Define padding operator ran_shp = (ray_trafo.range.shape[0], ray_trafo.range.shape[1] * 2 - 1) resizing = ResizingOperator(ray_trafo.range, ran_shp=ran_shp) fourier = FourierTransform(resizing.range, axes=1, impl=impl) fourier = fourier * resizing else: fourier = FourierTransform(ray_trafo.range, axes=1, impl=impl) elif ray_trafo.domain.ndim == 3: # Find the direction that the filter should be taken in rot_dir = _rotation_direction_in_detector(ray_trafo.geometry) # Find what axes should be used in the fourier transform used_axes = (rot_dir != 0) if used_axes[0] and not used_axes[1]: axes = [1] elif not used_axes[0] and used_axes[1]: axes = [2] else: axes = [1, 2] # Add scaling for cone-beam case if hasattr(ray_trafo.geometry, 'src_radius'): scale = (ray_trafo.geometry.src_radius / (ray_trafo.geometry.src_radius + ray_trafo.geometry.det_radius)) if ray_trafo.geometry.pitch != 0: # In helical geometry the whole volume is not in each # projection and we need to use another weighting. # Ideally each point in the volume effects only # the projections in a half rotation, so we assume that that # is the case. scale *= alen / (np.pi) else: scale = 1.0 # Define ramp filter def fourier_filter(x): # If axis is aligned to a coordinate axis, save some memory and # time by using broadcasting if not used_axes[0]: abs_freq = np.abs(rot_dir[1] * x[2]) elif not used_axes[1]: abs_freq = np.abs(rot_dir[0] * x[1]) else: abs_freq = np.abs(rot_dir[0] * x[1] + rot_dir[1] * x[2]) norm_freq = abs_freq / np.max(abs_freq) filt = _fbp_filter(norm_freq, filter_type, frequency_scaling) scaling = scale * np.max(abs_freq) / (2 * alen) return filt * scaling # Define (padded) fourier transform if padding: # Define padding operator if used_axes[0]: padded_shape_u = ray_trafo.range.shape[1] * 2 - 1 else: padded_shape_u = ray_trafo.range.shape[1] if used_axes[1]: padded_shape_v = ray_trafo.range.shape[2] * 2 - 1 else: padded_shape_v = ray_trafo.range.shape[2] ran_shp = (ray_trafo.range.shape[0], padded_shape_u, padded_shape_v) resizing = ResizingOperator(ray_trafo.range, ran_shp=ran_shp) fourier = FourierTransform(resizing.range, axes=axes, impl=impl) fourier = fourier * resizing else: fourier = FourierTransform(ray_trafo.range, axes=axes, impl=impl) else: raise NotImplementedError('FBP only implemented in 2d and 3d') # Create ramp in the detector direction ramp_function = fourier.range.element(fourier_filter) weight = 1 if not ray_trafo.range.is_weighted: # Compensate for potentially unweighted range of the ray transform weight *= ray_trafo.range.cell_volume if not ray_trafo.domain.is_weighted: # Compensate for potentially unweighted domain of the ray transform weight /= ray_trafo.domain.cell_volume ramp_function *= weight # Create ramp filter via the convolution formula with fourier transforms return fourier.inverse * ramp_function * fourier",0
607,Python,filter array,https://github.com/jim-easterbrook/pyctools/blob/2a958665326892f45f249bebe62c2c23f306732b/src/pyctools/components/interp/resize.py#L77-L95,"def get_filter(self): new_filter = self.input_buffer['filter'].peek() if not new_filter: return False if new_filter == self.filter_frame: return True self.send('filter', new_filter) filter_coefs = new_filter.as_numpy(dtype=numpy.float32) if filter_coefs.ndim != 3: self.logger.warning('Filter input must be 3 dimensional') return False ylen, xlen = filter_coefs.shape[:2] if (xlen % 2) != 1 or (ylen % 2) != 1: self.logger.warning('Filter input must have odd width & height') return False self.filter_frame = new_filter self.filter_coefs = filter_coefs self.fil_count = None return True",0
513,Python,extracting data from a text file,https://github.com/joshburnett/scanf/blob/52f8911581c1590a3dcc6f17594eeb7b39716d42/scanf.py#L158-L184,"def extractdata(pattern, text=None, filepath=None): """""" Read through an entire file or body of text one line at a time. Parse each line that matches the supplied pattern string and ignore the rest. If *text* is supplied, it will be parsed according to the *pattern* string. If *text* is not supplied, the file at *filepath* will be opened and parsed. """""" y = [] if text is None: textsource = open(filepath, 'r') else: textsource = text.splitlines() for line in textsource: match = scanf(pattern, line) if match: if len(y) == 0: y = [[s] for s in match] else: for i, ydata in enumerate(y): ydata.append(match[i]) if text is None: textsource.close() return y",3
1041,Python,extracting data from a text file,https://github.com/google/mobly/blob/38ba2cf7d29a20e6a2fca1718eecb337df38db26/mobly/controllers/monsoon.py#L562-L578,"def from_text_file(file_path): """"""Load MonsoonData objects from a text file generated by MonsoonData.save_to_text_file. Args: file_path: The full path of the file load from, including the file name. Returns: A list of MonsoonData objects. """""" results = [] with io.open(file_path, 'r', encoding='utf-8') as f: data_strs = f.read().split(MonsoonData.delimiter) for data_str in data_strs: results.append(MonsoonData.from_string(data_str)) return results",3
1980,Python,extracting data from a text file,https://github.com/log2timeline/plaso/blob/9c564698d2da3ffbe23607a3c54c0582ea18a6cc/plaso/engine/worker.py#L338-L374,"def _ExtractMetadataFromFileEntry(self, mediator, file_entry, data_stream): """"""Extracts metadata from a file entry. Args: mediator (ParserMediator): mediates the interactions between parsers and other components, such as storage and abort signals. file_entry (dfvfs.FileEntry): file entry to extract metadata from. data_stream (dfvfs.DataStream): data stream or None if the file entry has no data stream. """""" # Do not extract metadata from the root file entry when it is virtual. if file_entry.IsRoot() and file_entry.type_indicator not in ( self._TYPES_WITH_ROOT_METADATA): return # We always want to extract the file entry metadata but we only want # to parse it once per file entry, so we only use it if we are # processing the default data stream of regular files. if data_stream and not data_stream.IsDefault(): return display_name = mediator.GetDisplayName() logger.debug( '[ExtractMetadataFromFileEntry] processing file entry: {0:s}'.format( display_name)) self.processing_status = definitions.STATUS_INDICATOR_EXTRACTING if self._processing_profiler: self._processing_profiler.StartTiming('extracting') self._event_extractor.ParseFileEntryMetadata(mediator, file_entry) if self._processing_profiler: self._processing_profiler.StopTiming('extracting') self.processing_status = definitions.STATUS_INDICATOR_RUNNING",3
703,Python,extracting data from a text file,https://github.com/openmednlp/bedrock/blob/15796bd7837ddfe7ad5235fd188e5d7af8c0be49/bedrock/collection.py#L74-L79,"def extract_impression_from_file(file_path): # TODO: domain specific, move to propper location with open(file_path, ""r"", encoding='utf8') as f: text = f.read() return extract_impression_from_text(text)",2
829,Python,extracting data from a text file,https://github.com/explosion/spaCy/blob/8ee4100f8ffb336886208a1ea827bf4c745e2709/spacy/cli/profile.py#L55-L69,"def _read_inputs(loc, msg): if loc == ""-"": msg.info(""Reading input from sys.stdin"") file_ = sys.stdin file_ = (line.encode(""utf8"") for line in file_) else: input_path = Path(loc) if not input_path.exists() or not input_path.is_file(): msg.fail(""Not a valid input data file"", loc, exits=1) msg.info(""Using data from {}"".format(input_path.parts[-1])) file_ = input_path.open() for line in file_: data = srsly.json_loads(line) text = data[""text""] yield text",2
958,Python,extracting data from a text file,https://github.com/wtsi-hgi/python-common/blob/0376a6b574ff46e82e509e90b6cb3693a3dbb577/hgicommon/data_source/static_from_file.py#L72-L82,"def no_error_extract_data_from_file(self, file_path: str) -> Iterable[DataSourceType]: """""" Proxy for `extract_data_from_file` that suppresses any errors and instead just returning an empty list. :param file_path: see `extract_data_from_file` :return: see `extract_data_from_file` """""" try: return self.extract_data_from_file(file_path) except Exception as e: logging.warning(e) return []",2
1579,Python,extracting data from a text file,https://github.com/bakwc/JamSpell/blob/bfdfd889436df4dbcd9ec86b20d2baeb815068bd/evaluate/utils.py#L25-L28,"def loadText(fname): with codecs.open(fname, 'r', 'utf-8') as f: data = f.read() return normalize(data).split()",2
1790,Python,extracting data from a text file,https://github.com/lcharleux/argiope/blob/8170e431362dc760589f7d141090fd133dece259/argiope/abq/pypostproc.py#L23-L49,"def read_field_report(path, data_flag = ""*DATA"", meta_data_flag = ""*METADATA""): """""" Reads a field output report. """""" text = open(path).read() mdpos = text.find(meta_data_flag) dpos = text.find(data_flag) mdata = io.StringIO( ""\n"".join(text[mdpos:dpos].split(""\n"")[1:])) data = io.StringIO( ""\n"".join(text[dpos:].split(""\n"")[1:])) data = pd.read_csv(data, index_col = 0) data = data.groupby(data.index).mean() mdata = pd.read_csv(mdata, sep = ""="", header = None, index_col = 0)[1] mdata = mdata.to_dict() out = {} out[""step_num""] = int(mdata[""step_num""]) out[""step_label""] = mdata[""step_label""] out[""frame""] = int(mdata[""frame""]) out[""frame_value""] = float(mdata[""frame_value""]) out[""part""] = mdata[""instance""] position_map = {""NODAL"": ""node"", ""ELEMENT_CENTROID"": ""element"", ""WHOLE_ELEMENT"": ""element""} out[""position""] = position_map[mdata[""position""]] out[""label""] = mdata[""label""] out[""data""] = data field_class = getattr(argiope.mesh, mdata[""argiope_class""]) return field_class(**out)",2
1986,Python,extracting data from a text file,https://github.com/santoshphilip/eppy/blob/55410ff7c11722f35bc4331ff5e00a0b86f787e1/eppy/EPlusInterfaceFunctions/parse_idd.py#L142-L385,"def extractidddata(fname, debug=False): """""" extracts all the needed information out of the idd file if debug is True, it generates a series of text files. Each text file is incrementally different. You can do a diff see what the change is - this code is from 2004. it works. I am trying not to change it (until I rewrite the whole thing) to add functionality to it, I am using decorators So if Does not integrate group data into the results (@embedgroupdata does it) Does not integrate iddindex into the results (@make_idd_index does it) """""" try: if isinstance(fname, (file, StringIO)): astr = fname.read() try: astr = astr.decode('ISO-8859-2') except AttributeError: pass else: astr = mylib2.readfile(fname) # astr = astr.decode('ISO-8859-2') -> mylib1 does a decode except NameError: if isinstance(fname, (FileIO, StringIO)): astr = fname.read() try: astr = astr.decode('ISO-8859-2') except AttributeError: pass else: astr = mylib2.readfile(fname) # astr = astr.decode('ISO-8859-2') -> mylib2.readfile has decoded (nocom, nocom1, blocklst) = get_nocom_vars(astr) astr = nocom st1 = removeblanklines(astr) if debug: mylib1.write_str2file('nocom2.txt', st1.encode('latin-1')) #find the groups and the start object of the group #find all the group strings groupls = [] alist = st1.splitlines() for element in alist: lss = element.split() if lss[0].upper() == '\\group'.upper(): groupls.append(element) #find the var just after each item in groupls groupstart = [] for i in range(len(groupls)): iindex = alist.index(groupls[i]) groupstart.append([alist[iindex], alist[iindex+1]]) #remove the group commentline for element in groupls: alist.remove(element) if debug: st1 = '\n'.join(alist) mylib1.write_str2file('nocom3.txt', st1.encode('latin-1')) #strip each line for i in range(len(alist)): alist[i] = alist[i].strip() if debug: st1 = '\n'.join(alist) mylib1.write_str2file('nocom4.txt', st1.encode('latin-1')) #ensure that each line is a comment or variable #find lines that don't start with a comment #if this line has a comment in it # then move the comment to a new line below lss = [] for i in range(len(alist)): #find lines that don't start with a comment if alist[i][0] != '\\': #if this line has a comment in it pnt = alist[i].find('\\') if pnt != -1: #then move the comment to a new line below lss.append(alist[i][:pnt].strip()) lss.append(alist[i][pnt:].strip()) else: lss.append(alist[i]) else: lss.append(alist[i]) alist = lss[:] if debug: st1 = '\n'.join(alist) mylib1.write_str2file('nocom5.txt', st1.encode('latin-1')) #need to make sure that each line has only one variable - as in WindowGlassSpectralData, lss = [] for element in alist: # if the line is not a comment if element[0] != '\\': #test for more than one var llist = element.split(',') if llist[-1] == '': tmp = llist.pop() for elm in llist: if elm[-1] == ';': lss.append(elm.strip()) else: lss.append((elm+',').strip()) else: lss.append(element) ls_debug = alist[:] # needed for the next debug - 'nocom7.txt' alist = lss[:] if debug: st1 = '\n'.join(alist) mylib1.write_str2file('nocom6.txt', st1.encode('latin-1')) if debug: #need to make sure that each line has only one variable - as in WindowGlassSpectralData, #this is same as above. # but the variables are put in without the ';' and ',' #so we can do a diff between 'nocom7.txt' and 'nocom8.txt'. Should be identical lss_debug = [] for element in ls_debug: # if the line is not a comment if element[0] != '\\': #test for more than one var llist = element.split(',') if llist[-1] == '': tmp = llist.pop() for elm in llist: if elm[-1] == ';': lss_debug.append(elm[:-1].strip()) else: lss_debug.append((elm).strip()) else: lss_debug.append(element) ls_debug = lss_debug[:] st1 = '\n'.join(ls_debug) mylib1.write_str2file('nocom7.txt', st1.encode('latin-1')) #replace each var with '=====var======' #join into a string, #split using '=====var=====' for i in range(len(lss)): #if the line is not a comment if lss[i][0] != '\\': lss[i] = '=====var=====' st2 = '\n'.join(lss) lss = st2.split('=====var=====\n') lss.pop(0) # the above split generates an extra item at start if debug: fname = 'nocom8.txt' fhandle = open(fname, 'wb') k = 0 for i in range(len(blocklst)): for j in range(len(blocklst[i])): atxt = blocklst[i][j]+'\n' fhandle.write(atxt) atxt = lss[k] fhandle.write(atxt.encode('latin-1')) k = k+1 fhandle.close() #map the structure of the comments -(this is 'lss' now) to #the structure of blocklst - blocklst is a nested list #make lss a similar nested list k = 0 lst = [] for i in range(len(blocklst)): lst.append([]) for j in range(len(blocklst[i])): lst[i].append(lss[k]) k = k+1 if debug: fname = 'nocom9.txt' fhandle = open(fname, 'wb') k = 0 for i in range(len(blocklst)): for j in range(len(blocklst[i])): atxt = blocklst[i][j]+'\n' fhandle.write(atxt) fhandle.write(lst[i][j].encode('latin-1')) k = k+1 fhandle.close() #break up multiple line comment so that it is a list for i in range(len(lst)): for j in range(len(lst[i])): lst[i][j] = lst[i][j].splitlines() # remove the '\' for k in range(len(lst[i][j])): lst[i][j][k] = lst[i][j][k][1:] commlst = lst #copied with minor modifications from readidd2_2.py -- which has been erased ha ! clist = lst lss = [] for i in range(0, len(clist)): alist = [] for j in range(0, len(clist[i])): itt = clist[i][j] ddtt = {} for element in itt: if len(element.split()) == 0: break ddtt[element.split()[0].lower()] = [] for element in itt: if len(element.split()) == 0: break # ddtt[element.split()[0].lower()].append(string.join(element.split()[1:])) ddtt[element.split()[0].lower()].append(' '.join(element.split()[1:])) alist.append(ddtt) lss.append(alist) commdct = lss # add group information to commlst and commdct # glist = iddgroups.idd2grouplist(fname) # commlst = group2commlst(commlst, glist) # commdct = group2commdct(commdct, glist) return blocklst, commlst, commdct # give blocklst a better name :-(",2
31,Python,extracting data from a text file,https://github.com/iopipe/iopipe-python/blob/4eb653977341bc67f8b1b87aedb3aaaefc25af61/iopipe/report.py#L82-L117,"def extract_context_data(self): """""" Returns the contents of a AWS Lambda context. :returns: A dict of relevant context data. :rtype: dict """""" data = {} for k, v in { # camel case names in the report to align with AWS standards ""functionName"": ""function_name"", ""functionVersion"": ""function_version"", ""memoryLimitInMB"": ""memory_limit_in_mb"", ""invokedFunctionArn"": ""invoked_function_arn"", ""awsRequestId"": ""aws_request_id"", ""logGroupName"": ""log_group_name"", ""logStreamName"": ""log_stream_name"", }.items(): if hasattr(self.context, v): data[k] = getattr(self.context, v) if ( hasattr(self.context, ""invoked_function_arn"") and ""AWS_SAM_LOCAL"" in os.environ ): data[""invokedFunctionArn""] = ( ""arn:aws:lambda:local:0:function:%s"" % data.get(""functionName"", ""unknown"") ) if hasattr(self.context, ""get_remaining_time_in_millis"") and callable( self.context.get_remaining_time_in_millis ): data[ ""getRemainingTimeInMillis"" ] = self.context.get_remaining_time_in_millis() data[""traceId""] = os.getenv(""_X_AMZN_TRACE_ID"", """") return data",0
769,Python,extract latitude and longitude from given input,https://github.com/Turbo87/aerofiles/blob/d8b7b04a1fcea5c98f89500de1164619a4ec7ef4/aerofiles/seeyou/reader.py#L130-L143,"def decode_longitude(self, longitude): match = RE_LONGITUDE.match(longitude) if not match: raise ParserError('Reading longitude failed') longitude = int(match.group(1)) + float(match.group(2)) / 60. if not (0 <= longitude <= 180): raise ParserError('Longitude out of bounds') if match.group(3).upper() == 'W': longitude = -longitude return longitude",3
791,Python,extract latitude and longitude from given input,https://github.com/geometalab/pyGeoTile/blob/b1f44271698f5fc4d18c2add935797ed43254aa6/pygeotile/point.py#L12-L16,"def from_latitude_longitude(cls, latitude=0.0, longitude=0.0): """"""Creates a point from lat/lon in WGS84"""""" assert -180.0 <= longitude <= 180.0, 'Longitude needs to be a value between -180.0 and 180.0.' assert -90.0 <= latitude <= 90.0, 'Latitude needs to be a value between -90.0 and 90.0.' return cls(latitude=latitude, longitude=longitude)",3
478,Python,extract latitude and longitude from given input,https://github.com/Turbo87/utm/blob/efdd46ab0a341ce2aa45f8144d8b05a4fa0fd592/utm/conversion.py#L171-L246,"def from_latlon(latitude, longitude, force_zone_number=None, force_zone_letter=None): """"""This function convert Latitude and Longitude to UTM coordinate Parameters ---------- latitude: float Latitude between 80 deg S and 84 deg N, e.g. (-80.0 to 84.0) longitude: float Longitude between 180 deg W and 180 deg E, e.g. (-180.0 to 180.0). force_zone number: int Zone Number is represented with global map numbers of an UTM Zone Numbers Map. You may force conversion including one UTM Zone Number. More information see utmzones [1]_ .. _[1]: http://www.jaworski.ca/utmzones.htm """""" if not in_bounds(latitude, -80.0, 84.0): raise OutOfRangeError('latitude out of range (must be between 80 deg S and 84 deg N)') if not in_bounds(longitude, -180.0, 180.0): raise OutOfRangeError('longitude out of range (must be between 180 deg W and 180 deg E)') if force_zone_number is not None: check_valid_zone(force_zone_number, force_zone_letter) lat_rad = mathlib.radians(latitude) lat_sin = mathlib.sin(lat_rad) lat_cos = mathlib.cos(lat_rad) lat_tan = lat_sin / lat_cos lat_tan2 = lat_tan * lat_tan lat_tan4 = lat_tan2 * lat_tan2 if force_zone_number is None: zone_number = latlon_to_zone_number(latitude, longitude) else: zone_number = force_zone_number if force_zone_letter is None: zone_letter = latitude_to_zone_letter(latitude) else: zone_letter = force_zone_letter lon_rad = mathlib.radians(longitude) central_lon = zone_number_to_central_longitude(zone_number) central_lon_rad = mathlib.radians(central_lon) n = R / mathlib.sqrt(1 - E * lat_sin**2) c = E_P2 * lat_cos**2 a = lat_cos * (lon_rad - central_lon_rad) a2 = a * a a3 = a2 * a a4 = a3 * a a5 = a4 * a a6 = a5 * a m = R * (M1 * lat_rad - M2 * mathlib.sin(2 * lat_rad) + M3 * mathlib.sin(4 * lat_rad) - M4 * mathlib.sin(6 * lat_rad)) easting = K0 * n * (a + a3 / 6 * (1 - lat_tan2 + c) + a5 / 120 * (5 - 18 * lat_tan2 + lat_tan4 + 72 * c - 58 * E_P2)) + 500000 northing = K0 * (m + n * lat_tan * (a2 / 2 + a4 / 24 * (5 - lat_tan2 + 9 * c + 4 * c**2) + a6 / 720 * (61 - 58 * lat_tan2 + lat_tan4 + 600 * c - 330 * E_P2))) if mixed_signs(latitude): raise ValueError(""latitudes must all have the same sign"") elif negative(latitude): northing += 10000000 return easting, northing, zone_number, zone_letter",2
1595,Python,extract latitude and longitude from given input,https://github.com/nickmckay/LiPD-utilities/blob/5dab6bbeffc5effd68e3a6beaca6b76aa928e860/Python/lipd/excel.py#L1227-L1269,"def compile_geometry(lat, lon, elev): """""" Take in lists of lat and lon coordinates, and determine what geometry to create :param list lat: Latitude values :param list lon: Longitude values :param float elev: Elevation value :return dict: """""" logger_excel.info(""enter compile_geometry"") lat = _remove_geo_placeholders(lat) lon = _remove_geo_placeholders(lon) # 4 coordinate values if len(lat) == 2 and len(lon) == 2: logger_excel.info(""found 4 coordinates"") geo_dict = geometry_linestring(lat, lon, elev) # # 4 coordinate values # if (lat[0] != lat[1]) and (lon[0] != lon[1]): # geo_dict = geometry_polygon(lat, lon) # # 3 unique coordinates # else: # geo_dict = geometry_multipoint(lat, lon) # # 2 coordinate values elif len(lat) == 1 and len(lon) == 1: logger_excel.info(""found 2 coordinates"") geo_dict = geometry_point(lat, lon, elev) # coordinate range. one value given but not the other. elif (None in lon and None not in lat) or (len(lat) > 0 and len(lon) == 0): geo_dict = geometry_range(lat, elev, ""lat"") elif (None in lat and None not in lon) or (len(lon) > 0 and len(lat) == 0): geo_dict = geometry_range(lat, elev, ""lon"") # Too many points, or no points else: geo_dict = {} logger_excel.warn(""compile_geometry: invalid coordinates: lat: {}, lon: {}"".format(lat, lon)) logger_excel.info(""exit compile_geometry"") return geo_dict",2
1935,Python,extract latitude and longitude from given input,https://github.com/City-of-Helsinki/django-munigeo/blob/a358f0dd0eae3add660b650abec2acb4dd86d04c/munigeo/api.py#L222-L241,"def parse_lat_lon(query_params): lat = query_params.get('lat', None) lon = query_params.get('lon', None) if not lat and not lon: return None if not lat or not lon: raise ParseError(""you must supply both 'lat' and 'lon'"") try: lat = float(lat) lon = float(lon) except ValueError: raise ParseError(""'lat' and 'lon' must be floating point numbers"") point = Point(lon, lat, srid=DEFAULT_SRID) if DEFAULT_SRID != DATABASE_SRID: ct = CoordTransform(SpatialReference(DEFAULT_SRID), SpatialReference(DATABASE_SRID)) point.transform(ct) return point",2
19,Python,extract latitude and longitude from given input,https://github.com/Unidata/MetPy/blob/16f68a94919b9a82dcf9cada2169cf039129e67b/metpy/calc/tools.py#L819-L870,"def lat_lon_grid_deltas(longitude, latitude, **kwargs): r""""""Calculate the delta between grid points that are in a latitude/longitude format. Calculate the signed delta distance between grid points when the grid spacing is defined by delta lat/lon rather than delta x/y Parameters ---------- longitude : array_like array of longitudes defining the grid latitude : array_like array of latitudes defining the grid kwargs Other keyword arguments to pass to :class:`~pyproj.Geod` Returns ------- dx, dy: at least two dimensional arrays of signed deltas between grid points in the x and y direction Notes ----- Accepts 1D, 2D, or higher arrays for latitude and longitude Assumes [..., Y, X] for >=2 dimensional arrays """""" from pyproj import Geod # Inputs must be the same number of dimensions if latitude.ndim != longitude.ndim: raise ValueError('Latitude and longitude must have the same number of dimensions.') # If we were given 1D arrays, make a mesh grid if latitude.ndim < 2: longitude, latitude = np.meshgrid(longitude, latitude) geod_args = {'ellps': 'sphere'} if kwargs: geod_args = kwargs g = Geod(**geod_args) forward_az, _, dy = g.inv(longitude[..., :-1, :], latitude[..., :-1, :], longitude[..., 1:, :], latitude[..., 1:, :]) dy[(forward_az < -90.) | (forward_az > 90.)] *= -1 forward_az, _, dx = g.inv(longitude[..., :, :-1], latitude[..., :, :-1], longitude[..., :, 1:], latitude[..., :, 1:]) dx[(forward_az < 0.) | (forward_az > 180.)] *= -1 return dx * units.meter, dy * units.meter",1
214,Python,extract latitude and longitude from given input,https://github.com/gem/oq-engine/blob/8294553a0b8aba33fd96437a35065d03547d0040/openquake/hazardlib/valid.py#L437-L446,"def lon_lat(value): """""" :param value: a pair of coordinates :returns: a tuple (longitude, latitude) >>> lon_lat('12 14') (12.0, 14.0) """""" lon, lat = value.split() return longitude(lon), latitude(lat)",1
495,Python,extract latitude and longitude from given input,https://github.com/pvlib/pvlib-python/blob/2e844a595b820b43d1170269781fa66bd0ccc8a3/pvlib/iotools/ecmwf_macc.py#L199-L228,"def get_nearest_indices(self, latitude, longitude): """""" Get nearest indices to (latitude, longitude). Parmaeters ---------- latitude : float Latitude in degrees longitude : float Longitude in degrees Returns ------- idx_lat : int index of nearest latitude idx_lon : int index of nearest longitude """""" # index of nearest latitude idx_lat = int(round((latitude - 90.0) / self.delta_lat)) # avoid out of bounds latitudes if idx_lat < 0: idx_lat = 0 # if latitude == 90, north pole elif idx_lat > self.lat_size: idx_lat = self.lat_size # if latitude == -90, south pole # adjust longitude from -180/180 to 0/360 longitude = longitude % 360.0 # index of nearest longitude idx_lon = int(round(longitude / self.delta_lon)) % self.lon_size return idx_lat, idx_lon",1
1350,Python,extract latitude and longitude from given input,https://github.com/Turbo87/utm/blob/efdd46ab0a341ce2aa45f8144d8b05a4fa0fd592/utm/conversion.py#L261-L283,"def latlon_to_zone_number(latitude, longitude): # If the input is a numpy array, just use the first element # User responsibility to make sure that all points are in one zone if use_numpy: if isinstance(latitude, mathlib.ndarray): latitude = latitude.flat[0] if isinstance(longitude, mathlib.ndarray): longitude = longitude.flat[0] if 56 <= latitude < 64 and 3 <= longitude < 12: return 32 if 72 <= latitude <= 84 and longitude >= 0: if longitude < 9: return 31 elif longitude < 21: return 33 elif longitude < 33: return 35 elif longitude < 42: return 37 return int((longitude + 180) / 6) + 1",1
380,Python,extract data from html content,https://github.com/davgeo/clear/blob/5ec85d27efd28afddfcd4c3f44df17f0115a77aa/clear/epguides.py#L219-L245,"def _ExtractDataFromShowHtml(self, html): """""" Extracts csv show data from epguides html source. Parameters ---------- html : string Block of html text Returns ---------- string Show data extracted from html text in csv format. """""" htmlLines = html.splitlines() for count, line in enumerate(htmlLines): if line.strip() == r'<pre>': startLine = count+1 if line.strip() == r'</pre>': endLine = count try: dataList = htmlLines[startLine:endLine] dataString = '\n'.join(dataList) return dataString.strip() except: raise Exception(""Show content not found - check EPGuides html formatting"")",3
823,Python,extract data from html content,https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/extractors/html_metadata_extractor.py#L40-L90,"def extract(self, html_text: str, extract_title: bool = False, extract_meta: bool = False, extract_microdata: bool = False, microdata_base_url: str = """", extract_json_ld: bool = False, extract_rdfa: bool = False, rdfa_base_url: str = """") \ -> List[Extraction]: """""" Args: html_text (str): input html string to be extracted extract_title (bool): True if string of 'title' tag needs to be extracted, return as { ""title"": ""..."" } extract_meta (bool): True if string of 'meta' tags needs to be extracted, return as { ""meta"": { ""author"": ""..."", ...}} extract_microdata (bool): True if microdata needs to be extracted, returns as { ""microdata"": [...] } microdata_base_url (str): base namespace url for microdata, empty string if no base url is specified extract_json_ld (bool): True if json-ld needs to be extracted, return as { ""json-ld"": [...] } extract_rdfa (bool): True if rdfs needs to be extracted, returns as { ""rdfa"": [...] } rdfa_base_url (str): base namespace url for rdfa, empty string if no base url is specified Returns: List[Extraction]: the list of extraction or the empty list if there are no matches. """""" res = list() soup = BeautifulSoup(html_text, 'html.parser') if soup.title and extract_title: title = self._wrap_data(""title"", soup.title.string.encode('utf-8').decode('utf-8')) res.append(title) if soup.title and extract_meta: meta_content = self._wrap_meta_content(soup.find_all(""meta"")) meta_data = self._wrap_data(""meta"", meta_content) res.append(meta_data) if extract_microdata: mde = MicrodataExtractor() mde_data = self._wrap_data(""microdata"", mde.extract(html_text, microdata_base_url)) res.append(mde_data) if extract_json_ld: jslde = JsonLdExtractor() jslde_data = self._wrap_data(""json-ld"", jslde.extract(html_text)) res.append(jslde_data) if extract_rdfa: rdfae = RDFaExtractor() rdfae_data = self._wrap_data(""rdfa"", rdfae.extract(html_text, rdfa_base_url)) res.append(rdfae_data) return res",3
1290,Python,extract data from html content,https://github.com/usc-isi-i2/etk/blob/aab077c984ea20f5e8ae33af622fe11d3c4df866/etk/cli/html_content_extractor.py#L20-L32,"def run(args): """""" Args: args (argparse.Namespace) """""" html_content_extractor = HTMLContentExtractor() with warnings.catch_warnings(): warnings.simplefilter('ignore') extractions = html_content_extractor.extract(html_text=args.input_file) for e in extractions: print(e.value)",3
1365,Python,extract data from html content,https://github.com/kibitzr/kibitzr/blob/749da312488f1dda1ed1093cf4c95aaac0a604f7/kibitzr/transformer/jinja_transform.py#L81-L88,"def text_filter(html): if isinstance(html, list): html = """".join(html) ok, content = SoupOps.extract_text(html) if ok: return content else: raise RuntimeError(""Extract text failed"")",3
1957,Python,extract data from html content,https://github.com/dragnet-org/dragnet/blob/532c9d9f28e5b1b57f3cabc708218d3863a16322/dragnet/__init__.py#L9-L13,"def extract_content(html, encoding=None, as_blocks=False): if 'content' not in _LOADED_MODELS: _LOADED_MODELS['content'] = load_pickled_model( 'kohlschuetter_readability_weninger_content_model.pkl.gz') return _LOADED_MODELS['content'].extract(html, encoding=encoding, as_blocks=as_blocks)",3
76,Python,extract data from html content,https://github.com/monarch-initiative/dipper/blob/24cc80db355bbe15776edc5c7b41e0886959ba41/scripts/scrape-impc.py#L14-L40,"def parse_item(self, response): key = None if re.search(r'parameterontologies', response.url) and \ response.css('html body div#SiteWrapper div#Content h2 span.procedurekey' '.dark::text').extract() is not None and \ len(response.css('html body div#SiteWrapper div#Content h2 span.procedurekey' '.dark::text').extract()) > 0: key = response.css('html body div#SiteWrapper div#Content h2 span.procedurekey' '.dark::text').extract()[0] elif (re.search(r'parameters', response.url) or re.search(r'protocol', response.url)) and \ response.css('html body div#SiteWrapper div#Content h2 span.procedurekey' '.dark::text').extract() is not None and \ len(response.css('html body div#SiteWrapper div#Content h2 span.procedurekey' '.dark::text').extract()) > 0: key = response.css('html body div#SiteWrapper div#Content h2 span.procedurekey' '.dark::text').extract()[0] elif re.search(r'procedures', response.url) and \ response.css('html body div#SiteWrapper div#Content h2 span.pipelinekey' '::text').extract() is not None and \ len(response.css('html body div#SiteWrapper div#Content h2 span.pipelinekey' '::text').extract()) > 0: key = response.css('html body div#SiteWrapper div#Content h2 span.pipelinekey' '::text').extract()[0] if key is not None: yield { key: response.url }",2
498,Python,extract data from html content,https://github.com/RedHatInsights/insights-core/blob/b57cbf8ed7c089672426ede0441e0a4f789ef4a1/insights/parsers/crontab.py#L116-L156,"def parse_content(self, content): self.data = [] self.environment = {} self.invalid_lines = [] # Crontabs can use 'nicknames' for common event frequencies: nicknames = { '@yearly': '0 0 1 1 *', '@annually': '0 0 1 1 *', '@monthly': '0 0 1 * *', '@weekly': '0 0 * * 0', '@daily': '0 0 * * *', '@hourly': '0 * * * *', } cron_re = re.compile(_make_cron_re(), flags=re.IGNORECASE) env_re = re.compile(r'^\s*(?P<key>\w+)\s*=\s*(?P<value>\S.*)$') for line in get_active_lines(content): if line.startswith('@'): # Reboot is 'special': if line.startswith('@reboot'): parts = line.split(None, 2) self.data.append({'time': '@reboot', 'command': parts[1]}) continue else: parts = line.split(None, 2) if parts[0] not in nicknames: raise ParseException( ""{n} not recognised as a time specification 'nickname'"".format(n=parts[0]) ) # Otherwise, put the time spec nickname translation into # the line line = line.replace(parts[0], nicknames[parts[0]]) # And then we fall through to the rest of the parsing. cron_match = cron_re.match(line) env_match = env_re.match(line) if cron_match: self.data.append(cron_match.groupdict()) elif env_match: # Environment variable - capture in dictionary self.environment[env_match.group('key')] = env_match.group('value') else: self.invalid_lines.append(line)",2
1036,Python,extract data from html content,https://github.com/mailgun/talon/blob/cdd84563dd329c4f887591807870d10015e0c7a7/talon/quotations.py#L207-L216,"def extract_from(msg_body, content_type='text/plain'): try: if content_type == 'text/plain': return extract_from_plain(msg_body) elif content_type == 'text/html': return extract_from_html(msg_body) except Exception: log.exception('ERROR extracting message') return msg_body",2
1376,Python,extract data from html content,https://github.com/tintinweb/pyetherchain/blob/cfeee3944b84fd12842ec3031d1d08ec7d63d33c/pyetherchain/pyetherchain.py#L185-L193,"def get_hardforks(self): rows = self._parse_tbodies(self.session.get(""/hardForks"").text)[0] # only use first tbody result = [] for col in rows: result.append({'name': self._extract_text_from_html( col[0]), 'on_roadmap': True if ""yes"" in col[1].lower() else False, 'date': self._extract_text_from_html(col[2]), 'block': int(self._extract_text_from_html(col[3]))}) return result",2
675,Python,extract data from html content,https://github.com/jurismarches/chopper/blob/53c5489a53e3a5d205a5cb207df751c09633e7ce/chopper/extractor.py#L58-L110,"def extract(self, html_contents, css_contents=None, base_url=None): """""" Extracts the cleaned html tree as a string and only css rules matching the cleaned html tree :param html_contents: The HTML contents to parse :type html_contents: str :param css_contents: The CSS contents to parse :type css_contents: str :param base_url: The base page URL to use for relative to absolute links :type base_url: str :returns: cleaned HTML contents, cleaned CSS contents :rtype: str or tuple """""" # Clean HTML html_extractor = self.html_extractor( html_contents, self._xpaths_to_keep, self._xpaths_to_discard) has_matches = html_extractor.parse() if has_matches: # Relative to absolute URLs if base_url is not None: html_extractor.rel_to_abs(base_url) # Convert ElementTree to string cleaned_html = html_extractor.to_string() else: cleaned_html = None # Clean CSS if css_contents is not None: if cleaned_html is not None: css_extractor = self.css_extractor(css_contents, cleaned_html) css_extractor.parse() # Relative to absolute URLs if base_url is not None: css_extractor.rel_to_abs(base_url) cleaned_css = css_extractor.to_string() else: cleaned_css = None else: return cleaned_html return (cleaned_html, cleaned_css)",0
25,Python,export to excel,https://github.com/gpagliuca/pyfas/blob/5daa1199bd124d315d02bef0ad3888a8f58355b2/build/lib/pyfas/ppl.py#L127-L151,"def to_excel(self, *args): """""" Dump all the data to excel, fname and path can be passed as args """""" path = os.getcwd() fname = self.fname.replace("".ppl"", ""_ppl"") + "".xlsx"" if len(args) > 0 and args[0] != """": path = args[0] if os.path.exists(path) == False: os.mkdir(path) xl_file = pd.ExcelWriter(path + os.sep + fname) for idx in self.filter_data(""""): self.extract(idx) labels = list(self.filter_data("""").values()) for prof in self.data: data_df = pd.DataFrame() data_df[""X""] = self.data[prof][0] for timestep, data in zip(self.time, self.data[prof][1]): data_df[timestep] = data myvar = labels[prof-1].split("" "")[0] br_label = labels[prof-1].split(""\'"")[5] unit = labels[prof-1].split(""\'"")[7].replace(""/"", ""-"") mylabel = ""{} - {} - {}"".format(myvar, br_label, unit) data_df.to_excel(xl_file, sheet_name=mylabel) xl_file.save()",3
28,Python,export to excel,https://github.com/patarapolw/AnkiTools/blob/fab6836dfd9cf5171d9cbff5c55fbb14d2786f05/AnkiTools/excel.py#L155-L157,"def save(self): pyexcel_export.save_data(self.excel_filename, data=self.excel_raw, retain_meta=True, created=self.created, modified=datetime.now().isoformat())",3
63,Python,export to excel,https://github.com/turicas/rows/blob/c74da41ae9ed091356b803a64f8a30c641c5fc45/rows/plugins/xlsx.py#L152-L193,"def export_to_xlsx(table, filename_or_fobj=None, sheet_name=""Sheet1"", *args, **kwargs): """"""Export the rows.Table to XLSX file and return the saved file."""""" workbook = Workbook() sheet = workbook.active sheet.title = sheet_name prepared_table = prepare_to_export(table, *args, **kwargs) # Write header field_names = next(prepared_table) for col_index, field_name in enumerate(field_names): cell = sheet.cell(row=1, column=col_index + 1) cell.value = field_name # Write sheet rows _convert_row = _python_to_cell(list(map(table.fields.get, field_names))) for row_index, row in enumerate(prepared_table, start=1): for col_index, (value, number_format) in enumerate(_convert_row(row)): cell = sheet.cell(row=row_index + 1, column=col_index + 1) cell.value = value if number_format is not None: cell.number_format = number_format return_result = False if filename_or_fobj is None: filename_or_fobj = BytesIO() return_result = True source = Source.from_file(filename_or_fobj, mode=""wb"", plugin_name=""xlsx"") workbook.save(source.fobj) source.fobj.flush() if return_result: source.fobj.seek(0) result = source.fobj.read() else: result = source.fobj if source.should_close: source.fobj.close() return result",3
346,Python,export to excel,https://github.com/housecanary/hc-api-python/blob/2bb9e2208b34e8617575de45934357ee33b8531c/housecanary/excel/__init__.py#L18-L28,"def export_analytics_data_to_excel(data, output_file_name, result_info_key, identifier_keys): """"""Creates an Excel file containing data returned by the Analytics API Args: data: Analytics API data as a list of dicts output_file_name: File name for output Excel file (use .xlsx extension). """""" workbook = create_excel_workbook(data, result_info_key, identifier_keys) workbook.save(output_file_name) print('Saved Excel file to {}'.format(output_file_name))",3
444,Python,export to excel,https://github.com/IAMconsortium/pyam/blob/4077929ca6e7be63a0e3ecf882c5f1da97b287bf/pyam/core.py#L1049-L1070,"def to_excel(self, excel_writer, sheet_name='data', iamc_index=False, **kwargs): """"""Write timeseries data to Excel format Parameters ---------- excel_writer: string or ExcelWriter object file path or existing ExcelWriter sheet_name: string, default 'data' name of sheet which will contain `IamDataFrame.timeseries()` data iamc_index: bool, default False if True, use `['model', 'scenario', 'region', 'variable', 'unit']`; else, use all `data` columns """""" if not isinstance(excel_writer, pd.ExcelWriter): close = True excel_writer = pd.ExcelWriter(excel_writer) self._to_file_format(iamc_index)\ .to_excel(excel_writer, sheet_name=sheet_name, index=False, **kwargs) if close: excel_writer.close()",3
1046,Python,export to excel,https://github.com/rwl/pylon/blob/916514255db1ae1661406f0283df756baf960d14/contrib/pylontk.py#L377-L381,"def on_excel(self): from pylon.io.excel import ExcelWriter filename = asksaveasfilename(filetypes=[(""Excel file"", "".xls"")]) if filename: ExcelWriter(self.case).write(filename)",3
1207,Python,export to excel,https://github.com/fkarb/xltable/blob/7a592642d27ad5ee90d2aa8c26338abaa9d84bea/xltable/workbook.py#L82-L123,"def to_excel(self, xl_app=None, resize_columns=True): from win32com.client import Dispatch, gencache if xl_app is None: xl_app = Dispatch(""Excel.Application"") xl_app = gencache.EnsureDispatch(xl_app) # Add a new workbook with the correct number of sheets. # We aren't allowed to create an empty one. assert self.worksheets, ""Can't export workbook with no worksheets"" sheets_in_new_workbook = xl_app.SheetsInNewWorkbook try: xl_app.SheetsInNewWorkbook = float(len(self.worksheets)) self.workbook_obj = xl_app.Workbooks.Add() finally: xl_app.SheetsInNewWorkbook = sheets_in_new_workbook # Rename the worksheets, ensuring that there can never be two sheets with the same # name due to the sheets default names conflicting with the new names. sheet_names = {s.name for s in self.worksheets} assert len(sheet_names) == len(self.worksheets), ""Worksheets must have unique names"" for worksheet in self.workbook_obj.Sheets: i = 1 original_name = worksheet.Name while worksheet.Name in sheet_names: worksheet.Name = ""%s_%d"" % (original_name, i) i += 1 for worksheet, sheet in zip(self.workbook_obj.Sheets, self.worksheets): worksheet.Name = sheet.name # Export each sheet (have to use itersheets for this as it sets the # current active sheet before yielding each one). for worksheet, sheet in zip(self.workbook_obj.Sheets, self.itersheets()): worksheet.Select() sheet.to_excel(workbook=self, worksheet=worksheet, xl_app=xl_app, rename=False, resize_columns=resize_columns) return self.workbook_obj",3
2063,Python,export to excel,https://github.com/vicalloy/lbutils/blob/66ae7e73bc939f073cdc1b91602a95e67caf4ba6/lbutils/xlsxutils.py#L20-L32,"def export_xlsx(wb, output, fn): """""" export as excel wb: output: fn: file name """""" wb.close() output.seek(0) response = HttpResponse(output.read(), content_type=""application/vnd.ms-excel"") cd = codecs.encode('attachment;filename=%s' % fn, 'utf-8') response['Content-Disposition'] = cd return response",3
706,Python,export to excel,https://github.com/APSL/transmanager/blob/79157085840008e146b264521681913090197ed1/transmanager/export.py#L58-L67,"def export_translations(tasks_ids): qs = TransTask.objects.filter(pk__in=tasks_ids) export = ExportQueryset( qs, TransTask, ('id', 'object_name', 'object_pk', 'object_field_label', 'object_field_value', 'number_of_words', 'object_field_value_translation', 'date_modification', 'done') ) excel = export.get_excel() return excel",2
700,Python,export to excel,https://github.com/MacHu-GWU/pymongo_mate-project/blob/be53170c2db54cb705b9e548d32ef26c773ff7f3/pymongo_mate/pkg/pandas_mate/sql_io.py#L55-L97,"def excel_to_sql(excel_file_path, engine, read_excel_kwargs=None, to_generic_type_kwargs=None, to_sql_kwargs=None): """"""Create a database from excel. :param read_excel_kwargs: dict, arguments for ``pandas.read_excel`` method. example: ``{""employee"": {""skiprows"": 10}, ""department"": {}}`` :param to_sql_kwargs: dict, arguments for ``pandas.DataFrame.to_sql`` method. limitation: 1. If a integer column has None value, data type in database will be float. Because pandas thinks that it is ``np.nan``. 2. If a string column looks like integer, ``pandas.read_excel()`` method doesn't have options to convert it to string. """""" if read_excel_kwargs is None: read_excel_kwargs = dict() if to_sql_kwargs is None: to_sql_kwargs = dict() if to_generic_type_kwargs is None: to_generic_type_kwargs = dict() xl = pd.ExcelFile(excel_file_path) for sheet_name in xl.sheet_names: df = pd.read_excel( excel_file_path, sheet_name, **read_excel_kwargs.get(sheet_name, dict()) ) kwargs = to_generic_type_kwargs.get(sheet_name) if kwargs: data = to_dict_list_generic_type(df, **kwargs) smart_insert(data, sheet_name, engine) else: df.to_sql( sheet_name, engine, index=False, **to_sql_kwargs.get(sheet_name, dict(if_exists=""replace"")) )",1
56,Python,encrypt aes ctr mode,https://github.com/keybase/python-triplesec/blob/0a73e18cfe542d0cd5ee57bd823a67412b4b717e/triplesec/crypto.py#L86-L92,"def encrypt(cls, data, key, iv_data): validate_key_size(key, cls.key_size, ""AES"") iv, ctr = iv_data ciphertext = Crypto_AES.new(key, Crypto_AES.MODE_CTR, counter=ctr).encrypt(data) return iv + ciphertext",3
112,Python,encrypt aes ctr mode,https://github.com/jcassee/django-geckoboard/blob/6ebdaa86015fe645360abf1ba1290132de4cf6d6/django_geckoboard/decorators.py#L446-L460,"def _encrypt(data): """"""Equivalent to OpenSSL using 256 bit AES in CBC mode"""""" BS = AES.block_size def pad(s): n = BS - len(s) % BS char = chr(n).encode('utf8') return s + n * char password = settings.GECKOBOARD_PASSWORD salt = Random.new().read(BS - len('Salted__')) key, iv = _derive_key_and_iv(password, salt, 32, BS) cipher = AES.new(key, AES.MODE_CBC, iv) encrypted = b'Salted__' + salt + cipher.encrypt(pad(data)) return base64.b64encode(encrypted)",3
407,Python,encrypt aes ctr mode,https://github.com/ghackebeil/PyORAM/blob/b8832c1b753c0b2148ef7a143c5f5dd3bbbb61e7/src/pyoram/crypto/aes.py#L24-L27,"def CTREnc(key, plaintext): iv = os.urandom(AES.block_size) cipher = _cipher(_aes(key), _ctrmode(iv), backend=_backend).encryptor() return iv + cipher.update(plaintext) + cipher.finalize()",3
1056,Python,encrypt aes ctr mode,https://github.com/teitei-tk/Simple-AES-Cipher/blob/cb4bc69d762397f3a26f6b009e2b7fa5d706ed5f/simple_aes_cipher/cipher.py#L15-L19,"def encrypt(self, raw, mode=AES.MODE_CBC): raw = self._pad(raw, AES.block_size) iv = Random.new().read(AES.block_size) cipher = AES.new(self.key, mode, iv) return base64.b64encode(iv + cipher.encrypt(raw)).decode('utf-8')",3
1096,Python,encrypt aes ctr mode,https://github.com/ontio/ontology-python-sdk/blob/ac88bdda941896c5d2ced08422a9c5179d3f9b19/ontology/crypto/aes_handler.py#L72-L76,"def aes_cbc_encrypt(plain_text: bytes, key: bytes, iv: bytes = b''): if len(iv) == 0: iv = AESHandler.generate_iv() cipher = AES.new(key=key, mode=AES.MODE_CBC, iv=iv) return cipher.IV, cipher.encrypt(pad(plain_text, AES.block_size))",3
1680,Python,encrypt aes ctr mode,https://github.com/ontio/ontology-python-sdk/blob/ac88bdda941896c5d2ced08422a9c5179d3f9b19/ontology/crypto/aes_handler.py#L59-L63,"def aes_ctr_encrypt(plain_text: bytes, key: bytes): cipher = AES.new(key=key, mode=AES.MODE_CTR) cipher_text = cipher.encrypt(plain_text) nonce = cipher.nonce return nonce, cipher_text",3
285,Python,encrypt aes ctr mode,https://github.com/boldfield/s3-encryption/blob/d88549ba682745dc6b199934c5b5221de7f8d8bc/s3_encryption/crypto.py#L52-L57,"def aes_encrypt(key, data, mode='ECB', iv=None): aes = AES() aes.mode = mode aes.iv = iv aes.key = key return aes.encrypt(data)",2
486,Python,encrypt aes ctr mode,https://github.com/StorjOld/heartbeat/blob/4d54f2011f1e9f688073d4347bc51bb7bd682718/heartbeat/PySwizzle/PySwizzle.py#L162-L178,"def encrypt(self, key): """"""This method encrypts and signs the state to make it unreadable by the server, since it contains information that would allow faking proof of storage. :param key: the key to encrypt and sign with """""" if (self.encrypted): return # encrypt self.iv = Random.new().read(AES.block_size) aes = AES.new(key, AES.MODE_CFB, self.iv) self.f_key = aes.encrypt(self.f_key) self.alpha_key = aes.encrypt(self.alpha_key) self.encrypted = True # sign self.hmac = self.get_hmac(key)",2
1233,Python,encrypt aes ctr mode,https://github.com/boldfield/s3-encryption/blob/d88549ba682745dc6b199934c5b5221de7f8d8bc/s3_encryption/crypto.py#L20-L25,"def encrypt(self, data): if self.iv is None: cipher = pyAES.new(self.key, self.mode) else: cipher = pyAES.new(self.key, self.mode, self.iv) return cipher.encrypt(pad_data(AES.str_to_bytes(data)))",2
1265,Python,encrypt aes ctr mode,https://github.com/ghackebeil/PyORAM/blob/b8832c1b753c0b2148ef7a143c5f5dd3bbbb61e7/examples/aesctr_performance.py#L113-L115,"def main(): runtest(""AES - CTR Mode"", AES.CTREnc, AES.CTRDec) runtest(""AES - GCM Mode"", AES.GCMEnc, AES.GCMDec)",1
258,Python,encode url,https://github.com/dustin/twitty-twister/blob/8524750ee73adb57bbe14ef0cfd8aa08e1e59fb3/twittytwister/twitter.py#L171-L177,"def _urlencode(self, h): rv = [] for k,v in h.iteritems(): rv.append('%s=%s' % (urllib.quote(k.encode(""utf-8"")), urllib.quote(v.encode(""utf-8"")))) return '&'.join(rv)",3
303,Python,encode url,https://github.com/marvin-ai/marvin-python-toolbox/blob/7c95cb2f9698b989150ab94c1285f3a9eaaba423/marvin_python_toolbox/common/utils.py#L286-L295,"def url_encode(url): """""" Convert special characters using %xx escape. :param url: str :return: str - encoded url """""" if isinstance(url, text_type): url = url.encode('utf8') return quote(url, ':/%?&=')",3
434,Python,encode url,https://github.com/yunpian/yunpian-python-sdk/blob/405a1196ec83fdf29ff454f74ef036974be11970/yunpian_python_sdk/ypclient.py#L195-L208,"def urlEncodeAndJoin(self, seq, sepr=','): '''sepr.join(urlencode(seq)) Args: seq: string list to be urlencoded sepr: join seq with sepr Returns: str ''' try: from urllib.parse import quote_plus as encode return sepr.join([encode(x, encoding=CHARSET_UTF8) for x in seq]) except ImportError: from urllib import quote as encode return sepr.join([i for i in map(lambda x: encode(x), seq)])",3
459,Python,encode url,https://github.com/fred49/linshare-api/blob/be646c25aa8ba3718abb6869c620b157d53d6e41/linshareapi/admin/upgradetasks.py#L177-L191,"def trigger(self, identifier, force=True): """"""Trigger an upgrade task."""""" self.debug(identifier) url = ""{base}/{identifier}"".format( base=self.local_base_url, identifier=identifier ) param = {} if force: param['force'] = force encode = urllib.urlencode(param) if encode: url += ""?"" url += encode return self.core.update(url, {})",3
679,Python,encode url,https://github.com/fred49/linshare-api/blob/be646c25aa8ba3718abb6869c620b157d53d6e41/linshareapi/admin/jwt.py#L52-L64,"def _list(self, domain=None): # pylint: disable=arguments-differ url = ""{base}"".format( base=self.local_base_url ) param = {} if domain: param['domainUuid'] = domain encode = urllib.urlencode(param) if encode: url += ""?"" url += encode return self.core.list(url)",3
1005,Python,encode url,https://github.com/fadhiilrachman/line-py/blob/b7f5f2b3fc09fa3fbf6088d7ebdaf9e44d96ba69/linepy/server.py#L18-L19,"def urlEncode(self, url, path, params=[]): return url + path + '?' + urllib.parse.urlencode(params)",3
1042,Python,encode url,https://github.com/oauthlib/oauthlib/blob/30321dd3c0ca784d3508a1970cf90d9f76835c79/oauthlib/common.py#L83-L89,"def urlencode(params): utf8_params = encode_params_utf8(params) urlencoded = _urlencode(utf8_params) if isinstance(urlencoded, unicode_type): # PY3 returns unicode return urlencoded else: return urlencoded.decode(""utf-8"")",3
1285,Python,encode url,https://github.com/defensio/defensio-python/blob/c1d2b64be941acb63c452a6d9a5526c59cb37007/defensio/__init__.py#L121-L125,"def _urlencode(self, url): if is_python3(): return urllib.parse.urlencode(url) else: return urllib.urlencode(url)",3
1710,Python,encode url,https://github.com/couchbase/couchbase-python-client/blob/a7bada167785bf79a29c39f820d932a433a6a535/couchbase/connstr.py#L126-L143,"def encode(self): """""" Encodes the current state of the object into a string. :return: The encoded string """""" opt_dict = {} for k, v in self.options.items(): opt_dict[k] = v[0] ss = '{0}://{1}'.format(self.scheme, ','.join(self.hosts)) if self.bucket: ss += '/' + self.bucket # URL encode options then decoded forward slash / ss += '?' + urlencode(opt_dict).replace('%2F', '/') return ss",3
900,Python,encode url,https://github.com/frawau/aioblescan/blob/02d12e90db3ee6df7be6513fec171f20dc533de3/aioblescan/plugins/eddystone.py#L105-L151,"def url_encoder(self): encodedurl = [] encodedurl.append(aios.IntByte(""Tx Power"",self.power)) asisurl="""" myurl = urlparse(self.type_payload) myhostname = myurl.hostname mypath = myurl.path if (myurl.scheme,myhostname.startswith(""www."")) in url_schemes: encodedurl.append(aios.IntByte(""URL Scheme"", url_schemes.index((myurl.scheme,myhostname.startswith(""www.""))))) if myhostname.startswith(""www.""): myhostname = myhostname[4:] extval=None if myhostname.split(""."")[-1] in url_domain: extval = url_domain.index(myhostname.split(""."")[-1]) myhostname = ""."".join(myhostname.split(""."")[:-1]) if extval is not None and not mypath.startswith(""/""): extval+=7 else: if myurl.port is None: if extval is not None: mypath = mypath[1:] else: extval += 7 encodedurl.append(aios.String(""URL string"")) encodedurl[-1].val = myhostname if extval is not None: encodedurl.append(aios.IntByte(""URL Extention"",extval)) if myurl.port: asisurl += "":""+str(myurl.port)+mypath asisurl += mypath if myurl.params: asisurl += "";""+myurl.params if myurl.query: asisurl += ""?""+myurl.query if myurl.fragment: asisurl += ""#""+myurl.fragment encodedurl.append(aios.String(""Rest of URL"")) encodedurl[-1].val = asisurl tlength=0 for x in encodedurl: #Check the payload length tlength += len(x) if tlength > 19: #Actually 18 but we have tx power raise Exception(""Encoded url too long (max 18 bytes)"") self.service_data_length.val += tlength #Update the payload length return encodedurl",2
707,Python,deserialize json,https://github.com/ubc/github2gitlab/blob/795898f6d438621fa0c996a7156d70c382ff0493/github2gitlab/main.py#L434-L440,"def json_loads(payload): ""Log the payload that cannot be parsed"" try: return json.loads(payload) except ValueError as e: log.error(""unable to json.loads("" + payload + "")"") raise e",3
828,Python,deserialize json,https://github.com/steenzout/python-serialization-json/blob/583568e14cc02ba0bf711f56b8a0a3ad142c696d/steenzout/serialization/json/__init__.py#L50-L71,"def deserialize(json, cls=None): """"""Deserialize a JSON string into a Python object. Args: json (str): the JSON string. cls (:py:class:`object`): if the ``json`` is deserialized into a ``dict`` and this argument is set, the ``dict`` keys are passed as keyword arguments to the given ``cls`` initializer. Returns: Python object representation of the given JSON string. """""" LOGGER.debug('deserialize(%s)', json) out = simplejson.loads(json) if isinstance(out, dict) and cls is not None: return cls(**out) return out",3
852,Python,deserialize json,https://github.com/infobip/infobip-api-python-client/blob/2da11962f877294c53bd545715e62d6ce5c139f0/infobip/util/http.py#L14-L16,"def deserialize(self, s, cls): vals = json.JSONDecoder().decode(s) return self.deserialize_map(vals, cls)",3
1216,Python,deserialize json,https://github.com/jazzband/sorl-thumbnail/blob/22ccd9781462a820f963f57018ad3dcef85053ed/sorl/thumbnail/helpers.py#L55-L58,"def deserialize(s): if isinstance(s, bytes): return json.loads(s.decode('utf-8')) return json.loads(s)",3
1871,Python,deserialize json,https://github.com/go-macaroon-bakery/py-macaroon-bakery/blob/63ce1ef1dabe816eb8aaec48fbb46761c34ddf77/macaroonbakery/bakery/_macaroon.py#L242-L248,"def deserialize_json(cls, serialized_json): '''Return a macaroon deserialized from a string @param serialized_json The string to decode {str} @return {Macaroon} ''' serialized = json.loads(serialized_json) return Macaroon.from_dict(serialized)",3
98,Python,deserialize json,https://github.com/wtsi-hgi/python-hgijson/blob/6e8ccb562eabcaa816a136268a16504c2e0d4664/hgijson/json_converters/_converters.py#L61-L69,"def deserialize(self, deserializable: PrimitiveJsonType) -> Optional[SerializableType]: if not isinstance(self._decoder, ParsedJSONDecoder): # Decode must take a string (even though we have a richer representation) :/ json_as_string = json.dumps(deserializable) return self._decoder.decode(json_as_string) else: # Optimisation - no need to convert our relatively rich representation into a string (just to turn it back # again!) return self._decoder.decode_parsed(deserializable)",2
631,Python,deserialize json,https://github.com/F483/apigen/blob/f05ce1509030764721cc3393410fa12b609e88f2/apigen/apigen.py#L234-L244,"def _deserialize(kwargs): def deserialize(item): if isinstance(item[1], str): try: data = json.loads(item[1]) # load as json except: data = item[1].decode('utf-8') # must be a string else: data = item[1] # already deserialized (method default value) return (item[0], data) return dict(map(deserialize, kwargs.items()))",2
681,Python,deserialize json,https://github.com/sassoo/goldman/blob/b72540c9ad06b5c68aadb1b4fa8cb0b716260bf2/goldman/deserializers/json_7159.py#L22-L47,"def deserialize(self): """""" Invoke the RFC 7159 spec compliant parser :return: the parsed & vetted request body """""" super(Deserializer, self).deserialize() try: return json.loads(self.req.get_body()) except TypeError: link = 'tools.ietf.org/html/rfc7159' self.fail('Typically, this error is due to a missing JSON ' 'payload in your request when one was required. ' 'Otherwise, it could be a bug in our API.', link) except UnicodeDecodeError: link = 'tools.ietf.org/html/rfc7159#section-8.1' self.fail('We failed to process your JSON payload & it is ' 'most likely due to non UTF-8 encoded characters ' 'in your JSON.', link) except ValueError as exc: link = 'tools.ietf.org/html/rfc7159' self.fail('The JSON payload appears to be malformed & we ' 'failed to process it. The error with line & column ' 'numbers is: %s' % exc.message, link)",2
1079,Python,deserialize json,https://github.com/open511/open511/blob/3d573f59d7efa06ff1b5419ea5ff4d90a90b3cf8/open511/utils/serialization.py#L47-L60,"def deserialize(s): s = s.strip() try: doc = etree.fromstring(s) if is_tmdd(doc): # Transparently convert the TMDD on deserialize from ..converter.tmdd import tmdd_to_json return (tmdd_to_json(doc), 'json') return (doc, 'xml') except etree.XMLSyntaxError: try: return (json.loads(s), 'json') except ValueError: raise Exception(""Doesn't look like either JSON or XML"")",2
1767,Python,deserialize json,https://github.com/hyperledger/indy-sdk/blob/55240dc170308d7883c48f03f308130a6d077be6/vcx/wrappers/python3/vcx/api/proof.py#L55-L70,"async def deserialize(data: dict): """""" Builds a Proof object with defined attributes. Attributes are provided by a previous call to the serialize function. :param data: Example: name = ""proof name"" requested_attrs = [{""name"": ""age"", ""restrictions"": [{""schema_id"": ""6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11"", ""schema_name"":""Faber Student Info"", ""schema_version"":""1.0"", ""schema_issuer_did"":""6XFh8yBzrpJQmNyZzgoTqB"", ""issuer_did"":""8XFh8yBzrpJQmNyZzgoTqB"", ""cred_def_id"": ""8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766"" }, { ""schema_id"": ""5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11"", ""schema_name"":""BYU Student Info"", ""schema_version"":""1.0"", ""schema_issuer_did"":""5XFh8yBzrpJQmNyZzgoTqB"", ""issuer_did"":""66Fh8yBzrpJQmNyZzgoTqB"", ""cred_def_id"": ""66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766"" } ] }, { ""name"":""name"", ""restrictions"": [ { ""schema_id"": ""6XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11"", ""schema_name"":""Faber Student Info"", ""schema_version"":""1.0"", ""schema_issuer_did"":""6XFh8yBzrpJQmNyZzgoTqB"", ""issuer_did"":""8XFh8yBzrpJQmNyZzgoTqB"", ""cred_def_id"": ""8XFh8yBzrpJQmNyZzgoTqB:3:CL:1766"" }, { ""schema_id"": ""5XFh8yBzrpJQmNyZzgoTqB:2:schema_name:0.0.11"", ""schema_name"":""BYU Student Info"", ""schema_version"":""1.0"", ""schema_issuer_did"":""5XFh8yBzrpJQmNyZzgoTqB"", ""issuer_did"":""66Fh8yBzrpJQmNyZzgoTqB"", ""cred_def_id"": ""66Fh8yBzrpJQmNyZzgoTqB:3:CL:1766""}]}] proof = await Proof.create(source_id, name, requested_attrs) data = proof.serialize() proof2 = await Proof.deserialize(data) :return: Proof Object """""" return await Proof._deserialize(""vcx_proof_deserialize"", json.dumps(data), data.get('data').get('source_id'))",2
1067,Python,deducting the median from each column,https://github.com/modin-project/modin/blob/5b77d242596560c646b8405340c9ce64acb183cb/modin/backends/pandas/query_compiler.py#L1251-L1263,"def median(self, **kwargs): """"""Returns median of each column or row. Returns: A new QueryCompiler object containing the median of each column or row. """""" if self._is_transposed: kwargs[""axis""] = kwargs.get(""axis"", 0) ^ 1 return self.transpose().median(**kwargs) # Pandas default is 0 (though not mentioned in docs) axis = kwargs.get(""axis"", 0) func = self._build_mapreduce_func(pandas.DataFrame.median, **kwargs) return self._full_axis_reduce(axis, func)",3
1281,Python,deducting the median from each column,https://github.com/vstinner/perf/blob/cf096c0c0c955d0aa1c893847fa6393ba4922ada/perf/_utils.py#L481-L484,def median_abs_dev(values): # Median Absolute Deviation median = float(statistics.median(values)) return statistics.median([abs(median - sample) for sample in values]),3
173,Python,deducting the median from each column,https://github.com/hollenstein/maspy/blob/f15fcfd24df306d8420540460d902aa3073ec133/maspy/mit_stats.py#L48-L88,"def runningMedian(seq, M): """""" Purpose: Find the median for the points in a sliding window (odd number in size) as it is moved from left to right by one point at a time. Inputs: seq -- list containing items for which a running median (in a sliding window) is to be calculated M -- number of items in window (window size) -- must be an integer > 1 Otputs: medians -- list of medians with size N - M + 1 Note: 1. The median of a finite list of numbers is the ""center"" value when this list is sorted in ascending order. 2. If M is an even number the two elements in the window that are close to the center are averaged to give the median (this is not by definition) """""" seq = iter(seq) s = [] m = M // 2 #// does a truncated division like integer division in Python 2 # Set up list s (to be sorted) and load deque with first window of seq s = [item for item in islice(seq,M)] d = deque(s) # Simple lambda function to handle even/odd window sizes median = lambda : s[m] if bool(M&1) else (s[m-1]+s[m])*0.5 # Sort it in increasing order and extract the median (""center"" of the sorted window) s.sort() medians = [median()] # Now slide the window by one point to the right for each new position (each pass through # the loop). Stop when the item in the right end of the deque contains the last item in seq for item in seq: old = d.popleft() # pop oldest from left d.append(item) # push newest in from right del s[bisect_left(s, old)] # locate insertion point and then remove old insort(s, item) # insert newest such that new sort is not required medians.append(median()) return medians",2
225,Python,deducting the median from each column,https://github.com/EelcoHoogendoorn/Numpy_arraysetops_EP/blob/84dc8114bf8a79c3acb3f7f59128247b9fc97243/numpy_indexed/grouping.py#L343-L382,"def median(self, values, axis=0, average=True): """"""compute the median value over each group. Parameters ---------- values : array_like, [keys, ...] values to compute the median of per group axis : int, optional alternative reduction axis for values average : bool, optional when average is true, the average of the two central values is taken for groups with an even key-count Returns ------- unique: ndarray, [groups] unique keys reduced : ndarray, [groups, ...] value array, reduced over groups """""" mid_2 = self.index.start + self.index.stop hi = (mid_2 ) // 2 lo = (mid_2 - 1) // 2 #need this indirection for lex-index compatibility sorted_group_rank_per_key = self.index.sorted_group_rank_per_key def median1d(slc): #place values at correct keys; preconditions the upcoming lexsort slc = slc[self.index.sorter] #refine value sorting within each keygroup sorter = np.lexsort((slc, sorted_group_rank_per_key)) slc = slc[sorter] return (slc[lo]+slc[hi]) / 2 if average else slc[hi] values = np.asarray(values) if values.ndim>1: #is trying to skip apply_along_axis somewhat premature optimization? values = np.apply_along_axis(median1d, axis, values) else: values = median1d(values) return self.unique, values",2
413,Python,deducting the median from each column,https://github.com/CEA-COSMIC/ModOpt/blob/019b189cb897cbb4d210c44a100daaa08468830c/modopt/math/stats.py#L75-L106,"def mad(data): r""""""Median absolute deviation This method calculates the median absolute deviation of the input data. Parameters ---------- data : np.ndarray Input data array Returns ------- float MAD value Examples -------- >>> from modopt.math.stats import mad >>> a = np.arange(9).reshape(3, 3) >>> mad(a) 2.0 Notes ----- The MAD is calculated as follows: .. math:: \mathrm{MAD} = \mathrm{median}\left(|X_i - \mathrm{median}(X)|\right) """""" return np.median(np.abs(data - np.median(data)))",2
586,Python,deducting the median from each column,https://github.com/tamasgal/km3pipe/blob/7a9b59ac899a28775b5bdc5d391d9a5340d08040/km3pipe/stats.py#L114-L117,"def mad(v): """"""MAD -- Median absolute deviation. More robust than standard deviation. """""" return np.median(np.abs(v - np.median(v)))",2
1140,Python,deducting the median from each column,https://github.com/neherlab/treetime/blob/f6cdb58d19243a18ffdaa2b2ec71872fa00e65c0/treetime/utils.py#L129-L139,"def median_interp(interp_object): """""" Find the median of the function represented as an interpolation object. """""" new_grid = np.sort(np.concatenate([interp_object.x[:-1] + 0.1*ii*np.diff(interp_object.x) for ii in range(10)]).flatten()) tmp_prop = np.exp(-(interp_object(new_grid)-interp_object.y.min())) tmp_cumsum = np.cumsum(0.5*(tmp_prop[1:]+tmp_prop[:-1])*np.diff(new_grid)) median_index = min(len(tmp_cumsum)-3, max(2,np.searchsorted(tmp_cumsum, tmp_cumsum[-1]*0.5)+1)) return new_grid[median_index]",2
1594,Python,deducting the median from each column,https://github.com/rhenanbartels/hrv/blob/cd4c7e6e508299d943930886d20413f63845f60f/hrv/rri.py#L123-L124,"def median(self, *args, **kwargs): return np.median(self.rri, *args, **kwargs)",2
365,Python,deducting the median from each column,https://github.com/adaptive-learning/proso-apps/blob/8278c72e498d6ef8d392cc47b48473f4ec037142/proso/models/option_selection.py#L195-L206,"def adjust_to_level(self, level, x, op, median): if x > median: if level > 0.5: result = median + (x - median) * ((level - 0.5) / 0.5) else: result = op + (median - op) * (level / 0.5) else: if level > 0.5: result = x + (median - x) * ((level - 0.5) / 0.5) else: result = median + (op - median) * (level / 0.5) return result",0
1354,Python,deducting the median from each column,https://github.com/satellogic/telluric/blob/e752cd3ee71e339f79717e526fde362e80055d9e/telluric/plotting.py#L56-L79,"def zoom_level_from_geometry(geometry, splits=4): """"""Generate optimum zoom level for geometry. Notes ----- The obvious solution would be >>> mercantile.bounding_tile(*geometry.get_shape(WGS84_CRS).bounds).z However, if the geometry is split between two or four tiles, the resulting zoom level might be too big. """""" # This import is here to avoid cyclic references from telluric.vectors import generate_tile_coordinates # We split the geometry and compute the zoom level for each chunk levels = [] for chunk in generate_tile_coordinates(geometry, (splits, splits)): levels.append(mercantile.bounding_tile(*chunk.get_shape(WGS84_CRS).bounds).z) # We now return the median value using the median_low function, which # always picks the result from the list return median_low(levels)",0
2,Python,custom http error response,https://github.com/mapbox/mapbox-sdk-py/blob/72d19dbcf2d254a6ea08129a726471fd21f13023/mapbox/services/base.py#L122-L144,"def handle_http_error(self, response, custom_messages=None, raise_for_status=False): """"""Converts service errors to Python exceptions Parameters ---------- response : requests.Response A service response. custom_messages : dict, optional A mapping of custom exception messages to HTTP status codes. raise_for_status : bool, optional If True, the requests library provides Python exceptions. Returns ------- None """""" if not custom_messages: custom_messages = {} if response.status_code in custom_messages.keys(): raise errors.HTTPError(custom_messages[response.status_code]) if raise_for_status: response.raise_for_status()",3
1363,Python,custom http error response,https://github.com/nestauk/gtr/blob/49e1b8db1f4376612ea1ec8585a79375fa15a899/gtr/services/base.py#L25-L33,"def handle_http_error(self, response, custom_messages=None, raise_for_status=False): if not custom_messages: custom_messages = {} if response.status_code in custom_messages.keys(): raise requests.exceptions.HTTPError( custom_messages[response.status_code]) if raise_for_status: response.raise_for_status()",3
1535,Python,custom http error response,https://github.com/onecodex/onecodex/blob/326a0a1af140e3a57ccf31c3c9c5e17a5775c13d/onecodex/vendored/potion_client/links.py#L73-L90,"def raise_for_status(self, response): http_error_msg = '' if 400 <= response.status_code < 500: try: http_error_msg = response.json() except: http_error_msg = ('{code} Client Error: {reason} for url: {url}'.format( code=response.status_code, reason=response.reason, url=response.url) ) elif 500 <= response.status_code < 600: http_error_msg = ('{code} Server Error: {reason} for url: {url}'.format( code=response.status_code, reason=response.reason, url=response.url) ) if http_error_msg: raise HTTPError(http_error_msg, response=response)",3
371,Python,custom http error response,https://github.com/lepture/flask-oauthlib/blob/9e6f152a5bb360e7496210da21561c3e6d41b0e1/flask_oauthlib/provider/oauth1.py#L902-L905,"def _error_response(e): res = make_response(e.urlencoded, e.status_code) res.headers['Content-Type'] = 'application/x-www-form-urlencoded' return res",2
1389,Python,custom http error response,https://github.com/JamesGardiner/chwrapper/blob/50f9cb2f5264c59505e8cc4e45ee6dc5d5669134/chwrapper/services/base.py#L97-L109,"def handle_http_error( self, response, ignore=None, custom_messages=None, raise_for_status=True ): status = response.status_code ignore = ignore or [] custom_messages = custom_messages or {} if status in ignore or status in self._ignore_codes: return None elif response.status_code in custom_messages.keys(): raise requests.exceptions.HTTPError(custom_messages[response.status_code]) elif raise_for_status: response.raise_for_status()",2
905,Python,custom http error response,https://github.com/FelixSchwarz/pymta/blob/1884accc3311e6c2e89259784f9592314f6d34fc/pymta/session.py#L175-L180,"def _send_custom_response(self, reply): code, custom_response = reply if self._is_multiline_reply(custom_response): self.multiline_reply(code, custom_response) else: self.reply(code, custom_response)",1
1250,Python,custom http error response,https://github.com/swevm/scaleio-py/blob/d043a0137cb925987fd5c895a3210968ce1d9028/scaleiopy/im.py#L171-L200,"def _do_put(self, uri, **kwargs): """""" Convinient method for POST requests Returns http request status value from a POST request """""" #TODO: # Add error handling. Check for HTTP status here would be much more conveinent than in each calling method scaleioapi_put_headers = {'content-type':'application/json'} print ""_do_put()"" if kwargs: for key, value in kwargs.iteritems(): #if key == 'headers': # scaleio_post_headers = value # print ""Adding custom PUT headers"" if key == 'json': payload = value try: self.logger.debug(""do_put(): "" + ""{}"".format(uri)) #self._session.headers.update({'Content-Type':'application/json'}) response = self._session.put(url, headers=scaleioapi_put_headers, verify_ssl=self._im_verify_ssl, data=json.dumps(payload)) self.logger.debug(""_do_put() - Response: "" + ""{}"".format(response.text)) if response.status_code == requests.codes.ok: return response else: self.logger.error(""_do_put() - HTTP response error: "" + ""{}"".format(response.status_code)) raise RuntimeError(""_do_put() - HTTP response error"" + response.status_code) except: raise RuntimeError(""_do_put() - Communication error with ScaleIO gateway"") return response",1
1299,Python,custom http error response,https://github.com/alpacahq/alpaca-trade-api-python/blob/9c9dea3b4a37c909f88391b202e86ff356a8b4d7/alpaca_trade_api/rest.py#L40-L43,"def status_code(self): http_error = self._http_error if http_error is not None and hasattr(http_error, 'response'): return http_error.response.status_code",1
1457,Python,custom http error response,https://github.com/swevm/scaleio-py/blob/d043a0137cb925987fd5c895a3210968ce1d9028/scaleiopy/im.py#L202-L230,"def _do_post(self, url, **kwargs): """""" Convinient method for POST requests Returns http request status value from a POST request """""" #TODO: # Add error handling. Check for HTTP status here would be much more conveinent than in each calling method scaleioapi_post_headers = {'Content-type':'application/json','Version':'1.0'} self.logger.debug(""_do_post()"") if kwargs: for key, value in kwargs.iteritems(): if key == 'headers': scaleio_post_headers = value print ""Adding custom POST headers"" if key == 'files': upl_files = value print ""Adding files to upload"" try: response = self._session.post(url, headers=scaleioapi_post_headers, verify_ssl=self._im_verify_ssl, files=upl_files) self.logger.debug(""_do_post() - Response: "" + ""{}"".format(response.text)) if response.status_code == requests.codes.ok: return response else: self.logger.error(""_do_post() - Response Code: "" + ""{}"".format(response.status_code)) raise RuntimeError(""_do_post() - HTTP response error"" + response.status_code) except: raise RuntimeError(""_do_post() - Communication error with ScaleIO gateway"") return response",1
1868,Python,custom http error response,https://github.com/googleapis/google-cloud-python/blob/85e80125a59cb10f8cb105f25ecc099e4b940b50/storage/google/cloud/storage/blob.py#L2017-L2034,"def _raise_from_invalid_response(error): """"""Re-wrap and raise an ``InvalidResponse`` exception. :type error: :exc:`google.resumable_media.InvalidResponse` :param error: A caught exception from the ``google-resumable-media`` library. :raises: :class:`~google.cloud.exceptions.GoogleCloudError` corresponding to the failed status code """""" response = error.response error_message = str(error) message = u""{method} {url}: {error}"".format( method=response.request.method, url=response.request.url, error=error_message ) raise exceptions.from_http_status(response.status_code, message, response=response)",1
140,Python,create cookie,https://github.com/hubo1016/vlcp/blob/239055229ec93a99cc7e15208075724ccf543bd1/vlcp/utils/http.py#L175-L196,"def setcookie(self, key, value, max_age=None, expires=None, path='/', domain=None, secure=None, httponly=False): """""" Add a new cookie """""" newcookie = Morsel() newcookie.key = key newcookie.value = value newcookie.coded_value = value if max_age is not None: newcookie['max-age'] = max_age if expires is not None: newcookie['expires'] = expires if path is not None: newcookie['path'] = path if domain is not None: newcookie['domain'] = domain if secure: newcookie['secure'] = secure if httponly: newcookie['httponly'] = httponly self.sent_cookies = [c for c in self.sent_cookies if c.key != key] self.sent_cookies.append(newcookie) def bufferoutput(self):",3
336,Python,create cookie,https://github.com/pgjones/quart/blob/7cb2d3bd98e8746025764f2b933abc12041fa175/quart/utils.py#L29-L59,"def create_cookie( key: str, value: str='', max_age: Optional[Union[int, timedelta]]=None, expires: Optional[Union[int, float, datetime]]=None, path: str='/', domain: Optional[str]=None, secure: bool=False, httponly: bool=False, ) -> SimpleCookie: """"""Create a Cookie given the options set The arguments are the standard cookie morsels and this is a wrapper around the stdlib SimpleCookie code. """""" cookie = SimpleCookie() cookie[key] = value cookie[key]['path'] = path cookie[key]['httponly'] = httponly # type: ignore cookie[key]['secure'] = secure # type: ignore if isinstance(max_age, timedelta): cookie[key]['max-age'] = f""{max_age.total_seconds():d}"" if isinstance(max_age, int): cookie[key]['max-age'] = str(max_age) if expires is not None and isinstance(expires, (int, float)): cookie[key]['expires'] = format_date_time(int(expires)) elif expires is not None and isinstance(expires, datetime): cookie[key]['expires'] = format_date_time(expires.replace(tzinfo=timezone.utc).timestamp()) if domain is not None: cookie[key]['domain'] = domain return cookie",3
509,Python,create cookie,https://github.com/IdentityPython/oidcendpoint/blob/6c1d729d51bfb6332816117fe476073df7a1d823/src/oidcendpoint/cookie.py#L299-L342,"def create_cookie(self, value, typ, cookie_name=None, ttl=-1, kill=False): """""" :param value: Part of the cookie payload :param typ: Type of cookie :param cookie_name: :param ttl: Number of minutes before this cookie goes stale :param kill: Whether the the cookie should expire on arrival :return: A tuple to be added to headers """""" if kill: ttl = -1 elif ttl < 0: ttl = self.default_value['max_age'] if cookie_name is None: cookie_name = self.default_value['name'] c_args = {} srvdomain = self.default_value['domain'] if srvdomain and srvdomain not in ['localhost', '127.0.0.1', '0.0.0.0']: c_args['domain'] = srvdomain srvpath = self.default_value['path'] if srvpath: c_args['path'] = srvpath # now timestamp = str(int(time.time())) # create cookie payload try: cookie_payload = ""::"".join([value, timestamp, typ]) except TypeError: cookie_payload = ""::"".join([value[0], timestamp, typ]) cookie = make_cookie( cookie_name, cookie_payload, self.sign_key, timestamp=timestamp, enc_key=self.enc_key, max_age=ttl, sign_alg=self.sign_alg, **c_args) return cookie",3
578,Python,create cookie,https://github.com/decryptus/httpdis/blob/5d198cdc5558f416634602689b3df2c8aeb34984/httpdis/httpdis.py#L760-L769,"def set_cookie(self, name, value = '', expires = 0, path = '/', domain = '', secure = False, http_only = False): cook = Cookie.SimpleCookie() cook[name] = value cook[name]['expires'] = expires cook[name]['path'] = path cook[name]['domain'] = domain cook[name]['secure'] = secure cook[name]['httponly'] = http_only self.send_header('Set-Cookie', cook.output(header = ''))",3
911,Python,create cookie,https://github.com/pypa/pipenv/blob/cae8d76c210b9777e90aab76e9c4b0e53bb19cde/pipenv/vendor/requests/cookies.py#L441-L474,"def create_cookie(name, value, **kwargs): """"""Make a cookie from underspecified parameters. By default, the pair of `name` and `value` will be set for the domain '' and sent on every request (this is sometimes called a ""supercookie""). """""" result = { 'version': 0, 'name': name, 'value': value, 'port': None, 'domain': '', 'path': '/', 'secure': False, 'expires': None, 'discard': True, 'comment': None, 'comment_url': None, 'rest': {'HttpOnly': None}, 'rfc2109': False, } badargs = set(kwargs) - set(result) if badargs: err = 'create_cookie() got unexpected keyword arguments: %s' raise TypeError(err % list(badargs)) result.update(kwargs) result['port_specified'] = bool(result['port']) result['domain_specified'] = bool(result['domain']) result['domain_initial_dot'] = result['domain'].startswith('.') result['path_specified'] = bool(result['path']) return cookielib.Cookie(**result)",3
1518,Python,create cookie,https://github.com/wecatch/app-turbo/blob/75faf97371a9a138c53f92168d0a486636cb8a9c/turbo/session.py#L185-L193,"def _set_cookie(self, name, value): cookie_domain = self._config.cookie_domain cookie_path = self._config.cookie_path cookie_expires = self._config.cookie_expires if self._config.secure: return self.handler.set_secure_cookie( name, value, expires_days=cookie_expires / (3600 * 24), domain=cookie_domain, path=cookie_path) else: return self.handler.set_cookie(name, value, expires=cookie_expires, domain=cookie_domain, path=cookie_path)",3
1780,Python,create cookie,https://github.com/pyGrowler/Growler/blob/90c923ff204f28b86a01d741224987a22f69540f/growler/middleware/cookieparser.py#L34-L56,"def __call__(self, req, res): """""" Parses cookies of the header request (using the 'cookie' header key) and adds a callback to the 'on_headerstrings' response event. """""" # Do not clobber cookies if hasattr(req, 'cookies'): return # Create an empty cookie state req.cookies, res.cookies = SimpleCookie(), SimpleCookie() log.info(""{:d} built with {}"", id(self), json.dumps(self.opts)) # If the request had a cookie, load it! req.cookies.load(req.headers.get('COOKIE', '')) def _gen_cookie(): if res.cookies: cookie_string = res.cookies.output(header='', sep=res.EOL) return cookie_string res.headers['Set-Cookie'] = _gen_cookie",3
1985,Python,create cookie,https://github.com/lorien/grab/blob/8b301db2a08c830245b61c589e58af6234f4db79/grab/cookie.py#L118-L152,"def create_cookie(name, value, domain, httponly=None, **kwargs): """"""Creates `cookielib.Cookie` instance"""""" if domain == 'localhost': domain = '' config = dict( name=name, value=value, version=0, port=None, domain=domain, path='/', secure=False, expires=None, discard=True, comment=None, comment_url=None, rfc2109=False, rest={'HttpOnly': httponly}, ) for key in kwargs: if key not in config: raise GrabMisuseError('Function `create_cookie` does not accept ' '`%s` argument' % key) config.update(**kwargs) config['rest']['HttpOnly'] = httponly config['port_specified'] = bool(config['port']) config['domain_specified'] = bool(config['domain']) config['domain_initial_dot'] = (config['domain'] or '').startswith('.') config['path_specified'] = bool(config['path']) return Cookie(**config)",3
411,Python,create cookie,https://github.com/IdentityPython/oidcendpoint/blob/6c1d729d51bfb6332816117fe476073df7a1d823/src/oidcendpoint/cookie.py#L369-L404,"def append_cookie(self, cookie, name, payload, typ, domain=None, path=None, timestamp="""", max_age=0): """""" Adds a cookie to a SimpleCookie instance :param cookie: :param name: :param payload: :param typ: :param domain: :param path: :param timestamp: :param max_age: :return: """""" timestamp = str(int(time.time())) # create cookie payload try: _payload = ""::"".join([payload, timestamp, typ]) except TypeError: _payload = ""::"".join([payload[0], timestamp, typ]) content = make_cookie_content(name, _payload, self.sign_key, domain=domain, path=path, timestamp=timestamp, enc_key=self.enc_key, max_age=max_age, sign_alg=self.sign_alg) for name, args in content.items(): cookie[name] = args['value'] for key, value in args.items(): if key == 'value': continue cookie[name][key] = value return cookie",2
1531,Python,create cookie,https://github.com/UCL-INGI/INGInious/blob/cbda9a9c7f2b8e8eb1e6d7d51f0d18092086300c/inginious/frontend/cookieless_app.py#L231-L237,"def _setcookie(self, session_id, expires='', **kw): cookie_name = self._config.cookie_name cookie_domain = self._config.cookie_domain cookie_path = self._config.cookie_path httponly = self._config.httponly secure = self._config.secure web.setcookie(cookie_name, session_id, expires=expires, domain=cookie_domain, httponly=httponly, secure=secure, path=cookie_path)",2
95,Python,copying a file to a path,https://github.com/galaxyproject/pulsar/blob/9ab6683802884324652da0a9f0808c7eb59d3ab4/pulsar/client/client.py#L392-L395,"def _copy(from_path, to_path): message = ""Copying path [%s] to [%s]"" log.debug(message, from_path, to_path) copy(from_path, to_path)",3
97,Python,copying a file to a path,https://github.com/divio/django-filer/blob/946629087943d41eff290f07bfdf240b8853dd88/filer/admin/folderadmin.py#L1028-L1045,"def _copy_file(self, file_obj, destination, suffix, overwrite): if overwrite: # Not yet implemented as we have to find a portable (for different storage backends) way to overwrite files raise NotImplementedError # We are assuming here that we are operating on an already saved database objects with current database state available filename = self._generate_new_filename(file_obj.file.name, suffix) # Due to how inheritance works, we have to set both pk and id to None file_obj.pk = None file_obj.id = None file_obj.save() file_obj.folder = destination file_obj._file_data_changed_hint = False # no need to update size, sha1, etc. file_obj.file = file_obj._copy_file(filename) file_obj.original_filename = self._generate_new_filename(file_obj.original_filename, suffix) file_obj.save()",3
103,Python,copying a file to a path,https://github.com/frictionlessdata/datapackage-pipelines/blob/3a34bbdf042d13c3bec5eef46ff360ee41403874/datapackage_pipelines/lib/dump/to_path.py#L16-L25,"def write_file_to_output(self, filename, path): path = os.path.join(self.out_path, path) # Avoid rewriting existing files if self.add_filehash_to_path and os.path.exists(path): return path_part = os.path.dirname(path) PathDumper.__makedirs(path_part) shutil.copy(filename, path) os.chmod(path, 0o666) return path",3
396,Python,copying a file to a path,https://github.com/klen/starter/blob/24a65c10d4ac5a9ca8fc1d8b3d54b3fb13603f5f/starter/core.py#L52-L58,"def copy_file(self, from_path, to_path): """""" Copy file. """""" if not op.exists(op.dirname(to_path)): self.make_directory(op.dirname(to_path)) shutil.copy(from_path, to_path) logging.debug('File copied: {0}'.format(to_path))",3
522,Python,copying a file to a path,https://github.com/OLC-Bioinformatics/sipprverse/blob/d4f10cdf8e1a39dac0953db61c21c97efc6006de/cgecore/utility.py#L534-L574,"def copy_file(src, dst, ignore=None): """""" this function will simply copy the file from the source path to the dest path given as input """""" # Sanity checkpoint src = re.sub('[^\w/\-\.\*]', '', src) dst = re.sub('[^\w/\-\.\*]', '', dst) if len(re.sub('[\W]', '', src)) < 5 or len(re.sub('[\W]', '', dst)) < 5: debug.log(""Error: Copying file failed. Provided paths are invalid! src='%s' dst='%s'""%(src, dst)) else: # Check destination check = False if dst[-1] == '/': if os.path.exists(dst): check = True # Valid Dir else: debug.log(""Error: Copying file failed. Destination directory does not exist (%s)""%(dst)) #DEBUG elif os.path.exists(dst): if os.path.isdir(dst): check = True # Valid Dir dst += '/' # Add missing slash else: debug.log(""Error: Copying file failed. %s exists!""%dst) elif os.path.exists(os.path.dirname(dst)): check = True # Valid file path else: debug.log(""Error: Copying file failed. %s is an invalid distination!""%dst) if check: # Check source files = glob.glob(src) if ignore is not None: files = [fil for fil in files if not ignore in fil] if len(files) != 0: debug.log(""Copying File(s)..."", ""Copy from %s""%src, ""to %s""%dst) #DEBUG for file_ in files: # Check file exists if os.path.isfile(file_): debug.log(""Copying file: %s""%file_) #DEBUG shutil.copy(file_, dst) else: debug.log(""Error: Copying file failed. %s is not a regular file!""%file_) #DEBUG else: debug.log(""Error: Copying file failed. No files were found! (%s)""%src) #DEBUG",3
410,Python,copying a file to a path,https://github.com/RedHatInsights/insights-core/blob/b57cbf8ed7c089672426ede0441e0a4f789ef4a1/insights/client/archive.py#L93-L104,"def copy_dir(self, path): """""" Recursively copy directory """""" for directory in path: if os.path.isdir(path): full_path = os.path.join(self.archive_dir, directory.lstrip('/')) logger.debug(""Copying %s to %s"", directory, full_path) shutil.copytree(directory, full_path) else: logger.debug(""Not a directory: %s"", directory) return path",2
561,Python,copying a file to a path,https://github.com/Neurita/boyle/blob/2dae7199849395a209c887d5f30506e1de8a9ad9/scripts/filetree.py#L20-L72,"def copy(configfile='', destpath='', overwrite=False, sub_node=''): """"""Copies the files in the built file tree map to despath. :param configfile: string Path to the FileTreeMap config file :param destpath: string Path to the files destination :param overwrite: bool Overwrite files if they already exist. :param sub_node: string Tree map configuration sub path. Will copy only the contents within this sub-node """""" log.info('Running {0} {1} {2}'.format(os.path.basename(__file__), whoami(), locals())) assert(os.path.isfile(configfile)) if os.path.exists(destpath): if os.listdir(destpath): raise FolderAlreadyExists('Folder {0} already exists. Please clean ' 'it or change destpath.'.format(destpath)) else: log.info('Creating folder {0}'.format(destpath)) path(destpath).makedirs_p() from boyle.files.file_tree_map import FileTreeMap file_map = FileTreeMap() try: file_map.from_config_file(configfile) except Exception as e: raise FileTreeMapError(str(e)) if sub_node: sub_map = file_map.get_node(sub_node) if not sub_map: raise FileTreeMapError('Could not find sub node ' '{0}'.format(sub_node)) file_map._filetree = {} file_map._filetree[sub_node] = sub_map try: file_map.copy_to(destpath, overwrite=overwrite) except Exception as e: raise FileTreeMapError(str(e))",2
843,Python,copying a file to a path,https://github.com/joelfrederico/SciSalt/blob/7bf57c49c7dde0a8b0aa337fbd2fbd527ce7a67f/scisalt/facettools/print2elog.py#L13-L23,"def _copy_file(filepath, fulltime): if filepath is None: filepath_out = '' else: filename = _os.path.basename(filepath) root, ext = _os.path.splitext(filename) filepath_out = fulltime + ext copypath = _os.path.join(basedir, filepath_out) _shutil.copyfile(filepath, copypath) return filepath_out",2
1200,Python,copying a file to a path,https://github.com/tjguk/winshell/blob/1509d211ab3403dd1cff6113e4e13462d6dec35b/winshell.py#L266-L294,"def copy_file( source_path, target_path, allow_undo=True, no_confirm=False, rename_on_collision=True, silent=False, extra_flags=0, hWnd=None ): """"""Perform a shell-based file copy. Copying in this way allows the possibility of undo, auto-renaming, and showing the ""flying file"" animation during the copy. The default options allow for undo, don't automatically clobber on a name clash, automatically rename on collision and display the animation. """""" return _file_operation( shellcon.FO_COPY, source_path, target_path, allow_undo, no_confirm, rename_on_collision, silent, extra_flags, hWnd )",2
235,Python,copying a file to a path,https://github.com/dsoprea/PathManifest/blob/0f5cfd4925a61cc0eac150ff354200392d07ec74/pm/manifest.py#L239-L271,"def __inject_files_to_staging(self, rel_filepaths, temp_path): patch_files = {} for rel_filepath in rel_filepaths: from_filepath = os.path.join(self.__root_path, rel_filepath) to_filepath = os.path.join(temp_path, rel_filepath) _LOGGER.debug(""Copying file to patch path: [%s] => [%s]"", from_filepath, to_filepath) to_path = os.path.dirname(to_filepath) if os.path.exists(to_path) is False: os.makedirs(to_path) with open(from_filepath, 'rb') as f: with open(to_filepath, 'wb') as g: shutil.copyfileobj(f, g) s = os.stat(from_filepath) mtime_epoch = int(s.st_mtime) filesize_b = s.st_size # Set patch mtime. os.utime(to_filepath, (mtime_epoch, mtime_epoch)) hash_ = self.__get_md5_for_rel_filepath(rel_filepath) patch_files[rel_filepath] = { 'mtime_epoch': mtime_epoch, 'filesize_b': filesize_b, 'hash_md5': hash_, } return patch_files",1
66,Python,copy to clipboard,https://github.com/ctuning/ck/blob/7e009814e975f8742790d3106340088a46223714/ck/kernel.py#L1770-L1830,"def copy_to_clipboard(i): # pragma: no cover """""" Input: { string - string to copy Output: { return - return code = 0, if successful > 0, if error (error) - error text if return > 0 } """""" s=i['string'] failed=False ee='' # Try to load pyperclip (seems to work fine on Windows) try: import pyperclip except Exception as e: ee=format(e) failed=True pass if not failed: pyperclip.copy(s) else: failed=False # Try to load Tkinter try: from Tkinter import Tk except ImportError as e: ee=format(e) failed=True pass if failed: failed=False try: from tkinter import Tk except ImportError as e: ee=format(e) failed=True pass if failed: return {'return':1, 'error':'none of pyperclip/Tkinter/tkinter packages is installed'} # Copy to clipboard try: r = Tk() r.withdraw() r.clipboard_clear() r.clipboard_append(s) r.destroy() except Exception as e: return {'return':1, 'error':'problem copying string to clipboard ('+format(e)+')'} return {'return':0}",3
302,Python,copy to clipboard,https://github.com/zeekay/soundcloud-cli/blob/8a83013683e1acf32f093239bbb6d3c02bc50b37/soundcloud_cli/utils.py#L7-L23,"def copy_to_clipboard(text): # reliable on mac if sys.platform == 'darwin': os.system('echo ""{0}"" | pbcopy'.format(text)) return # okay we'll try cross-platform way try: from Tkinter import Tk except ImportError: return r = Tk() r.withdraw() r.clipboard_clear() r.clipboard_append(text.encode('ascii')) r.destroy()",3
468,Python,copy to clipboard,https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/variableexplorer/widgets/collectionseditor.py#L1242-L1252,"def paste(self): """"""Import text/data/code from clipboard"""""" clipboard = QApplication.clipboard() cliptext = '' if clipboard.mimeData().hasText(): cliptext = to_text_string(clipboard.text()) if cliptext.strip(): self.import_from_string(cliptext, title=_(""Import from clipboard"")) else: QMessageBox.warning(self, _( ""Empty clipboard""), _(""Nothing to be imported from clipboard.""))",3
785,Python,copy to clipboard,https://github.com/bitesofcode/projexui/blob/f18a73bec84df90b034ca69b9deea118dbedfc4d/projexui/widgets/xfilepathedit.py#L136-L142,"def copyFilepath( self ): """""" Copies the current filepath contents to the current clipboard. """""" clipboard = QApplication.instance().clipboard() clipboard.setText(self.filepath()) clipboard.setText(self.filepath(), clipboard.Selection)",3
1109,Python,copy to clipboard,https://github.com/jedie/DragonPy/blob/6659e5b5133aab26979a498ee7453495773a4f6c/dragonpy/components/periphery.py#L163-L168,"def copy_to_clipboard(self, event): log.critical(""Copy to clipboard"") text = self.text.get(""1.0"", tkinter.END) print(text) self.root.clipboard_clear() self.root.clipboard_append(text)",3
1566,Python,copy to clipboard,https://github.com/Guake/guake/blob/4153ef38f9044cbed6494075fce80acd5809df2b/guake/terminal.py#L136-L141,"def copy_clipboard(self): if self.get_has_selection(): super(GuakeTerminal, self).copy_clipboard() elif self.matched_value: guake_clipboard = Gtk.Clipboard.get_default(self.guake.window.get_display()) guake_clipboard.set_text(self.matched_value, len(self.matched_value))",3
1742,Python,copy to clipboard,https://github.com/devassistant/devassistant/blob/2dbfeaa666a64127263664d18969c55d19ecc83e/devassistant/gui/gui_helper.py#L466-L473,"def create_clipboard(self, text, selection=Gdk.SELECTION_CLIPBOARD): """""" Function creates a clipboard """""" clipboard = Gtk.Clipboard.get(selection) clipboard.set_text('\n'.join(text), -1) clipboard.store() return clipboard",3
709,Python,copy to clipboard,https://github.com/nerdvegas/rez/blob/1d3b846d53b5b5404edfe8ddb9083f9ceec8c5e7/src/rezgui/windows/ContextSubWindow.py#L103-L108,"def copy_request_to_clipboard(self): txt = "" "".join(self.context_model.request) clipboard = app.clipboard() clipboard.setText(txt) with app.status(""Copied request to clipboard""): pass",2
1151,Python,copy to clipboard,https://github.com/spyder-ide/spyder/blob/f76836ce1b924bcc4efd3f74f2960d26a4e528e0/spyder/plugins/variableexplorer/widgets/arrayeditor.py#L526-L530,"def copy(self): """"""Copy text to clipboard"""""" cliptxt = self._sel_to_text( self.selectedIndexes() ) clipboard = QApplication.clipboard() clipboard.setText(cliptxt)",1
69,Python,converting uint8 array to image,https://github.com/dade-ai/snipy/blob/408520867179f99b3158b57520e2619f3fecd69b/snipy/img/imageutil.py#L479-L482,def _convert_uint8(im): if im.dtype != np.uint8: im = np.uint8(im * 255) return im,3
1099,Python,converting uint8 array to image,https://github.com/pyviz/holoviews/blob/ae0dd2f3de448b0ca5e9065aabd6ef8d84c7e655/holoviews/operation/datashader.py#L1058-L1069,"def uint8_to_uint32(self, element): img = np.dstack([element.dimension_values(d, flat=False) for d in element.vdims]) if img.shape[2] == 3: # alpha channel not included alpha = np.ones(img.shape[:2]) if img.dtype.name == 'uint8': alpha = (alpha*255).astype('uint8') img = np.dstack([img, alpha]) if img.dtype.name != 'uint8': img = (img*255).astype(np.uint8) N, M, _ = img.shape return img.view(dtype=np.uint32).reshape((N, M))",3
1991,Python,converting uint8 array to image,https://github.com/radjkarl/imgProcessor/blob/7c5a28718f81c01a430152c60a686ac50afbfd7c/imgProcessor/transformations.py#L11-L75,"def toUIntArray(img, dtype=None, cutNegative=True, cutHigh=True, range=None, copy=True): ''' transform a float to an unsigned integer array of a fitting dtype adds an offset, to get rid of negative values range = (min, max) - scale values between given range cutNegative - all values <0 will be set to 0 cutHigh - set to False to rather scale values to fit ''' mn, mx = None, None if range is not None: mn, mx = range if dtype is None: if mx is None: mx = np.nanmax(img) dtype = np.uint16 if mx > 255 else np.uint8 dtype = np.dtype(dtype) if dtype == img.dtype: return img # get max px value: b = {'uint8': 255, 'uint16': 65535, 'uint32': 4294967295, 'uint64': 18446744073709551615}[dtype.name] if copy: img = img.copy() if range is not None: img = np.asfarray(img) img -= mn # img[img<0]=0 # print np.nanmin(img), np.nanmax(img), mn, mx, range, b img *= b / (mx - mn) # print np.nanmin(img), np.nanmax(img), mn, mx, range, b img = np.clip(img, 0, b) else: if cutNegative: img[img < 0] = 0 else: # add an offset to all values: mn = np.min(img) if mn < 0: img -= mn # set minimum to 0 if cutHigh: #ind = img > b img[img > b] = b else: # scale values mx = np.nanmax(img) img = np.asfarray(img) * (float(b) / mx) img = img.astype(dtype) # if range is not None and cutHigh: # img[ind] = b return img",3
531,Python,converting uint8 array to image,https://github.com/BerkeleyAutomation/perception/blob/03d9b37dd6b66896cdfe173905c9413c8c3c5df6/perception/image.py#L252-L291,"def from_array(x, frame='unspecified'): """""" Converts an array of data to an Image based on the values in the array and the data format. """""" if not Image.can_convert(x): raise ValueError('Cannot convert array to an Image!') dtype = x.dtype height = x.shape[0] width = x.shape[1] channels = 1 if len(x.shape) == 3: channels = x.shape[2] if dtype == np.uint8: if channels == 1: if np.any((x % BINARY_IM_MAX_VAL) > 0): return GrayscaleImage(x, frame) return BinaryImage(x, frame) elif channels == 3: return ColorImage(x, frame) else: raise ValueError( 'No available image conversion for uint8 array with 2 channels') elif dtype == np.uint16: if channels != 1: raise ValueError( 'No available image conversion for uint16 array with 2 or 3 channels') return GrayscaleImage(x, frame) elif dtype == np.float32 or dtype == np.float64: if channels == 1: return DepthImage(x, frame) elif channels == 2: return GdImage(x, frame) elif channels == 3: logging.warning('Converting float array to uint8') return ColorImage(x.astype(np.uint8), frame) return RgbdImage(x, frame) else: raise ValueError( 'Conversion for dtype %s not supported!' % (str(dtype)))",2
810,Python,converting uint8 array to image,https://github.com/aleju/imgaug/blob/786be74aa855513840113ea523c5df495dc6a8af/imgaug/augmentables/heatmaps.py#L392-L406,"def to_uint8(self): """""" Convert this heatmaps object to a 0-to-255 array. Returns ------- arr_uint8 : (H,W,C) ndarray Heatmap as a 0-to-255 array (dtype is uint8). """""" # TODO this always returns (H,W,C), even if input ndarray was originall (H,W) # does it make sense here to also return (H,W) if self.arr_was_2d? arr_0to255 = np.clip(np.round(self.arr_0to1 * 255), 0, 255) arr_uint8 = arr_0to255.astype(np.uint8) return arr_uint8",2
1104,Python,converting uint8 array to image,https://github.com/wandb/client/blob/7d08954ed5674fee223cd85ed0d8518fe47266b2/wandb/data_types.py#L727-L746,"def to_uint8(self, data): """""" Converts floating point image on the range [0,1] and integer images on the range [0,255] to uint8, clipping if necessary. """""" np = util.get_module( ""numpy"", required=""wandb.Image requires numpy if not supplying PIL Images: pip install numpy"") # I think it's better to check the image range vs the data type, since many # image libraries will return floats between 0 and 255 # some images have range -1...1 or 0-1 dmin = np.min(data) if dmin < 0: data = (data - np.min(data)) / np.ptp(data) if np.max(data) <= 1.0: data = (data * 255).astype(np.int32) #assert issubclass(data.dtype.type, np.integer), 'Illegal image format.' return data.clip(0, 255).astype(np.uint8)",2
2033,Python,converting uint8 array to image,https://github.com/radjkarl/imgProcessor/blob/7c5a28718f81c01a430152c60a686ac50afbfd7c/imgProcessor/imgIO.py#L39-L73,"def imread(img, color=None, dtype=None): ''' dtype = 'noUint', uint8, float, 'float', ... ''' COLOR2CV = {'gray': cv2.IMREAD_GRAYSCALE, 'all': cv2.IMREAD_COLOR, None: cv2.IMREAD_ANYCOLOR } c = COLOR2CV[color] if callable(img): img = img() elif isinstance(img, string_types): # from_file = True # try: # ftype = img[img.find('.'):] # img = READERS[ftype](img)[0] # except KeyError: # open with openCV # grey - 8 bit if dtype in (None, ""noUint"") or np.dtype(dtype) != np.uint8: c |= cv2.IMREAD_ANYDEPTH img2 = cv2.imread(img, c) if img2 is None: raise IOError(""image '%s' is not existing"" % img) img = img2 elif color == 'gray' and img.ndim == 3: # multi channel img like rgb # cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #cannot handle float64 img = toGray(img) # transform array to uint8 array due to openCV restriction if dtype is not None: if isinstance(img, np.ndarray): img = _changeArrayDType(img, dtype, cutHigh=False) return img",2
137,Python,converting uint8 array to image,https://github.com/pyviz/holoviews/blob/ae0dd2f3de448b0ca5e9065aabd6ef8d84c7e655/holoviews/operation/datashader.py#L932-L936,"def uint32_to_uint8(cls, img): """""" Cast uint32 RGB image to 4 uint8 channels. """""" return np.flipud(img.view(dtype=np.uint8).reshape(img.shape + (4,)))",1
207,Python,converting uint8 array to image,https://github.com/sony/nnabla/blob/aaf3d33b7cbb38f2a03aa754178ba8f7c8481320/python/src/nnabla/utils/image_utils/common.py#L62-L66,"def upscale_float_image(img, as_uint16): if as_uint16: return np.asarray((img * 65535), np.uint16) return np.asarray((img * 255), np.uint8)",1
275,Python,converting uint8 array to image,https://github.com/quintusdias/glymur/blob/8b8fb091130fff00f1028dc82219e69e3f9baf6d/glymur/jp2k.py#L660-L667,"def _validate_image_datatype(self, img_array): """""" Only uint8 and uint16 images are currently supported. """""" if img_array.dtype != np.uint8 and img_array.dtype != np.uint16: msg = (""Only uint8 and uint16 datatypes are currently supported "" ""when writing."") raise RuntimeError(msg)",1
956,Python,convert string to number,https://github.com/shidenggui/easytrader/blob/e5ae4daeda4ea125763a95b280dd694c7f68257d/easytrader/helpers.py#L137-L139,"def str2num(num_str, convert_type=""float""): num = float(grep_comma(num_str)) return num if convert_type == ""float"" else int(num)",3
1100,Python,convert string to number,https://github.com/nickmckay/LiPD-utilities/blob/5dab6bbeffc5effd68e3a6beaca6b76aa928e860/Python/lipd/noaa_lpd.py#L546-L556,"def __convert_num(number): """""" All path items are automatically strings. If you think it's an int or float, this attempts to convert it. :param str number: :return float or str: """""" try: return float(number) except ValueError as e: logger_noaa_lpd.warn(""convert_num: ValueError: {}"".format(e)) return number",3
1444,Python,convert string to number,https://github.com/gunthercox/ChatterBot/blob/1a03dcb45cba7bdc24d3db5e750582e0cb1518e2/chatterbot/parsing.py#L506-L517,"def convert_string_to_number(value): """""" Convert strings to numbers """""" if value is None: return 1 if isinstance(value, int): return value if value.isdigit(): return int(value) num_list = map(lambda s: NUMBERS[s], re.findall(numbers + '+', value.lower())) return sum(num_list)",3
1087,Python,convert string to number,https://github.com/pasztorpisti/json-cfg/blob/4627b14a92521ef8a39bbedaa7af8d380d406d07/src/jsoncfg/tree_python.py#L61-L69,"def default_number_converter(number_str): """""" Converts the string representation of a json number into its python object equivalent, an int, long, float or whatever type suits. """""" is_int = (number_str.startswith('-') and number_str[1:].isdigit()) or number_str.isdigit() # FIXME: this handles a wider range of numbers than allowed by the json standard, # etc.: float('nan') and float('inf'). But is this a problem? return int(number_str) if is_int else float(number_str)",2
1461,Python,convert string to number,https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/pipeline/disambiguate/run.py#L35-L39,"def nat_cmp(a, b): convert = lambda text: int(text) if text.isdigit() else text # lambda function to convert text to int if number present alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] # split string to piecewise strings and string numbers #return cmp(alphanum_key(a), alphanum_key(b)) # use internal cmp to compare piecewise strings and numbers return (alphanum_key(a) > alphanum_key(b))-(alphanum_key(a) < alphanum_key(b))",2
154,Python,convert string to number,https://github.com/google/grumpy/blob/3ec87959189cfcdeae82eb68a47648ac25ceb10b/third_party/pypy/_struct.py#L101-L108,"def pack_unsigned_int(number, size, le): if not isinstance(number, int): raise StructError(""argument for i,I,l,L,q,Q,h,H must be integer"") if number < 0: raise TypeError(""can't convert negative long to unsigned"") if number > (1 << (8 * size)) - 1: raise OverflowError(""Number:%i too large to convert"" % number) return pack_int(number, size, le)",1
394,Python,convert string to number,https://github.com/klen/zeta-library/blob/b76f89000f467e10ddcc94aded3f6c6bf4a0e5bd/zetalibrary/scss/__init__.py#L1898-L1910,"def to_str(num): if isinstance(num, dict): s = sorted(num.items()) sp = num.get('_', '') return (sp + ' ').join(to_str(v) for n, v in s if n != '_') elif isinstance(num, float): num = ('%0.03f' % round(num, 3)).rstrip('0').rstrip('.') return num elif isinstance(num, bool): return 'true' if num else 'false' elif num is None: return '' return str(num)",1
1666,Python,convert string to number,https://github.com/lawsie/guizero/blob/84c7f0b314fa86f9fc88eb11c9a0f6c4b57155e2/examples/binary_counter.py#L6-L11,"def to_str(n,base): convert_string = ""0123456789ABCDEF"" if n < base: return convert_string[n] else: return to_str(n//base,base) + convert_string[n%base]",1
612,Python,convert string to number,https://github.com/LettError/MutatorMath/blob/10318fc4e7c9cee9df6130826829baea3054a42b/Lib/mutatorMath/objects/location.py#L14-L29,"def numberToString(value): # return a nicely formatted string of this value # return tuples as a tuple-looking string with formatted numbers # return ints as ints, no commas # return floats as compact rounded value if value is None: return ""None"" if type(value)==tuple: t = [] for v in value: t.append(numberToString(v)) return ""(%s)""%("","".join(t)) if int(value) == value: # it is an int return ""%d""%(value) return ""%3.3f""%value",0
713,Python,convert string to number,https://github.com/balemessenger/bale-bot-python/blob/92bfd60016b075179f16c212fc3fc20a4e5f16f4/balebot/utils/util_functions.py#L70-L75,"def standardize_phone_number(number): number_str = str(number) if number_str.startswith(""0098""): return ""+98"" + number_str[4:] elif number_str.startswith(""0""): return ""+98"" + number_str[1:]",0
253,Python,convert json to csv,https://github.com/mosesschwartz/scrypture/blob/d51eb0c9835a5122a655078268185ce8ab9ec86a/scrypture/demo_scripts/Utils/json_to_csv.py#L28-L47,"def json_to_csv(json_input): ''' Convert simple JSON to CSV Accepts a JSON string or JSON object ''' try: json_input = json.loads(json_input) except: pass # If loads fails, it's probably already parsed headers = set() for json_row in json_input: headers.update(json_row.keys()) csv_io = StringIO.StringIO() csv_out = csv.DictWriter(csv_io,headers) csv_out.writeheader() for json_row in json_input: csv_out.writerow(json_row) csv_io.seek(0) return csv_io.read()",3
1577,Python,convert json to csv,https://github.com/poldracklab/niworkflows/blob/254f4b4fcc5e6ecb29d2f4602a30786b913ecce5/niworkflows/interfaces/utils.py#L793-L864,"def _tsv2json(in_tsv, out_json, index_column, additional_metadata=None, drop_columns=None, enforce_case=True): """""" Convert metadata from TSV format to JSON format. Parameters ---------- in_tsv: str Path to the metadata in TSV format. out_json: str Path where the metadata should be saved in JSON format after conversion. If this is None, then a dictionary is returned instead. index_column: str Name of the column in the TSV to be used as an index (top-level key in the JSON). additional_metadata: dict Any additional metadata that should be applied to all entries in the JSON. drop_columns: list List of columns from the input TSV to be dropped from the JSON. enforce_case: bool Indicates whether BIDS case conventions should be followed. Currently, this means that index fields (column names in the associated data TSV) use snake case and other fields use camel case. Returns ------- str Path to the metadata saved in JSON format. """""" import pandas as pd # Adapted from https://dev.to/rrampage/snake-case-to-camel-case-and- ... # back-using-regular-expressions-and-python-m9j re_to_camel = r'(.*?)_([a-zA-Z0-9])' re_to_snake = r'(^.+?|.*?)((?<![_A-Z])[A-Z]|(?<![_0-9])[0-9]+)' def snake(match): return '{}_{}'.format(match.group(1).lower(), match.group(2).lower()) def camel(match): return '{}{}'.format(match.group(1), match.group(2).upper()) # from fmriprep def less_breakable(a_string): """""" hardens the string to different envs (i.e. case insensitive, no whitespace, '#' """""" return ''.join(a_string.split()).strip('#') drop_columns = drop_columns or [] additional_metadata = additional_metadata or {} tsv_data = pd.read_csv(in_tsv, '\t') for k, v in additional_metadata.items(): tsv_data[k] = v for col in drop_columns: tsv_data.drop(labels=col, axis='columns', inplace=True) tsv_data.set_index(index_column, drop=True, inplace=True) if enforce_case: tsv_data.index = [re.sub(re_to_snake, snake, less_breakable(i), 0).lower() for i in tsv_data.index] tsv_data.columns = [re.sub(re_to_camel, camel, less_breakable(i).title(), 0) for i in tsv_data.columns] json_data = tsv_data.to_json(orient='index') json_data = json.JSONDecoder( object_pairs_hook=OrderedDict).decode(json_data) if out_json is None: return json_data with open(out_json, 'w') as f: json.dump(json_data, f, indent=4) return out_json",2
229,Python,convert json to csv,https://github.com/nickmckay/LiPD-utilities/blob/5dab6bbeffc5effd68e3a6beaca6b76aa928e860/Python/lipd/jsons.py#L231-L247,"def get_csv_from_json(d): """""" Get CSV values when mixed into json data. Pull out the CSV data and put it into a dictionary. :param dict d: JSON with CSV values :return dict: CSV values. (i.e. { CSVFilename1: { Column1: [Values], Column2: [Values] }, CSVFilename2: ... } """""" logger_jsons.info(""enter get_csv_from_json"") csv_data = OrderedDict() if ""paleoData"" in d: csv_data = _get_csv_from_section(d, ""paleoData"", csv_data) if ""chronData"" in d: csv_data = _get_csv_from_section(d, ""chronData"", csv_data) logger_jsons.info(""exit get_csv_from_json"") return csv_data",1
734,Python,convert json to csv,https://github.com/PythonSanSebastian/docstamp/blob/b43808f2e15351b0b2f0b7eade9c7ef319c9e646/docstamp/file_utils.py#L138-L170,"def csv_to_json(csv_filepath, json_filepath, fieldnames, ignore_first_line=True): """""" Convert a CSV file in `csv_filepath` into a JSON file in `json_filepath`. Parameters ---------- csv_filepath: str Path to the input CSV file. json_filepath: str Path to the output JSON file. Will be overwritten if exists. fieldnames: List[str] Names of the fields in the CSV file. ignore_first_line: bool """""" import csv import json csvfile = open(csv_filepath, 'r') jsonfile = open(json_filepath, 'w') reader = csv.DictReader(csvfile, fieldnames) rows = [] if ignore_first_line: next(reader) for row in reader: rows.append(row) json.dump(rows, jsonfile) jsonfile.close() csvfile.close()",1
1519,Python,convert json to csv,https://github.com/oplatek/csv2json/blob/f2f95db71ba2ce683fd6d0d3e2f13c9d0a77ceb6/csv2json/__init__.py#L20-L51,"def convert(csv, json, **kwargs): '''Convert csv to json. csv: filename or file-like object json: filename or file-like object if csv is '-' or None: stdin is used for input if json is '-' or None: stdout is used for output ''' csv_local, json_local = None, None try: if csv == '-' or csv is None: csv = sys.stdin elif isinstance(csv, str): csv = csv_local = open(csv, 'r') if json == '-' or json is None: json = sys.stdout elif isinstance(json, str): json = json_local = open(json, 'w') data = load_csv(csv, **kwargs) save_json(data, json, **kwargs) finally: if csv_local is not None: csv_local.close() if json_local is not None: json_local.close()",1
1660,Python,convert json to csv,https://github.com/albertyw/csv-ical/blob/cdb55a226cd0cb6cc214d896a6cea41a5b92c9ed/csv_ical/convert.py#L92-L97,"def save_csv(self, csv_location): # type: (str) -> None """""" Save the csv to a file """""" with open(csv_location, 'w') as csv_handle: writer = csv.writer(csv_handle) for row in self.csv_data: writer.writerow(row)",1
158,Python,convert json to csv,https://github.com/pybel/pybel/blob/c8a7a1bdae4c475fa2a8c77f3a9a5f6d79556ca0/src/pybel/cli.py#L222-L246,"def serialize(graph: BELGraph, csv, sif, gsea, graphml, json, bel): """"""Serialize a graph to various formats."""""" if csv: log.info('Outputting CSV to %s', csv) to_csv(graph, csv) if sif: log.info('Outputting SIF to %s', sif) to_sif(graph, sif) if graphml: log.info('Outputting GraphML to %s', graphml) to_graphml(graph, graphml) if gsea: log.info('Outputting GRP to %s', gsea) to_gsea(graph, gsea) if json: log.info('Outputting JSON to %s', json) to_json_file(graph, json) if bel: log.info('Outputting BEL to %s', bel) to_bel(graph, bel)",0
617,Python,convert json to csv,https://github.com/linkedin/naarad/blob/261e2c0760fd6a6b0ee59064180bd8e3674311fe/lib/luminol/src/luminol/utils.py#L40-L58,"def read_csv(csv_name): """""" Read data from a csv file into a dictionary. :param str csv_name: path to a csv file. :return dict: a dictionary represents the data in file. """""" data = {} if not isinstance(csv_name, (str, unicode)): raise exceptions.InvalidDataFormat('luminol.utils: csv_name has to be a string!') with open(csv_name, 'r') as csv_data: reader = csv.reader(csv_data, delimiter=',', quotechar='|') for row in reader: try: key = to_epoch(row[0]) value = float(row[1]) data[key] = value except ValueError: pass return data",0
1043,Python,convert json to csv,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mysql_to_gcs.py#L144-L199,"def _write_local_data_files(self, cursor): """""" Takes a cursor, and writes results to a local file. :return: A dictionary where keys are filenames to be used as object names in GCS, and values are file handles to local files that contain the data for the GCS objects. """""" schema = list(map(lambda schema_tuple: schema_tuple[0], cursor.description)) col_type_dict = self._get_col_type_dict() file_no = 0 tmp_file_handle = NamedTemporaryFile(delete=True) if self.export_format == 'csv': file_mime_type = 'text/csv' else: file_mime_type = 'application/json' files_to_upload = [{ 'file_name': self.filename.format(file_no), 'file_handle': tmp_file_handle, 'file_mime_type': file_mime_type }] if self.export_format == 'csv': csv_writer = self._configure_csv_file(tmp_file_handle, schema) for row in cursor: # Convert datetime objects to utc seconds, and decimals to floats. # Convert binary type object to string encoded with base64. row = self._convert_types(schema, col_type_dict, row) if self.export_format == 'csv': csv_writer.writerow(row) else: row_dict = dict(zip(schema, row)) # TODO validate that row isn't > 2MB. BQ enforces a hard row size of 2MB. s = json.dumps(row_dict, sort_keys=True).encode('utf-8') tmp_file_handle.write(s) # Append newline to make dumps BigQuery compatible. tmp_file_handle.write(b'\n') # Stop if the file exceeds the file size limit. if tmp_file_handle.tell() >= self.approx_max_file_size_bytes: file_no += 1 tmp_file_handle = NamedTemporaryFile(delete=True) files_to_upload.append({ 'file_name': self.filename.format(file_no), 'file_handle': tmp_file_handle, 'file_mime_type': file_mime_type }) if self.export_format == 'csv': csv_writer = self._configure_csv_file(tmp_file_handle, schema) return files_to_upload",0
301,Python,convert int to string,https://github.com/espressif/esptool/blob/c583756c118039cfcfe256f7a3285618914d16a5/ecdsa/ecdsa.py#L169-L175,"def string_to_int( s ): """"""Convert a string of bytes into an integer, as per X9.62."""""" result = 0 for c in s: if not isinstance(c, int): c = ord( c ) result = 256 * result + c return result",3
648,Python,convert int to string,https://github.com/DeepHorizons/iarm/blob/b913c9fd577b793a6bbced78b78a5d8d7cd88de4/iarm/arm_instructions/_meta.py#L126-L132,"def convert_to_integer(self, str): if str.startswith('0x') or str.startswith('0X'): return int(str, 16) elif str.startswith('2_'): return int(str[2:], 2) else: return int(str)",3
393,Python,convert int to string,https://github.com/kpdyer/regex2dfa/blob/109f877e60ef0dfcb430f11516d215930b7b9936/third_party/re2/re2/unicode.py#L26-L45,"def _UInt(s): """"""Converts string to Unicode code point ('263A' => 0x263a). Args: s: string to convert Returns: Unicode code point Raises: InputError: the string is not a valid Unicode value. """""" try: v = int(s, 16) except ValueError: v = -1 if len(s) < 4 or len(s) > 6 or v < 0 or v > _RUNE_MAX: raise InputError(""invalid Unicode value %s"" % (s,)) return v",1
1113,Python,convert int to string,https://github.com/osrg/ryu/blob/6f906e72c92e10bd0264c9b91a2f7bb85b97780c/ryu/lib/ofctl_utils.py#L206-L210,"def to_match_packet_type(value): if isinstance(value, (list, tuple)): return str_to_int(value[0]) << 16 | str_to_int(value[1]) else: return str_to_int(value)",1
1134,Python,convert int to string,https://github.com/MozillaSecurity/laniakea/blob/7e80adc6ae92c6c1332d4c08473bb271fb3b6833/laniakea/core/userdata.py#L43-L51,"def convert_str_to_int(arg): """""" """""" for k, v in list(arg.items()): # pylint: disable=invalid-name try: arg[String(k)] = int(v) except ValueError: pass return arg",1
1150,Python,convert int to string,https://github.com/commonwealth-of-puerto-rico/libre/blob/5b32f4ab068b515d2ea652b182e161271ba874e8/libre/apps/data_drivers/models.py#L341-L369,"def _convert_value(self, item): """""" Handle different value types for XLS. Item is a cell object. """""" # Types: # 0 = empty u'' # 1 = unicode text # 2 = float (convert to int if possible, then convert to string) # 3 = date (convert to unambiguous date/time string) # 4 = boolean (convert to string ""0"" or ""1"") # 5 = error (convert from code to error text) # 6 = blank u'' # Thx to Augusto C Men to point fast solution for XLS/XLSX dates if item.ctype == 3: # XL_CELL_DATE: try: return datetime.datetime(*xlrd.xldate_as_tuple(item.value, self._book.datemode)) except ValueError: # TODO: make toggable # Invalid date return item.value if item.ctype == 2: # XL_CELL_NUMBER: if item.value % 1 == 0: # integers return int(item.value) else: return item.value return item.value",1
1783,Python,convert int to string,https://github.com/osrg/ryu/blob/6f906e72c92e10bd0264c9b91a2f7bb85b97780c/ryu/lib/ofctl_utils.py#L198-L203,"def to_match_masked_int(value): if isinstance(value, str) and '/' in value: value = value.split('/') return str_to_int(value[0]), str_to_int(value[1]) return str_to_int(value)",1
219,Python,convert int to string,https://github.com/raphaelm/python-sepaxml/blob/187b699b1673c862002b2bae7e1bd62fe8623aec/sepaxml/utils.py#L64-L76,"def int_to_decimal_str(integer): """""" Helper to convert integers (representing cents) into decimal currency string. WARNING: DO NOT TRY TO DO THIS BY DIVISION, FLOATING POINT ERRORS ARE NO FUN IN FINANCIAL SYSTEMS. @param integer The amount in cents @return string The amount in currency with full stop decimal separator """""" int_string = str(integer) if len(int_string) < 2: return ""0."" + int_string.zfill(2) else: return int_string[:-2] + ""."" + int_string[-2:]",0
719,Python,convert int to string,https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/pipeline/disambiguate/run.py#L35-L39,"def nat_cmp(a, b): convert = lambda text: int(text) if text.isdigit() else text # lambda function to convert text to int if number present alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ] # split string to piecewise strings and string numbers #return cmp(alphanum_key(a), alphanum_key(b)) # use internal cmp to compare piecewise strings and numbers return (alphanum_key(a) > alphanum_key(b))-(alphanum_key(a) < alphanum_key(b))",0
986,Python,convert int to string,https://github.com/etingof/pysmi/blob/379a0a384c81875731be51a054bdacced6260fd8/pysmi/codegen/base.py#L290-L303,"def str2int(self, s): if self.isBinary(s): if s[1:-2]: return int(s[1:-2], 2) else: raise error.PySmiSemanticError('empty binary string to int conversion') elif self.isHex(s): if s[1:-2]: return int(s[1:-2], 16) else: raise error.PySmiSemanticError('empty hex string to int conversion') else: return int(s)",0
333,Python,convert int to bool,https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/convert.py#L43-L73,"def convert_to_bool(x: Any, default: bool = None) -> bool: """""" Transforms its input to a ``bool`` (or returns ``default`` if ``x`` is falsy but not itself a boolean). Accepts various common string versions. """""" if isinstance(x, bool): return x if not x: # None, zero, blank string... return default try: return int(x) != 0 except (TypeError, ValueError): pass try: return float(x) != 0 except (TypeError, ValueError): pass if not isinstance(x, str): raise Exception(""Unknown thing being converted to bool: {!r}"".format(x)) x = x.upper() if x in [""Y"", ""YES"", ""T"", ""TRUE""]: return True if x in [""N"", ""NO"", ""F"", ""FALSE""]: return False raise Exception(""Unknown thing being converted to bool: {!r}"".format(x))",3
581,Python,convert int to bool,https://github.com/Rapptz/discord.py/blob/05d4f7f9620ef33635d6ac965b26528e09cdaf5b/discord/ext/commands/core.py#L94-L101,"def _convert_to_bool(argument): lowered = argument.lower() if lowered in ('yes', 'y', 'true', 't', '1', 'enable', 'on'): return True elif lowered in ('no', 'n', 'false', 'f', '0', 'disable', 'off'): return False else: raise BadArgument(lowered + ' is not a recognised boolean option')",3
871,Python,convert int to bool,https://github.com/AustralianSynchrotron/lightflow/blob/dc53dbc1d961e20fb144273baca258060705c03e/lightflow/models/parameters.py#L62-L93,"def convert(self, value): """""" Convert the specified value to the type of the option. Args: value: The value that should be converted. Returns: The value with the type given by the option. """""" if self._type is str: return str(value) elif self._type is int: try: return int(value) except (UnicodeError, ValueError): raise WorkflowArgumentError('Cannot convert {} to int'.format(value)) elif self._type is float: try: return float(value) except (UnicodeError, ValueError): raise WorkflowArgumentError('Cannot convert {} to float'.format(value)) elif self._type is bool: if isinstance(value, bool): return bool(value) value = value.lower() if value in ('true', '1', 'yes', 'y'): return True elif value in ('false', '0', 'no', 'n'): return False raise WorkflowArgumentError('Cannot convert {} to bool'.format(value)) else: return value",3
1159,Python,convert int to bool,https://github.com/bigchaindb/bigchaindb/blob/835fdfcf598918f76139e3b88ee33dd157acaaa7/bigchaindb/commands/utils.py#L53-L76,"def _convert(value, default=None, convert=None): def convert_bool(value): if value.lower() in ('true', 't', 'yes', 'y'): return True if value.lower() in ('false', 'f', 'no', 'n'): return False raise ValueError('{} cannot be converted to bool'.format(value)) if value == '': value = None if convert is None: if default is not None: convert = type(default) else: convert = str if convert == bool: convert = convert_bool if value is None: return default else: return convert(value)",3
1338,Python,convert int to bool,https://github.com/thebjorn/pydeps/blob/1e6715b7bea47a40e8042821b57937deaaa0fdc3/pydeps/arguments.py#L20-L31,"def boolval(v): if isinstance(v, bool): return v if isinstance(v, int): return bool(v) if is_string(v): v = v.lower() if v in {'j', 'y', 'ja', 'yes', '1', 'true'}: return True if v in {'n', 'nei', 'no', '0', 'false'}: return False raise ValueError(""Don't know how to convert %r to bool"" % v)",3
1699,Python,convert int to bool,https://github.com/rberrelleza/511-transit/blob/ab676d6e3b57a073405cbfa2ffe1a57b85808fd1/fiveoneone/model.py#L18-L27,"def to_bool(self, value): if value == None: return False elif isinstance(value, bool): return value else: if str(value).lower() in [""true"", ""1"", ""yes""]: return True else: return False",3
800,Python,convert int to bool,https://github.com/msfrank/cifparser/blob/ecd899ba2e7b990e2cec62b115742d830e7e4384/cifparser/converters.py#L23-L29,"def str_to_bool(s): s = s.lower() if s in ('true', 'yes', '1'): return True if s in ('false', 'no', '0'): return False raise ConversionError(""failed to convert {0} to bool"".format(s))",2
878,Python,convert int to bool,https://github.com/yandex/yandex-tank/blob/d71d63b6ab5de8b8a5ea2b728b6ab9ac0b1ba71b/yandextank/config_converter/converter.py#L100-L104,def to_bool(value): try: return bool(int(value)) except ValueError: return True if 'true' == value.lower() else False,2
594,Python,convert int to bool,https://github.com/boriel/zxbasic/blob/23b28db10e41117805bdb3c0f78543590853b132/arch/zx48k/backend/__init__.py#L2193-L2207,"def convertToBool(): """""" Convert a byte value to boolean (0 or 1) if the global flag strictBool is True """""" if not OPTIONS.strictBool.value: return [] REQUIRES.add('strictbool.asm') result = [] result.append('pop af') result.append('call __NORMALIZE_BOOLEAN') result.append('push af') return result",1
1748,Python,convert int to bool,https://github.com/yaybu/callsign/blob/e70e5368bfe4fd3ae3fdd1ed43944b53ffa1e100/callsign/config.py#L54-L60,"def to_bool(x): if x.lower() in (""true"", ""yes"", ""on"", ""1""): return True elif x.lower() in (""false"", ""no"", ""off"", ""0""): return False else: raise ValueError(""%r in config file is not boolean"" % x)",1
844,Python,convert html to pdf,https://github.com/thewca/wca-regulations-compiler/blob/3ebbd8fe8fec7c9167296f59b2677696fe61a954/wrc/wrc.py#L85-L113,"def html_to_pdf(tmp_filenames, output_directory, lang_options): input_html = output_directory + ""/"" + tmp_filenames[0] wkthml_cmd = [""wkhtmltopdf""] # Basic margins etc wkthml_cmd.extend([""--margin-left"", ""18""]) wkthml_cmd.extend([""--margin-right"", ""18""]) wkthml_cmd.extend([""--page-size"", ""Letter""]) # Header and Footer header_file = pkg_resources.resource_filename(""wrc"", ""data/header.html"") footer_file = pkg_resources.resource_filename(""wrc"", ""data/footer.html"") wkthml_cmd.extend([""--header-html"", header_file]) wkthml_cmd.extend([""--footer-html"", footer_file]) wkthml_cmd.extend([""--header-spacing"", ""8""]) wkthml_cmd.extend([""--footer-spacing"", ""8""]) wkthml_cmd.append(input_html) wkthml_cmd.append(output_directory + ""/"" + lang_options['pdf'] + '.pdf') try: check_call(wkthml_cmd) print ""Successfully generated pdf file!"" print ""Cleaning temporary file (%s)..."" % input_html os.remove(input_html) except CalledProcessError as err: print ""Error while generating pdf:"" print err sys.exit(1) except OSError as err: print ""Error when running command \"""" + "" "".join(wkthml_cmd) + ""\"""" print err sys.exit(1)",3
929,Python,convert html to pdf,https://github.com/nigma/django-easy-pdf/blob/327605b91a445b453d8969b341ef74b12ab00a83/easy_pdf/rendering.py#L51-L78,"def html_to_pdf(content, encoding=""utf-8"", link_callback=fetch_resources, **kwargs): """""" Converts html ``content`` into PDF document. :param unicode content: html content :returns: PDF content :rtype: :class:`bytes` :raises: :exc:`~easy_pdf.exceptions.PDFRenderingError` """""" src = BytesIO(content.encode(encoding)) dest = BytesIO() pdf = pisa.pisaDocument(src, dest, encoding=encoding, link_callback=link_callback, **kwargs) if pdf.err: logger.error(""Error rendering PDF document"") for entry in pdf.log: if entry[0] == xhtml2pdf.default.PML_ERROR: logger_x2p.error(""line %s, msg: %s, fragment: %s"", entry[1], entry[2], entry[3]) raise PDFRenderingError(""Errors rendering PDF"", content=content, log=pdf.log) if pdf.warn: for entry in pdf.log: if entry[0] == xhtml2pdf.default.PML_WARNING: logger_x2p.warning(""line %s, msg: %s, fragment: %s"", entry[1], entry[2], entry[3]) return dest.getvalue()",3
1006,Python,convert html to pdf,https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/pdf.py#L232-L402,"def make_pdf_from_html( # Mandatory parameters: on_disk: bool, html: str, # Disk options: output_path: str = None, # Shared options: header_html: str = None, footer_html: str = None, wkhtmltopdf_filename: str = _WKHTMLTOPDF_FILENAME, wkhtmltopdf_options: Dict[str, Any] = None, file_encoding: str = ""utf-8"", debug_options: bool = False, debug_content: bool = False, debug_wkhtmltopdf_args: bool = True, fix_pdfkit_encoding_bug: bool = None, processor: str = _DEFAULT_PROCESSOR) -> Union[bytes, bool]: """""" Takes HTML and either returns a PDF in memory or makes one on disk. For preference, uses ``wkhtmltopdf`` (with ``pdfkit``): - faster than ``xhtml2pdf`` - tables not buggy like ``Weasyprint`` - however, doesn't support CSS Paged Media, so we have the ``header_html`` and ``footer_html`` options to allow you to pass appropriate HTML content to serve as the header/footer (rather than passing it within the main HTML). Args: on_disk: make file on disk (rather than returning it in memory)? html: main HTML output_path: if ``on_disk``, the output filename header_html: optional page header, as HTML footer_html: optional page footer, as HTML wkhtmltopdf_filename: filename of the ``wkhtmltopdf`` executable wkhtmltopdf_options: options for ``wkhtmltopdf`` file_encoding: encoding to use when writing the header/footer to disk debug_options: log ``wkhtmltopdf`` config/options passed to ``pdfkit``? debug_content: log the main/header/footer HTML? debug_wkhtmltopdf_args: log the final command-line arguments to that will be used by ``pdfkit`` when it calls ``wkhtmltopdf``? fix_pdfkit_encoding_bug: attempt to work around bug in e.g. ``pdfkit==0.5.0`` by encoding ``wkhtmltopdf_filename`` to UTF-8 before passing it to ``pdfkit``? If you pass ``None`` here, then a default value is used, from :func:`get_default_fix_pdfkit_encoding_bug`. processor: a PDF processor type from :class:`Processors` Returns: the PDF binary as a ``bytes`` object Raises: AssertionError: if bad ``processor`` RuntimeError: if requested processor is unavailable """""" wkhtmltopdf_options = wkhtmltopdf_options or {} # type: Dict[str, Any] assert_processor_available(processor) if debug_content: log.debug(""html: {}"", html) log.debug(""header_html: {}"", header_html) log.debug(""footer_html: {}"", footer_html) if fix_pdfkit_encoding_bug is None: fix_pdfkit_encoding_bug = get_default_fix_pdfkit_encoding_bug() if processor == Processors.XHTML2PDF: if on_disk: with open(output_path, mode='wb') as outfile: # noinspection PyUnresolvedReferences xhtml2pdf.document.pisaDocument(html, outfile) return True else: memfile = io.BytesIO() # noinspection PyUnresolvedReferences xhtml2pdf.document.pisaDocument(html, memfile) # ... returns a document, but we don't use it, so we don't store it # to stop pychecker complaining # http://xhtml2pdf.appspot.com/static/pisa-en.html memfile.seek(0) return memfile.read() # http://stackoverflow.com/questions/3310584 elif processor == Processors.WEASYPRINT: if on_disk: return weasyprint.HTML(string=html).write_pdf(output_path) else: # http://ampad.de/blog/generating-pdfs-django/ return weasyprint.HTML(string=html).write_pdf() elif processor == Processors.PDFKIT: # Config: if not wkhtmltopdf_filename: config = None else: if fix_pdfkit_encoding_bug: # needs to be True for pdfkit==0.5.0 log.debug(""Attempting to fix bug in pdfkit (e.g. version 0.5.0)"" "" by encoding wkhtmltopdf_filename to UTF-8"") config = pdfkit.configuration( wkhtmltopdf=wkhtmltopdf_filename.encode('utf-8')) # the bug is that pdfkit.pdfkit.PDFKit.__init__ will attempt to # decode the string in its configuration object; # https://github.com/JazzCore/python-pdfkit/issues/32 else: config = pdfkit.configuration(wkhtmltopdf=wkhtmltopdf_filename) # Temporary files that a subprocess can read: # http://stackoverflow.com/questions/15169101 # wkhtmltopdf requires its HTML files to have "".html"" extensions: # http://stackoverflow.com/questions/5776125 h_filename = None f_filename = None try: if header_html: h_fd, h_filename = tempfile.mkstemp(suffix='.html') os.write(h_fd, header_html.encode(file_encoding)) os.close(h_fd) wkhtmltopdf_options[""header-html""] = h_filename if footer_html: f_fd, f_filename = tempfile.mkstemp(suffix='.html') os.write(f_fd, footer_html.encode(file_encoding)) os.close(f_fd) wkhtmltopdf_options[""footer-html""] = f_filename if debug_options: log.debug(""wkhtmltopdf config: {!r}"", config) log.debug(""wkhtmltopdf_options: {}"", pformat(wkhtmltopdf_options)) kit = pdfkit.pdfkit.PDFKit(html, 'string', configuration=config, options=wkhtmltopdf_options) if on_disk: path = output_path else: path = None # With ""path=None"", the to_pdf() function directly returns # stdout from a subprocess.Popen().communicate() call (see # pdfkit.py). Since universal_newlines is not set, stdout will # be bytes in Python 3. if debug_wkhtmltopdf_args: log.debug(""Probable current user: {!r}"", getpass.getuser()) log.debug(""wkhtmltopdf arguments will be: {!r}"", kit.command(path=path)) return kit.to_pdf(path=path) finally: if h_filename: os.remove(h_filename) if f_filename: os.remove(f_filename) else: raise AssertionError(""Unknown PDF engine"")",3
1187,Python,convert html to pdf,https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/browser/referenceanalysis.py#L85-L88,def toPdf(self): html = safe_unicode(self.template()).encode('utf-8') pdf_data = createPdf(html) return pdf_data,3
1460,Python,convert html to pdf,https://github.com/senaite/senaite.core/blob/7602ce2ea2f9e81eb34e20ce17b98a3e70713f85/bika/lims/browser/stickers.py#L281-L291,"def pdf_from_post(self): """"""Returns a pdf stream with the stickers """""" html = self.request.form.get(""html"") style = self.request.form.get(""style"") reporthtml = ""<html><head>{0}</head><body>{1}</body></html>"" reporthtml = reporthtml.format(style, html) reporthtml = safe_unicode(reporthtml).encode(""utf-8"") pdf_fn = tempfile.mktemp(suffix="".pdf"") pdf_file = createPdf(htmlreport=reporthtml, outfile=pdf_fn) return pdf_file",3
428,Python,convert html to pdf,https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/pdf.py#L161-L186,"def add_to_writer(self, writer: PdfFileWriter, start_recto: bool = True) -> None: """""" Add the PDF described by this class to a PDF writer. Args: writer: a :class:`PyPDF2.PdfFileWriter` start_recto: start a new right-hand page? """""" if self.is_html: pdf = get_pdf_from_html( html=self.html, header_html=self.header_html, footer_html=self.footer_html, wkhtmltopdf_filename=self.wkhtmltopdf_filename, wkhtmltopdf_options=self.wkhtmltopdf_options) append_memory_pdf_to_writer(pdf, writer, start_recto=start_recto) elif self.is_filename: if start_recto and writer.getNumPages() % 2 != 0: writer.addBlankPage() writer.appendPagesFromReader(PdfFileReader( open(self.filename, 'rb'))) else: raise AssertionError(""PdfPlan: shouldn't get here!"")",2
439,Python,convert html to pdf,https://github.com/pikepdf/pikepdf/blob/07154f4dec007e2e9c0c6a8c07b964fd06bc5f77/src/pikepdf/_methods.py#L76-L83,"def _single_page_pdf(page): """"""Construct a single page PDF from the provided page in memory"""""" pdf = Pdf.new() pdf.pages.append(page) bio = BytesIO() pdf.save(bio) bio.seek(0) return bio.read()",2
660,Python,convert html to pdf,https://github.com/jbarlow83/OCRmyPDF/blob/79c84eefa353632a3d7ccddbd398c6678c1c1777/src/ocrmypdf/hocrtransform.py#L156-L231,"def to_pdf( self, outFileName, imageFileName=None, showBoundingboxes=False, fontname=""Helvetica"", invisibleText=False, interwordSpaces=False, ): """""" Creates a PDF file with an image superimposed on top of the text. Text is positioned according to the bounding box of the lines in the hOCR file. The image need not be identical to the image used to create the hOCR file. It can have a lower resolution, different color mode, etc. """""" # create the PDF file # page size in points (1/72 in.) pdf = Canvas(outFileName, pagesize=(self.width, self.height), pageCompression=1) # draw bounding box for each paragraph # light blue for bounding box of paragraph pdf.setStrokeColorRGB(0, 1, 1) # light blue for bounding box of paragraph pdf.setFillColorRGB(0, 1, 1) pdf.setLineWidth(0) # no line for bounding box for elem in self.hocr.findall("".//%sp[@class='%s']"" % (self.xmlns, ""ocr_par"")): elemtxt = self._get_element_text(elem).rstrip() if len(elemtxt) == 0: continue pxl_coords = self.element_coordinates(elem) pt = self.pt_from_pixel(pxl_coords) # draw the bbox border if showBoundingboxes: pdf.rect( pt.x1, self.height - pt.y2, pt.x2 - pt.x1, pt.y2 - pt.y1, fill=1 ) found_lines = False for line in self.hocr.findall( "".//%sspan[@class='%s']"" % (self.xmlns, ""ocr_line"") ): found_lines = True self._do_line( pdf, line, ""ocrx_word"", fontname, invisibleText, interwordSpaces, showBoundingboxes, ) if not found_lines: # Tesseract did not report any lines (just words) root = self.hocr.find("".//%sdiv[@class='%s']"" % (self.xmlns, ""ocr_page"")) self._do_line( pdf, root, ""ocrx_word"", fontname, invisibleText, interwordSpaces, showBoundingboxes, ) # put the image on the page, scaled to fill the page if imageFileName is not None: pdf.drawImage(imageFileName, 0, 0, width=self.width, height=self.height) # finish up the page and save it pdf.showPage() pdf.save()",2
1060,Python,convert html to pdf,https://github.com/alexhayes/django-pdfkit/blob/02774ae2cb67d05dd5e4cb50661c56464ebb2413/django_pdfkit/views.py#L55-L74,"def render_pdf(self, *args, **kwargs): """""" Render the PDF and returns as bytes. :rtype: bytes """""" html = self.render_html(*args, **kwargs) options = self.get_pdfkit_options() if 'debug' in self.request.GET and settings.DEBUG: options['debug-javascript'] = 1 kwargs = {} wkhtmltopdf_bin = os.environ.get('WKHTMLTOPDF_BIN') if wkhtmltopdf_bin: kwargs['configuration'] = pdfkit.configuration(wkhtmltopdf=wkhtmltopdf_bin) pdf = pdfkit.from_string(html, False, options, **kwargs) return pdf",2
1326,Python,convert html to pdf,https://github.com/tutorcruncher/pydf/blob/53dd030f02f112593ed6e2655160a40b892a23c0/docker-entrypoint.py#L36-L53,"async def generate(request): start = time() config = {} for k, v in request.headers.items(): if k.startswith('Pdf-') or k.startswith('Pdf_'): config[k[4:].lower()] = v.lower() data = await request.read() if not data: logger.info('Request with no body data') raise web.HTTPBadRequest(text='400: no HTML data to convert to PDF in request body\n') try: pdf_content = await app['apydf'].generate_pdf(data.decode(), **config) except RuntimeError as e: logger.info('Error generating PDF, time %0.2fs, config: %s', time() - start, config) return web.Response(text=str(e) + '\n', status=418) else: logger.info('PDF generated in %0.2fs, html-len %d, pdf-len %d', time() - start, len(data), len(pdf_content)) return web.Response(body=pdf_content, content_type='application/pdf')",2
520,Python,convert decimal to hex,https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/pydevd_attach_to_process/winappdbg/textio.py#L121-L140,"def hexadecimal(token): """""" Convert a strip of hexadecimal numbers into binary data. @type token: str @param token: String to parse. @rtype: str @return: Parsed string value. """""" token = ''.join([ c for c in token if c.isalnum() ]) if len(token) % 2 != 0: raise ValueError(""Missing characters in hex data"") data = '' for i in compat.xrange(0, len(token), 2): x = token[i:i+2] d = int(x, 16) s = struct.pack('<B', d) data += s return data",3
1038,Python,convert decimal to hex,https://github.com/mathiasertl/django-ca/blob/976d7ea05276320f20daed2a6d59c8f5660fe976/ca/django_ca/utils.py#L211-L222,"def int_to_hex(i): """"""Create a hex-representation of the given serial. >>> int_to_hex(12345678) 'BC:61:4E' """""" s = hex(i)[2:].upper() if six.PY2 is True and isinstance(i, long): # pragma: only py2 # NOQA # Strip the ""L"" suffix, since hex(1L) -> 0x1L. # NOTE: Do not convert to int earlier. int(<very-large-long>) is still long s = s[:-1] return add_colons(s)",3
1129,Python,convert decimal to hex,https://github.com/clach04/python-tuya/blob/7b89d38c56f6e25700e2a333000d25bc8d923622/pytuya/__init__.py#L408-L420,"def _hexvalue_to_rgb(hexvalue): """""" Converts the hexvalue used by tuya for colour representation into an RGB value. Args: hexvalue(string): The hex representation generated by BulbDevice._rgb_to_hexvalue() """""" r = int(hexvalue[0:2], 16) g = int(hexvalue[2:4], 16) b = int(hexvalue[4:6], 16) return (r, g, b)",3
1195,Python,convert decimal to hex,https://github.com/restran/mountains/blob/a97fee568b112f4e10d878f815d0db3dd0a98d74/mountains/encoding/converter.py#L163-L170,"def dec2str(n): """""" decimal number to string. """""" s = hex(int(n))[2:].rstrip('L') if len(s) % 2 != 0: s = '0' + s return hex2str(s)",3
998,Python,convert decimal to hex,https://github.com/markperdue/pyvesync/blob/7552dd1a6dd5ebc452acf78e33fd8f6e721e8cfc/src/pyvesync/helpers.py#L122-L127,"def calculate_hex(hex_string): """"""Credit for conversion to itsnotlupus/vesync_wsproxy"""""" hex_conv = hex_string.split(':') converted_hex = (int(hex_conv[0], 16) + int(hex_conv[1], 16))/8192 return converted_hex",2
1169,Python,convert decimal to hex,https://github.com/boriel/zxbasic/blob/23b28db10e41117805bdb3c0f78543590853b132/asmlex.py#L226-L239,"def t_BIN(self, t): r'(%[01]+)|([01]+[bB])' # A Binary integer # Note 00B is a 0 binary, but # 00Bh is a 12 in hex. So this pattern must come # after HEXA if t.value[0] == '%': t.value = t.value[1:] # Remove initial % else: t.value = t.value[:-1] # Remove last 'b' t.value = int(t.value, 2) # Convert to decimal t.type = 'INTEGER' return t",2
1227,Python,convert decimal to hex,https://github.com/maxweisspoker/simplebitcoinfuncs/blob/ad332433dfcc067e86d2e77fa0c8f1a27daffb63/simplebitcoinfuncs/miscbitcoinfuncs.py#L190-L199,"def inttoDER(a): ''' Format an int/long to DER hex format ''' o = dechex(a,1) if int(o[:2],16) > 127: o = '00' + o olen = dechex(len(o)//2,1) return '02' + olen + o",2
619,Python,convert decimal to hex,https://github.com/restran/mountains/blob/a97fee568b112f4e10d878f815d0db3dd0a98d74/mountains/encoding/converter.py#L84-L93,"def hex2dec(s): """""" hex2dec 十六进制 to 十进制 :param s: :return: """""" if not isinstance(s, str): s = str(s) return int(s.upper(), 16)",1
1144,Python,convert decimal to hex,https://github.com/talkincode/txradius/blob/b86fdbc9be41183680b82b07d3a8e8ea10926e01/txradius/mschap/mschap.py#L94-L101,"def convert_to_hex_string(string): hex_str = """" for c in string: hex_tmp = hex(ord(c))[2:] if len(hex_tmp) == 1: hex_tmp = ""0"" + hex_tmp hex_str += hex_tmp return hex_str.upper()",1
624,Python,convert decimal to hex,https://github.com/tanghaibao/jcvi/blob/d2e31a77b6ade7f41f3b321febc2b4744d1cdeca/jcvi/utils/webcolors.py#L356-L396,"def normalize_hex(hex_value): """""" Normalize a hexadecimal color value to the following form and return the result:: #[a-f0-9]{6} In other words, the following transformations are applied as needed: * If the value contains only three hexadecimal digits, it is expanded to six. * The value is normalized to lower-case. If the supplied value cannot be interpreted as a hexadecimal color value, ``ValueError`` is raised. Examples: >>> normalize_hex('#0099cc') '#0099cc' >>> normalize_hex('#0099CC') '#0099cc' >>> normalize_hex('#09c') '#0099cc' >>> normalize_hex('#09C') '#0099cc' >>> normalize_hex('0099cc') Traceback (most recent call last): ... ValueError: '0099cc' is not a valid hexadecimal color value. """""" try: hex_digits = HEX_COLOR_RE.match(hex_value).groups()[0] except AttributeError: raise ValueError(""'%s' is not a valid hexadecimal color value."" % hex_value) if len(hex_digits) == 3: hex_digits = ''.join([2 * s for s in hex_digits]) return '#%s' % hex_digits.lower()",0
355,Python,convert a utc time to epoch,https://github.com/theonion/django-bulbs/blob/0c0e6e3127a7dc487b96677fab95cacd2b3806da/bulbs/utils/methods.py#L66-L68,def datetime_to_epoch_seconds(value): epoch = datetime.utcfromtimestamp(0).replace(tzinfo=timezone.utc) return (value - epoch).total_seconds(),3
1012,Python,convert a utc time to epoch,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L67-L79,"def utc_epoch(): """""" Gets the epoch in the users timezone :return: """""" # pendulum utcnow() is not used as that sets a TimezoneInfo object # instead of a Timezone. This is not pickable and also creates issues # when using replace() d = dt.datetime(1970, 1, 1) d = d.replace(tzinfo=utc) return d",3
1272,Python,convert a utc time to epoch,https://github.com/pymacaron/pymacaron/blob/af244f203f8216108b39d374d46bf8e1813f13d5/pymacaron/utils.py#L48-L60,"def to_epoch(t): """"""Take a datetime, either as a string or a datetime.datetime object, and return the corresponding epoch"""""" if isinstance(t, str): if '+' not in t: t = t + '+00:00' t = parser.parse(t) elif t.tzinfo is None or t.tzinfo.utcoffset(t) is None: t = t.replace(tzinfo=pytz.timezone('utc')) t0 = datetime.datetime(1970, 1, 1, 0, 0, 0, 0, pytz.timezone('utc')) delta = t - t0 return int(delta.total_seconds())",3
1714,Python,convert a utc time to epoch,https://github.com/aws/aws-xray-sdk-python/blob/707358cd3a516d51f2ebf71cf34f00e8d906a667/aws_xray_sdk/core/sampling/connector.py#L136-L149,"def _dt_to_epoch(self, dt): """""" Convert a offset-aware datetime to POSIX time. """""" if PY2: # The input datetime is from botocore unmarshalling and it is # offset-aware so the timedelta of subtracting this time # to 01/01/1970 using the same tzinfo gives us # Unix Time (also known as POSIX Time). time_delta = dt - datetime(1970, 1, 1).replace(tzinfo=dt.tzinfo) return int(time_delta.total_seconds()) else: # Added in python 3.3+ and directly returns POSIX time. return int(dt.timestamp())",3
2068,Python,convert a utc time to epoch,https://github.com/myusuf3/delorean/blob/3e8a7b8cfd4c26546f62bde2f34002893adfa08a/delorean/dates.py#L492-L512,"def epoch(self): """""" Returns the total seconds since epoch associated with the Delorean object. .. testsetup:: from datetime import datetime from delorean import Delorean .. doctest:: >>> d = Delorean(datetime(2015, 1, 1), timezone='US/Pacific') >>> d.epoch 1420099200.0 """""" epoch_sec = pytz.utc.localize(datetime.utcfromtimestamp(0)) now_sec = pytz.utc.normalize(self._dt) delta_sec = now_sec - epoch_sec return get_total_second(delta_sec)",3
255,Python,convert a utc time to epoch,https://github.com/erdc/RAPIDpy/blob/50e14e130554b254a00ff23b226cd7e4c6cfe91a/RAPIDpy/dataset.py#L296-L412,"def get_time_array(self, datetime_simulation_start=None, simulation_time_step_seconds=None, return_datetime=False, time_index_array=None): """""" This method extracts or generates an array of time. The new version of RAPID output has the time array stored. However, the old version requires the user to know when the simulation began and the time step of the output. Parameters ---------- datetime_simulation_start: :obj:`datetime.datetime`, optional The start datetime of the simulation. Only required if the time variable is not included in the file. simulation_time_step_seconds: int, optional The time step of the file in seconds. Only required if the time variable is not included in the file. return_datetime: bool, optional If true, it converts the data to a list of datetime objects. Default is False. time_index_array: list or :obj:`numpy.array`, optional This is used to extract the datetime values by index from the main list. This can be from the *get_time_index_range* function. Returns ------- list: An array of integers representing seconds since Jan 1, 1970 UTC or datetime objects if *return_datetime* is set to True. These examples demonstrates how to retrieve or generate a time array to go along with your RAPID streamflow series. CF-Compliant Qout File Example: .. code:: python from RAPIDpy import RAPIDDataset path_to_rapid_qout = '/path/to/Qout.nc' with RAPIDDataset(path_to_rapid_qout) as qout_nc: #retrieve integer timestamp array time_array = qout_nc.get_time_array() #or, to get datetime array time_datetime = qout_nc.get_time_array(return_datetime=True) Legacy Qout File Example: .. code:: python from RAPIDpy import RAPIDDataset path_to_rapid_qout = '/path/to/Qout.nc' with RAPIDDataset(path_to_rapid_qout, datetime_simulation_start=datetime(1980, 1, 1), simulation_time_step_seconds=3 * 3600)\ as qout_nc: #retrieve integer timestamp array time_array = qout_nc.get_time_array() #or, to get datetime array time_datetime = qout_nc.get_time_array(return_datetime=True) """""" # Original Qout file if datetime_simulation_start is not None: self.datetime_simulation_start = datetime_simulation_start if simulation_time_step_seconds is not None: self.simulation_time_step_seconds = simulation_time_step_seconds epoch = datetime.datetime(1970, 1, 1, tzinfo=utc) time_units = ""seconds since {0}"".format(epoch) # CF-1.6 compliant file if self.is_time_variable_valid(): time_array = self.qout_nc.variables['time'][:] if self.qout_nc.variables['time'].units: time_units = self.qout_nc.variables['time'].units # Original Qout file elif self._is_legacy_time_valid(): initial_time_seconds = ((self.datetime_simulation_start .replace(tzinfo=utc) - epoch) .total_seconds() + self.simulation_time_step_seconds) final_time_seconds = (initial_time_seconds + self.size_time * self.simulation_time_step_seconds) time_array = np.arange(initial_time_seconds, final_time_seconds, self.simulation_time_step_seconds) else: raise ValueError(""This file does not contain the time"" "" variable. To get time array, add"" "" datetime_simulation_start and"" "" simulation_time_step_seconds"") if time_index_array is not None: time_array = time_array[time_index_array] if return_datetime: time_array = num2date(time_array, time_units) if self.out_tzinfo is not None: for i in xrange(len(time_array)): # convert time to output timezone time_array[i] = utc.localize(time_array[i]) \ .astimezone(self.out_tzinfo) \ .replace(tzinfo=None) return time_array",2
932,Python,convert a utc time to epoch,https://github.com/halcy/Mastodon.py/blob/35c43562dd3d34d6ebf7a0f757c09e8fcccc957c/mastodon/Mastodon.py#L2443-L2458,"def __datetime_to_epoch(self, date_time): """""" Converts a python datetime to unix epoch, accounting for time zones and such. Assumes UTC if timezone is not given. """""" date_time_utc = None if date_time.tzinfo is None: date_time_utc = date_time.replace(tzinfo=pytz.utc) else: date_time_utc = date_time.astimezone(pytz.utc) epoch_utc = datetime.datetime.utcfromtimestamp(0).replace(tzinfo=pytz.utc) return (date_time_utc - epoch_utc).total_seconds()",2
1954,Python,convert a utc time to epoch,https://github.com/collectiveacuity/labPack/blob/52949ece35e72e3cc308f54d9ffa6bfbd96805b8/labpack/records/time.py#L181-L206,"def fromEpoch(cls, epoch_time): ''' a method for constructing a labDT object from epoch timestamp :param epoch_time: number with epoch timestamp info :return: labDT object ''' # validate input title = 'Epoch time input for labDT.fromEpoch' if not isinstance(epoch_time, float) and not isinstance(epoch_time, int): raise TypeError('\n%s must be an integer or float.' % title) # construct labDT from epoch time dT = datetime.utcfromtimestamp(epoch_time).replace(tzinfo=pytz.utc) dt_kwargs = { 'year': dT.year, 'month': dT.month, 'day': dT.day, 'hour': dT.hour, 'minute': dT.minute, 'second': dT.second, 'microsecond': dT.microsecond, 'tzinfo': dT.tzinfo } return labDT(**dt_kwargs)",2
232,Python,convert a utc time to epoch,https://github.com/DataBiosphere/dsub/blob/443ce31daa6023dc2fd65ef2051796e19d18d5a7/dsub/providers/google.py#L466-L473,"def _datetime_to_utc_int(date): """"""Convert the integer UTC time value into a local datetime."""""" if date is None: return None # Convert localized datetime to a UTC integer epoch = dsub_util.replace_timezone(datetime.utcfromtimestamp(0), pytz.utc) return (date - epoch).total_seconds()",1
910,Python,convert a utc time to epoch,https://github.com/google/mobly/blob/38ba2cf7d29a20e6a2fca1718eecb337df38db26/mobly/utils.py#L147-L165,"def epoch_to_human_time(epoch_time): """"""Converts an epoch timestamp to human readable time. This essentially converts an output of get_current_epoch_time to an output of get_current_human_time Args: epoch_time: An integer representing an epoch timestamp in milliseconds. Returns: A time string representing the input time. None if input param is invalid. """""" if isinstance(epoch_time, int): try: d = datetime.datetime.fromtimestamp(epoch_time / 1000) return d.strftime(""%m-%d-%Y %H:%M:%S "") except ValueError: return None",1
129,Python,convert a date string into yyyymmdd,https://github.com/bram85/topydo/blob/b59fcfca5361869a6b78d4c9808c7c6cd0a18b58/topydo/lib/Utils.py#L28-L46,"def date_string_to_date(p_date): """""" Given a date in YYYY-MM-DD, returns a Python date object. Throws a ValueError if the date is invalid. """""" result = None if p_date: parsed_date = re.match(r'(\d{4})-(\d{2})-(\d{2})', p_date) if parsed_date: result = date( int(parsed_date.group(1)), # year int(parsed_date.group(2)), # month int(parsed_date.group(3)) # day ) else: raise ValueError return result",3
1064,Python,convert a date string into yyyymmdd,https://github.com/why2pac/dp-tornado/blob/a5948f5693f6ee2d9bab31f611fedc074e1caa96/dp_tornado/helper/datetime/date.py#L74-L76,"def yyyymmdd(self, auto=None, datetime=None, timezone=None, timestamp=None, ms=False, concat=''): datetime = self.convert(auto=auto, datetime=datetime, timezone=timezone, timestamp=timestamp, ms=ms) return '%04d%s%02d%s%02d' % (datetime.year, concat, datetime.month, concat, datetime.day)",3
1591,Python,convert a date string into yyyymmdd,https://github.com/mushkevych/scheduler/blob/6740331360f49083c208085fb5a60ce80ebf418b/synergy/system/time_helper.py#L78-L82,"def day_to_month(timeperiod): """""":param timeperiod: as string in YYYYMMDD00 format :return string in YYYYMM0000 format"""""" t = datetime.strptime(timeperiod, SYNERGY_DAILY_PATTERN) return t.strftime(SYNERGY_MONTHLY_PATTERN)",3
106,Python,convert a date string into yyyymmdd,https://github.com/scheibler/khard/blob/0f69430c2680f1ff5f073a977a3c5b753b96cc17/khard/helpers.py#L81-L107,"def string_to_date(input): """"""Convert string to date object. :param input: the date string to parse :type input: str :returns: the parsed datetime object :rtype: datetime.datetime """""" # try date formats --mmdd, --mm-dd, yyyymmdd, yyyy-mm-dd and datetime # formats yyyymmddThhmmss, yyyy-mm-ddThh:mm:ss, yyyymmddThhmmssZ, # yyyy-mm-ddThh:mm:ssZ. for format_string in (""--%m%d"", ""--%m-%d"", ""%Y%m%d"", ""%Y-%m-%d"", ""%Y%m%dT%H%M%S"", ""%Y-%m-%dT%H:%M:%S"", ""%Y%m%dT%H%M%SZ"", ""%Y-%m-%dT%H:%M:%SZ""): try: return datetime.strptime(input, format_string) except ValueError: pass # try datetime formats yyyymmddThhmmsstz and yyyy-mm-ddThh:mm:sstz where tz # may look like -06:00. for format_string in (""%Y%m%dT%H%M%S%z"", ""%Y-%m-%dT%H:%M:%S%z""): try: return datetime.strptime(''.join(input.rsplit("":"", 1)), format_string) except ValueError: pass raise ValueError",2
164,Python,convert a date string into yyyymmdd,https://github.com/pypa/pipenv/blob/cae8d76c210b9777e90aab76e9c4b0e53bb19cde/pipenv/vendor/parse.py#L551-L636,"def date_convert(string, match, ymd=None, mdy=None, dmy=None, d_m_y=None, hms=None, am=None, tz=None, mm=None, dd=None): '''Convert the incoming string containing some date / time info into a datetime instance. ''' groups = match.groups() time_only = False if mm and dd: y=datetime.today().year m=groups[mm] d=groups[dd] elif ymd is not None: y, m, d = re.split(r'[-/\s]', groups[ymd]) elif mdy is not None: m, d, y = re.split(r'[-/\s]', groups[mdy]) elif dmy is not None: d, m, y = re.split(r'[-/\s]', groups[dmy]) elif d_m_y is not None: d, m, y = d_m_y d = groups[d] m = groups[m] y = groups[y] else: time_only = True H = M = S = u = 0 if hms is not None and groups[hms]: t = groups[hms].split(':') if len(t) == 2: H, M = t else: H, M, S = t if '.' in S: S, u = S.split('.') u = int(float('.' + u) * 1000000) S = int(S) H = int(H) M = int(M) if am is not None: am = groups[am] if am: am = am.strip() if am == 'AM' and H == 12: # correction for ""12"" hour functioning as ""0"" hour: 12:15 AM = 00:15 by 24 hr clock H -= 12 elif am == 'PM' and H == 12: # no correction needed: 12PM is midday, 12:00 by 24 hour clock pass elif am == 'PM': H += 12 if tz is not None: tz = groups[tz] if tz == 'Z': tz = FixedTzOffset(0, 'UTC') elif tz: tz = tz.strip() if tz.isupper(): # TODO use the awesome python TZ module? pass else: sign = tz[0] if ':' in tz: tzh, tzm = tz[1:].split(':') elif len(tz) == 4: # 'snnn' tzh, tzm = tz[1], tz[2:4] else: tzh, tzm = tz[1:3], tz[3:5] offset = int(tzm) + int(tzh) * 60 if sign == '-': offset = -offset tz = FixedTzOffset(offset, tz) if time_only: d = time(H, M, S, u, tzinfo=tz) else: y = int(y) if m.isdigit(): m = int(m) else: m = MONTHS_MAP[m] d = int(d) d = datetime(y, m, d, H, M, S, u, tzinfo=tz) return d",2
518,Python,convert a date string into yyyymmdd,https://github.com/gambogi/CSHLDAP/blob/09cb754b1e72437834e0d8cb4c7ac1830cfa6829/CSHLDAP.py#L336-L348,"def dateFromLDAPTimestamp(timestamp): """""" Takes an LDAP date (In the form YYYYmmdd with whatever is after that) and returns a datetime.date object. """""" # only check the first 8 characters: YYYYmmdd numberOfCharacters = len(""YYYYmmdd"") timestamp = timestamp[:numberOfCharacters] try: day = datetime.strptime(timestamp, '%Y%m%d') return date(year=day.year, month=day.month, day=day.day) except: print(timestamp)",2
943,Python,convert a date string into yyyymmdd,https://github.com/why2pac/dp-tornado/blob/a5948f5693f6ee2d9bab31f611fedc074e1caa96/dp_tornado/helper/datetime/time.py#L38-L53,"def convert(self, auto=None, datetime=None, timezone=None, timestamp=None, yyyymmdd=None, yyyymmddhhiiss=None, ms=False): return self.helper.datetime.convert( auto=auto, datetime=datetime, timezone=timezone, timestamp=timestamp, yyyymmdd=yyyymmdd, yyyymmddhhiiss=yyyymmddhhiiss, ms=ms)",2
818,Python,convert a date string into yyyymmdd,https://github.com/google/transitfeed/blob/eb2991a3747ba541b2cb66502b305b6304a1f85f/merge.py#L1166-L1206,"def DisjoinCalendars(self, cutoff): """"""Forces the old and new calendars to be disjoint about a cutoff date. This truncates the service periods of the old schedule so that service stops one day before the given cutoff date and truncates the new schedule so that service only begins on the cutoff date. Args: cutoff: The cutoff date as a string in YYYYMMDD format. The timezone is the same as used in the calendar.txt file. """""" def TruncatePeriod(service_period, start, end): """"""Truncate the service period to into the range [start, end]. Args: service_period: The service period to truncate. start: The start date as a string in YYYYMMDD format. end: The end date as a string in YYYYMMDD format. """""" service_period.start_date = max(service_period.start_date, start) service_period.end_date = min(service_period.end_date, end) dates_to_delete = [] for k in service_period.date_exceptions: if (k < start) or (k > end): dates_to_delete.append(k) for k in dates_to_delete: del service_period.date_exceptions[k] # find the date one day before cutoff year = int(cutoff[:4]) month = int(cutoff[4:6]) day = int(cutoff[6:8]) cutoff_date = datetime.date(year, month, day) one_day_delta = datetime.timedelta(days=1) before = (cutoff_date - one_day_delta).strftime('%Y%m%d') for a in self.feed_merger.a_schedule.GetServicePeriodList(): TruncatePeriod(a, 0, before) for b in self.feed_merger.b_schedule.GetServicePeriodList(): TruncatePeriod(b, cutoff, '9'*8)",1
45,Python,convert a date string into yyyymmdd,https://github.com/pbrisk/businessdate/blob/79a0c5a4e557cbacca82a430403b18413404a9bc/businessdate/businessdate.py#L260-L267,"def to_date(self): """""" construct datetime.date instance represented calendar date of BusinessDate instance :return datetime.date: """""" y, m, d = self.to_ymd() return date(y, m, d)",0
1431,Python,convert a date string into yyyymmdd,https://github.com/tanghaibao/goatools/blob/407682e573a108864a79031f8ca19ee3bf377626/goatools/anno/annoreader_base.py#L139-L141,"def get_date_yyyymmdd(yyyymmdd): """"""Return datetime.date given string."""""" return date(int(yyyymmdd[:4]), int(yyyymmdd[4:6], base=10), int(yyyymmdd[6:], base=10))",0
113,Python,connect to sql,https://github.com/whiteclover/dbpy/blob/3d9ce85f55cfb39cced22081e525f79581b26b3a/db/pymysql/connection.py#L53-L56,def connect(self): self.close() self._connect = pymysql.connect(**self._db_options) self._connect.autocommit(True),3
254,Python,connect to sql,https://github.com/guilhermechapiewski/simple-db-migrate/blob/7ea6ffd0c58f70079cc344eae348430c7bdaaab3/simple_db_migrate/mysql.py#L30-L40,"def __mysql_connect(self, connect_using_database_name=True): try: conn = self.__mysql_driver.connect(host=self.__mysql_host, port=self.__mysql_port, user=self.__mysql_user, passwd=self.__mysql_passwd) conn.set_character_set(self.__mysql_encoding) if connect_using_database_name: conn.select_db(self.__mysql_db) return conn except Exception as e: raise Exception(""could not connect to database: %s"" % e)",3
326,Python,connect to sql,https://github.com/robinandeer/puzzle/blob/9476f05b416d3a5135d25492cb31411fdf831c58/puzzle/plugins/sql/store.py#L62-L86,"def connect(self, db_uri, debug=False): """"""Configure connection to a SQL database. Args: db_uri (str): path/URI to the database to connect to debug (Optional[bool]): whether to output logging information """""" kwargs = {'echo': debug, 'convert_unicode': True} # connect to the SQL database if 'mysql' in db_uri: kwargs['pool_recycle'] = 3600 elif '://' not in db_uri: logger.debug(""detected sqlite path URI: {}"".format(db_uri)) db_path = os.path.abspath(os.path.expanduser(db_uri)) db_uri = ""sqlite:///{}"".format(db_path) self.engine = create_engine(db_uri, **kwargs) logger.debug('connection established successfully') # make sure the same engine is propagated to the BASE classes BASE.metadata.bind = self.engine # start a session self.session = scoped_session(sessionmaker(bind=self.engine)) # shortcut to query method self.query = self.session.query return self",3
666,Python,connect to sql,https://github.com/aouyar/PyMunin/blob/4f58a64b6b37c85a84cc7e1e07aafaa0321b249d/pysysinfo/mysql.py#L64-L69,"def _connect(self): """"""Establish connection to MySQL Database."""""" if self._connParams: self._conn = MySQLdb.connect(**self._connParams) else: self._conn = MySQLdb.connect('')",3
682,Python,connect to sql,https://github.com/coleifer/peewee/blob/ea9403b01acb039adb3a2472186d795c796b77a0/peewee.py#L3708-L3712,"def _connect(self): if mysql is None: raise ImproperlyConfigured('MySQL driver not installed!') conn = mysql.connect(db=self.database, **self.connect_params) return conn",3
1407,Python,connect to sql,https://github.com/coleifer/peewee/blob/ea9403b01acb039adb3a2472186d795c796b77a0/playhouse/mysql_ext.py#L14-L17,"def _connect(self): if mysql_connector is None: raise ImproperlyConfigured('MySQL connector not installed!') return mysql_connector.connect(db=self.database, **self.connect_params)",3
1762,Python,connect to sql,https://github.com/RudolfCardinal/pythonlib/blob/0b84cb35f38bd7d8723958dae51b480a829b7227/cardinal_pythonlib/rnc_db.py#L1649-L1908,"def _connect(self, engine: str = None, interface: str = None, host: str = None, port: int = None, database: str = None, driver: str = None, dsn: str = None, odbc_connection_string: str = None, user: str = None, password: str = None, autocommit: bool = True, charset: str = ""utf8"", use_unicode: bool = True) -> bool: # Check engine if engine == ENGINE_MYSQL: self.flavour = MySQL() self.schema = database elif engine == ENGINE_SQLSERVER: self.flavour = SQLServer() if database: self.schema = database else: self.schema = ""dbo"" # default for SQL server elif engine == ENGINE_ACCESS: self.flavour = Access() self.schema = ""dbo"" # default for SQL server else: raise ValueError(""Unknown engine"") # Default interface if interface is None: if engine == ENGINE_MYSQL: interface = INTERFACE_MYSQL else: interface = INTERFACE_ODBC # Default port if port is None: if engine == ENGINE_MYSQL: port = 3306 elif engine == ENGINE_SQLSERVER: port = 1433 # Default driver if driver is None: if engine == ENGINE_MYSQL and interface == INTERFACE_ODBC: driver = ""{MySQL ODBC 5.1 Driver}"" self._engine = engine self._interface = interface self._server = host self._port = port self._database = database self._user = user self._password = password self._charset = charset self._use_unicode = use_unicode self.autocommit = autocommit # Report intent log.info( ""Opening database: engine={e}, interface={i}, "" ""use_unicode={u}, autocommit={a}"".format( e=engine, i=interface, u=use_unicode, a=autocommit)) # Interface if interface == INTERFACE_MYSQL: if pymysql: self.db_pythonlib = PYTHONLIB_PYMYSQL elif MySQLdb: self.db_pythonlib = PYTHONLIB_MYSQLDB else: raise ImportError(_MSG_MYSQL_DRIVERS_UNAVAILABLE) elif interface == INTERFACE_ODBC: if not pyodbc: raise ImportError(_MSG_PYODBC_UNAVAILABLE) self.db_pythonlib = PYTHONLIB_PYODBC elif interface == INTERFACE_JDBC: if not jaydebeapi: raise ImportError(_MSG_JDBC_UNAVAILABLE) if host is None: raise ValueError(""Missing host parameter"") if port is None: raise ValueError(""Missing port parameter"") # if database is None: # raise ValueError(""Missing database parameter"") if user is None: raise ValueError(""Missing user parameter"") self.db_pythonlib = PYTHONLIB_JAYDEBEAPI else: raise ValueError(""Unknown interface"") # --------------------------------------------------------------------- # Connect # --------------------------------------------------------------------- if engine == ENGINE_MYSQL and interface == INTERFACE_MYSQL: # Connects to a MySQL database via MySQLdb/PyMySQL. # http://dev.mysql.com/doc/refman/5.1/en/connector-odbc-configuration-connection-parameters.html # noqa # http://code.google.com/p/pyodbc/wiki/ConnectionStrings # Between MySQLdb 1.2.3 and 1.2.5, the DateTime2literal function # stops producing e.g. # '2014-01-03 18:15:51' # and starts producing e.g. # '2014-01-03 18:15:51.842097+00:00'. # Let's fix that... datetimetype = datetime.datetime # as per MySQLdb times.py converters = mysql.converters.conversions.copy() converters[datetimetype] = datetime2literal_rnc # See also: # http://stackoverflow.com/questions/11053941 log.info( ""{i} connect: host={h}, port={p}, user={u}, "" ""database={d}"".format( i=interface, h=host, p=port, u=user, d=database)) self.db = mysql.connect( host=host, port=port, user=user, passwd=password, db=database, charset=charset, use_unicode=use_unicode, conv=converters ) # noinspection PyCallingNonCallable self.db.autocommit(autocommit) # http://mysql-python.sourceforge.net/MySQLdb.html # http://dev.mysql.com/doc/refman/5.0/en/mysql-autocommit.html # https://github.com/PyMySQL/PyMySQL/blob/master/pymysql/connections.py # noqa # MySQL character sets and collations: # http://dev.mysql.com/doc/refman/5.1/en/charset.html # Create a database using UTF8: # ... CREATE DATABASE mydb DEFAULT CHARACTER SET utf8 # DEFAULT COLLATE utf8_general_ci; # What is my database using? # ... SHOW VARIABLES LIKE 'character_set_%'; # Change a database character set: # ... ALTER DATABASE mydatabasename charset=utf8; # http://docs.moodle.org/23/en/ # Converting_your_MySQL_database_to_UTF8 # # Python talking to MySQL in Unicode: # http://www.harelmalka.com/?p=81 # http://stackoverflow.com/questions/6001104 elif engine == ENGINE_MYSQL and interface == INTERFACE_ODBC: log.info( ""ODBC connect: DRIVER={dr};SERVER={s};PORT={p};"" ""DATABASE={db};USER={u};PASSWORD=[censored]"".format( dr=driver, s=host, p=port, db=database, u=user)) dsn = ( ""DRIVER={0};SERVER={1};PORT={2};DATABASE={3};"" ""USER={4};PASSWORD={5}"".format(driver, host, port, database, user, password) ) self.db = pyodbc.connect(dsn) self.db.autocommit = autocommit # http://stackoverflow.com/questions/1063770 elif engine == ENGINE_MYSQL and interface == INTERFACE_JDBC: # https://help.ubuntu.com/community/JDBCAndMySQL # https://github.com/baztian/jaydebeapi/issues/1 jclassname = ""com.mysql.jdbc.Driver"" url = ""jdbc:mysql://{host}:{port}/{database}"".format( host=host, port=port, database=database) driver_args = [url, user, password] jars = None libs = None log.info( ""JDBC connect: jclassname={jclassname}, "" ""url={url}, user={user}, password=[censored]"".format( jclassname=jclassname, url=url, user=user, ) ) self._jdbc_connect(jclassname, driver_args, jars, libs, autocommit) elif engine == ENGINE_SQLSERVER and interface == INTERFACE_ODBC: # SQL Server: # http://code.google.com/p/pyodbc/wiki/ConnectionStrings if odbc_connection_string: log.info(""Using raw ODBC connection string [censored]"") connectstring = odbc_connection_string elif dsn: log.info( ""ODBC connect: DSN={dsn};UID={u};PWD=[censored]"".format( dsn=dsn, u=user)) connectstring = ""DSN={};UID={};PWD={}"".format(dsn, user, password) else: log.info( ""ODBC connect: DRIVER={dr};SERVER={s};DATABASE={db};"" ""UID={u};PWD=[censored]"".format( dr=driver, s=host, db=database, u=user)) connectstring = ( ""DRIVER={};SERVER={};DATABASE={};UID={};PWD={}"".format( driver, host, database, user, password) ) self.db = pyodbc.connect(connectstring, unicode_results=True) self.db.autocommit = autocommit # http://stackoverflow.com/questions/1063770 elif engine == ENGINE_SQLSERVER and interface == INTERFACE_JDBC: # jar tvf sqljdbc41.jar # https://msdn.microsoft.com/en-us/sqlserver/aa937724.aspx # https://msdn.microsoft.com/en-us/library/ms378428(v=sql.110).aspx # https://msdn.microsoft.com/en-us/library/ms378988(v=sql.110).aspx jclassname = 'com.microsoft.sqlserver.jdbc.SQLServerDriver' urlstem = 'jdbc:sqlserver://{host}:{port};'.format( host=host, port=port ) nvp = {} if database: nvp['databaseName'] = database nvp['user'] = user nvp['password'] = password nvp['responseBuffering'] = 'adaptive' # default is 'full' # ... THIS CHANGE (responseBuffering = adaptive) stops the JDBC # driver crashing on cursor close [in a socket recv() call] when # it's fetched a VARBINARY(MAX) field. nvp['selectMethod'] = 'cursor' # trying this; default is 'direct' url = urlstem + ';'.join( '{}={}'.format(x, y) for x, y in nvp.items()) nvp['password'] = '[censored]' url_censored = urlstem + ';'.join( '{}={}'.format(x, y) for x, y in nvp.items()) log.info( 'jdbc connect: jclassname={jclassname}, url = {url}'.format( jclassname=jclassname, url=url_censored ) ) driver_args = [url] jars = None libs = None self._jdbc_connect(jclassname, driver_args, jars, libs, autocommit) elif engine == ENGINE_ACCESS and interface == INTERFACE_ODBC: dsn = ""DSN={}"".format(dsn) log.info(""ODBC connect: DSN={}"", dsn) self.db = pyodbc.connect(dsn) self.db.autocommit = autocommit # http://stackoverflow.com/questions/1063770 else: raise ValueError( ""Unknown 'engine'/'interface' combination: {}/{}"".format( engine, interface ) ) return True",3
1845,Python,connect to sql,https://github.com/snower/TorMySQL/blob/01ff87f03a850a2a6e466700d020878eecbefa5d/tormysql/pool.py#L102-L111,"def connect(self): future = super(RecordQueryConnection, self).connect() origin_query = self._connection.query def query(sql, unbuffered=False): self._last_query_sql = sql return origin_query(sql, unbuffered) self._connection.query = query return future",3
1934,Python,connect to sql,https://github.com/posativ/isso/blob/78997f491044b7d694ac7170edc32030544095b7/isso/db/__init__.py#L56-L62,"def execute(self, sql, args=()): if isinstance(sql, (list, tuple)): sql = ' '.join(sql) with sqlite3.connect(self.path) as con: return con.execute(sql, args)",3
1238,Python,connect to sql,https://github.com/census-instrumentation/opencensus-python/blob/992b223f7e34c5dcb65922b7d5c827e7a1351e7d/contrib/opencensus-ext-django/examples/app/views.py#L110-L130,"def sqlalchemy_mysql_trace(request): try: engine = sqlalchemy.create_engine( 'mysql+mysqlconnector://{}:{}@{}'.format('root', MYSQL_PASSWORD, DB_HOST)) conn = engine.connect() query = 'SELECT 2*3' result_set = conn.execute(query) result = [] for item in result_set: result.append(item) return HttpResponse(str(result)) except Exception: msg = ""Query failed. Check your env vars for connection settings."" return HttpResponse(msg, status=500)",2
386,Python,confusion matrix,https://github.com/totalgood/pugnlp/blob/c43445b14afddfdeadc5f3076675c9e8fc1ee67c/src/pugnlp/stats.py#L536-L562,"def from_existing(cls, confusion, *args, **kwargs): """"""Creates a confusion matrix from a DataFrame that already contains confusion counts (but not meta stats) >>> df = pd.DataFrame(np.matrix([[0,1,2,0,1,2,1,2,2,1],[0,1,2,1,2,0,0,1,2,0]]).T, columns=['True', 'Pred']) >>> c = Confusion(df) >>> c2 = pd.DataFrame(c) >>> hasattr(c2, '_binary_sensitivity') False >>> c3 = Confusion.from_existing(c2) >>> hasattr(c3, '_binary_sensitivity') True >>> (c3 == c).all().all() True >>> c3 Pred 0 1 2 True 0 1 1 0 1 2 1 1 2 1 1 2 """""" # Extremely brute-force to recreate data from a confusion matrix! df = [] for t, p in product(confusion.index.values, confusion.columns.values): df += [[t, p]] * confusion[p][t] if confusion.index.name is not None and confusion.columns.name is not None: return Confusion(pd.DataFrame(df, columns=[confusion.index.name, confusion.columns.name])) return Confusion(pd.DataFrame(df))",3
388,Python,confusion matrix,https://github.com/edublancas/sklearn-evaluation/blob/79ee6e4dfe911b5a5a9b78a5caaed7c73eef6f39/sklearn_evaluation/evaluator.py#L85-L89,"def confusion_matrix(self): """"""Confusion matrix plot """""" return plot.confusion_matrix(self.y_true, self.y_pred, self.target_names, ax=_gen_ax())",3
470,Python,confusion matrix,https://github.com/sentinel-hub/eo-learn/blob/b8c390b9f553c561612fe9eb64e720611633a035/ml_tools/eolearn/ml_tools/validator.py#L201-L208,"def confusion_matrix(self): """""" Returns the normalised confusion matrix """""" confusion_matrix = self.pixel_classification_sum.astype(np.float) confusion_matrix = np.divide(confusion_matrix.T, self.pixel_truth_sum.T).T return confusion_matrix * 100.0",3
1423,Python,confusion matrix,https://github.com/openmednlp/bedrock/blob/15796bd7837ddfe7ad5235fd188e5d7af8c0be49/bedrock/viz.py#L47-L65,"def plot_confusion_reports(y, y_hat, class_names=None): if class_names is None: class_names = list(set(y).union(set(y_hat))) # Compute confusion matrix cnf_matrix = confusion_matrix(y, y_hat) np.set_printoptions(precision=2) # Plot non-normalized confusion matrix plt.figure() plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix, without normalization') # Plot normalized confusion matrix plt.figure() plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, title='Normalized confusion matrix') plt.show()",3
1484,Python,confusion matrix,https://github.com/rjrequina/Cebuano-POS-Tagger/blob/fb1b46448c40b23ac8e8e373f073f1f8934b6dc6/eval/evaluator.py#L72-L92,"def confusion_matrix(actual=[], pred=[]): idx = { 'ADJ' : 0, 'ADV' : 1, 'CONJ': 2, 'DET' : 3, 'NOUN': 4, 'NUM' : 5, 'OTH' : 6, 'PART': 7, 'PRON': 8, 'SYM' : 9, 'VERB': 10 } matrix = [[0 for i in range(11)] for j in range(11)] for i in range(0, len(actual)): matrix[idx[actual[i]]][idx[pred[i]]] += 1 return matrix",3
16,Python,confusion matrix,https://github.com/rigetti/grove/blob/dc6bf6ec63e8c435fe52b1e00f707d5ce4cdb9b3/grove/tomography/operator_utils.py#L70-L94,"def make_diagonal_povm(pi_basis, confusion_rate_matrix): """""" Create a DiagonalPOVM from a ``pi_basis`` and the ``confusion_rate_matrix`` associated with a readout. See also the grove documentation. :param OperatorBasis pi_basis: An operator basis of rank-1 projection operators. :param numpy.ndarray confusion_rate_matrix: The matrix of detection probabilities conditional on a prepared qubit state. :return: The POVM corresponding to confusion_rate_matrix. :rtype: DiagonalPOVM """""" confusion_rate_matrix = np.asarray(confusion_rate_matrix) if not np.allclose(confusion_rate_matrix.sum(axis=0), np.ones(confusion_rate_matrix.shape[1])): raise CRMUnnormalizedError(""Unnormalized confusion matrix:\n{}"".format( confusion_rate_matrix)) if not (confusion_rate_matrix >= 0).all() or not (confusion_rate_matrix <= 1).all(): raise CRMValueError(""Confusion matrix must have values in [0, 1]:"" ""\n{}"".format(confusion_rate_matrix)) ops = [sum((pi_j * pjk for (pi_j, pjk) in izip(pi_basis.ops, pjs)), 0) for pjs in confusion_rate_matrix] return DiagonalPOVM(pi_basis=pi_basis, confusion_rate_matrix=confusion_rate_matrix, ops=ops)",2
1510,Python,confusion matrix,https://github.com/openmednlp/bedrock/blob/15796bd7837ddfe7ad5235fd188e5d7af8c0be49/bedrock/viz.py#L12-L44,"def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues): """""" This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. """""" if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] print(""Normalized confusion matrix"") else: print('Confusion matrix, without normalization') print(cm) plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = '.2f' if normalize else 'd' thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=""center"", color=""white"" if cm[i, j] > thresh else ""black"") plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label')",2
423,Python,confusion matrix,https://github.com/h2oai/h2o-3/blob/dd62aaa1e7f680a8b16ee14bc66b0fb5195c2ad8/h2o-py/h2o/model/multinomial.py#L18-L26,"def confusion_matrix(self, data): """""" Returns a confusion matrix based of H2O's default prediction threshold for a dataset. :param H2OFrame data: the frame with the prediction results for which the confusion matrix should be extracted. """""" assert_is_type(data, H2OFrame) j = h2o.api(""POST /3/Predictions/models/%s/frames/%s"" % (self._id, data.frame_id)) return j[""model_metrics""][0][""cm""][""table""]",1
1309,Python,confusion matrix,https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/regions/knn_classifier_region.py#L684-L688,"def _getAccuracy(self): n = self.confusion.shape[0] assert n == self.confusion.shape[1], ""Confusion matrix is non-square."" return self.confusion[range(n), range(n)].sum(), self.confusion.sum()",1
449,Python,concatenate several file remove header lines,https://github.com/bihealth/vcfpy/blob/99e2165df30f11e0c95f3170f31bc5191d9e9e15/vcfpy/header.py#L227-L253,"def header_without_lines(header, remove): """"""Return :py:class:`Header` without lines given in ``remove`` ``remove`` is an iterable of pairs ``key``/``ID`` with the VCF header key and ``ID`` of entry to remove. In the case that a line does not have a ``mapping`` entry, you can give the full value to remove. .. code-block:: python # header is a vcfpy.Header, e.g., as read earlier from file new_header = vcfpy.without_header_lines( header, [('assembly', None), ('FILTER', 'PASS')]) # now, the header lines starting with ""##assembly="" and the ""PASS"" # filter line will be missing from new_header """""" remove = set(remove) # Copy over lines that are not removed lines = [] for line in header.lines: if hasattr(line, ""mapping""): if (line.key, line.mapping.get(""ID"", None)) in remove: continue # filter out else: if (line.key, line.value) in remove: continue # filter out lines.append(line) return Header(lines, header.samples)",2
792,Python,concatenate several file remove header lines,https://github.com/totalgood/nlpia/blob/efa01126275e9cd3c3a5151a644f1c798a9ec53f/src/nlpia/clean_alice.py#L59-L82,"def concatenate_aiml(path='aiml-en-us-foundation-alice.v1-9.zip', outfile='aiml-en-us-foundation-alice.v1-9.aiml'): """"""Strip trailing </aiml> tag and concatenate all valid AIML files found in the ZIP."""""" path = find_data_path(path) or path zf = zipfile.ZipFile(path) for name in zf.namelist(): if not name.lower().endswith('.aiml'): continue with zf.open(name) as fin: happyending = '#!*@!!BAD' for i, line in enumerate(fin): try: line = line.decode('utf-8').strip() except UnicodeDecodeError: line = line.decode('ISO-8859-1').strip() if line.lower().startswith('</aiml>') or line.lower().endswith('</aiml>'): happyending = (i, line) break else: pass if happyending != (i, line): print('Invalid AIML format: {}\nLast line (line number {}) was: {}\nexpected ""</aiml>""'.format( name, i, line))",2
1393,Python,concatenate several file remove header lines,https://github.com/danielhrisca/asammdf/blob/3c7a1fd19c957ceebe4dcdbb2abf00806c2bdb66/asammdf/mdf.py#L1821-L2190,"def concatenate(files, version=""4.10"", sync=True, add_samples_origin=False, **kwargs): """""" concatenates several files. The files must have the same internal structure (same number of groups, and same channels in each group) Parameters ---------- files : list | tuple list of *MDF* file names or *MDF* instances version : str merged file version sync : bool sync the files based on the start of measurement, default *True* add_samples_origin : bool option to create a new ""__samples_origin"" channel that will hold the index of the measurement from where each timestamp originated Returns ------- concatenate : MDF new *MDF* object with concatenated channels Raises ------ MdfException : if there are inconsistencies between the files """""" if not files: raise MdfException(""No files given for merge"") callback = kwargs.get(""callback"", None) if callback: callback(0, 100) mdf_nr = len(files) versions = [] if sync: timestamps = [] for file in files: if isinstance(file, MDF): timestamps.append(file.header.start_time) versions.append(file.version) else: with open(file, ""rb"") as mdf: mdf.seek(64) blk_id = mdf.read(2) if blk_id == b""HD"": header = HeaderV3 versions.append(""3.00"") else: versions.append(""4.00"") blk_id += mdf.read(2) if blk_id == b""##HD"": header = HeaderV4 else: raise MdfException(f'""{file}"" is not a valid MDF file') header = header(address=64, stream=mdf) timestamps.append(header.start_time) try: oldest = min(timestamps) except TypeError: timestamps = [ timestamp.astimezone(timezone.utc) for timestamp in timestamps ] oldest = min(timestamps) offsets = [(timestamp - oldest).total_seconds() for timestamp in timestamps] offsets = [offset if offset > 0 else 0 for offset in offsets] else: file = files[0] if isinstance(file, MDF): oldest = file.header.start_time versions.append(file.version) else: with open(file, ""rb"") as mdf: mdf.seek(64) blk_id = mdf.read(2) if blk_id == b""HD"": versions.append(""3.00"") header = HeaderV3 else: versions.append(""4.00"") blk_id += mdf.read(2) if blk_id == b""##HD"": header = HeaderV4 else: raise MdfException(f'""{file}"" is not a valid MDF file') header = header(address=64, stream=mdf) oldest = header.start_time offsets = [0 for _ in files] version = validate_version_argument(version) merged = MDF(version=version, callback=callback) merged.header.start_time = oldest encodings = [] included_channel_names = [] if add_samples_origin: origin_conversion = {} for i, mdf in enumerate(files): origin_conversion[f'val_{i}'] = i if isinstance(mdf, MDF): origin_conversion[f'text_{i}'] = str(mdf.name) else: origin_conversion[f'text_{i}'] = str(mdf) origin_conversion = from_dict(origin_conversion) for mdf_index, (offset, mdf) in enumerate(zip(offsets, files)): if not isinstance(mdf, MDF): mdf = MDF(mdf) try: for can_id, info in mdf.can_logging_db.items(): if can_id not in merged.can_logging_db: merged.can_logging_db[can_id] = {} merged.can_logging_db[can_id].update(info) except AttributeError: pass if mdf_index == 0: last_timestamps = [None for gp in mdf.groups] groups_nr = len(mdf.groups) cg_nr = -1 for i, group in enumerate(mdf.groups): included_channels = mdf._included_channels(i) if mdf_index == 0: included_channel_names.append( [group.channels[k].name for k in included_channels] ) else: names = [group.channels[k].name for k in included_channels] if names != included_channel_names[i]: if sorted(names) != sorted(included_channel_names[i]): raise MdfException(f""internal structure of file {mdf_index} is different"") else: logger.warning( f'Different channel order in channel group {i} of file {mdf_index}.' ' Data can be corrupted if the there are channels with the same ' 'name in this channel group' ) included_channels = [ mdf._validate_channel_selection( name=name_, group=i, )[1] for name_ in included_channel_names[i] ] if included_channels: cg_nr += 1 else: continue channels_nr = len(group.channels) y_axis = MERGE idx = np.searchsorted(CHANNEL_COUNT, channels_nr, side=""right"") - 1 if idx < 0: idx = 0 read_size = y_axis[idx] idx = 0 last_timestamp = last_timestamps[i] first_timestamp = None original_first_timestamp = None if read_size: mdf.configure(read_fragment_size=int(read_size)) parents, dtypes = mdf._prepare_record(group) data = mdf._load_data(group) for fragment in data: if dtypes.itemsize: group.record = np.core.records.fromstring( fragment[0], dtype=dtypes ) else: group.record = None if mdf_index == 0 and idx == 0: encodings_ = [] encodings.append(encodings_) signals = [] for j in included_channels: sig = mdf.get( group=i, index=j, data=fragment, raw=True, ignore_invalidation_bits=True, copy_master=False, ) if version < ""4.00"": if sig.samples.dtype.kind == ""S"": encodings_.append(sig.encoding) strsig = mdf.get( group=i, index=j, samples_only=True, ignore_invalidation_bits=True, )[0] sig.samples = sig.samples.astype(strsig.dtype) del strsig if sig.encoding != ""latin-1"": if sig.encoding == ""utf-16-le"": sig.samples = ( sig.samples.view(np.uint16) .byteswap() .view(sig.samples.dtype) ) sig.samples = encode( decode(sig.samples, ""utf-16-be""), ""latin-1"", ) else: sig.samples = encode( decode(sig.samples, sig.encoding), ""latin-1"", ) else: encodings_.append(None) if not sig.samples.flags.writeable: sig.samples = sig.samples.copy() signals.append(sig) if signals and len(signals[0]): if offset > 0: timestamps = sig[0].timestamps + offset for sig in signals: sig.timestamps = timestamps last_timestamp = signals[0].timestamps[-1] first_timestamp = signals[0].timestamps[0] original_first_timestamp = first_timestamp if add_samples_origin: if signals: _s = signals[-1] signals.append( Signal( samples=np.ones(len(_s), dtype='<u2')*mdf_index, timestamps=_s.timestamps, conversion=origin_conversion, name='__samples_origin', ) ) _s = None if signals: merged.append(signals, common_timebase=True) try: if group.channel_group.flags & v4c.FLAG_CG_BUS_EVENT: merged.groups[-1].channel_group.flags = group.channel_group.flags merged.groups[-1].channel_group.acq_name = group.channel_group.acq_name merged.groups[-1].channel_group.acq_source = group.channel_group.acq_source merged.groups[-1].channel_group.comment = group.channel_group.comment except AttributeError: pass else: break idx += 1 else: master = mdf.get_master(i, fragment, copy_master=False) _copied = False if len(master): if original_first_timestamp is None: original_first_timestamp = master[0] if offset > 0: master = master + offset _copied = True if last_timestamp is None: last_timestamp = master[-1] else: if last_timestamp >= master[0]: if len(master) >= 2: delta = master[1] - master[0] else: delta = 0.001 if _copied: master -= master[0] else: master = master - master[0] _copied = True master += last_timestamp + delta last_timestamp = master[-1] signals = [(master, None)] for k, j in enumerate(included_channels): sig = mdf.get( group=i, index=j, data=fragment, raw=True, samples_only=True, ignore_invalidation_bits=True, ) signals.append(sig) if version < ""4.00"": encoding = encodings[i][k] samples = sig[0] if encoding: if encoding != ""latin-1"": if encoding == ""utf-16-le"": samples = ( samples.view(np.uint16) .byteswap() .view(samples.dtype) ) samples = encode( decode(samples, ""utf-16-be""), ""latin-1"", ) else: samples = encode( decode(samples, encoding), ""latin-1"" ) sig.samples = samples if signals: if add_samples_origin: _s = signals[-1][0] signals.append( (np.ones(len(_s), dtype='<u2')*mdf_index, None) ) _s = None merged.extend(cg_nr, signals) if first_timestamp is None: first_timestamp = master[0] idx += 1 group.record = None last_timestamps[i] = last_timestamp if callback: callback(i + 1 + mdf_index * groups_nr, groups_nr * mdf_nr) if MDF._terminate: return merged._transfer_events(mdf) return merged",2
464,Python,concatenate several file remove header lines,https://github.com/utiasSTARS/pykitti/blob/d3e1bb81676e831886726cc5ed79ce1f049aef2c/pykitti/downloader/tracking.py#L26-L38,"def clean_file(filename): f = open(filename, 'r') new_lines = [] for line in f.readlines(): new_lines.append(line.rstrip()) f.close() f = open(filename, 'w') for line in new_lines: f.write(line + '\n') f.close() def clean_lsvm(lsvm_dir):",1
598,Python,concatenate several file remove header lines,https://github.com/LuminosoInsight/langcodes/blob/0cedf9ca257ebf7250de5d3a63ec33a7d198db58/langcodes/registry_parser.py#L6-L25,"def parse_file(file): """""" Take an open file containing the IANA subtag registry, and yield a dictionary of information for each subtag it describes. """""" lines = [] for line in file: line = line.rstrip('\n') if line == '%%': # This is a separator between items. Parse the data we've # collected and yield the result. yield from parse_item(lines) lines.clear() elif line.startswith(' '): # This is a continuation line. Concatenate it to the previous # line, including one of the spaces. lines[-1] += line[1:] else: lines.append(line) yield from parse_item(lines)",1
807,Python,concatenate several file remove header lines,https://github.com/tanghaibao/jcvi/blob/d2e31a77b6ade7f41f3b321febc2b4744d1cdeca/jcvi/formats/fpc.py#L96-L111,"def main(fpcfile): fw = sys.stdout f = FpcReader(fpcfile) # first several lines are comments header = '\t'.join(('bac_name', 'ctg_name', 'map_left', 'map_right', 'bands', 'probes', 'remark')) print(header, file=fw) recs = list(f) logging.debug(""%d records parsed"" % len(recs)) recs.sort(key=lambda x: (x.ctg_name, x.map_left, x.map_right)) for rec in recs: print(rec, file=fw)",1
808,Python,concatenate several file remove header lines,https://github.com/cdeboever3/cdpybio/blob/38efdf0e11d01bc00a135921cb91a19c03db5d5c/cdpybio/variants.py#L83-L110,"def vcf_as_df(fn): """""" Read VCF file into pandas DataFrame. Parameters: ----------- fn : str Path to VCF file. Returns ------- df : pandas.DataFrame The VCF file as a data frame. Note that all header information is thrown away. """""" header_lines = 0 with open(fn, 'r') as f: line = f.readline().strip() header_lines += 1 while line[0] == '#': line = f.readline().strip() header_lines += 1 header_lines -= 2 df = pd.read_table(fn, skiprows=header_lines, header=0) df.columns = ['CHROM'] + list(df.columns[1:]) return df",1
882,Python,concatenate several file remove header lines,https://github.com/wummel/linkchecker/blob/c2ce810c3fb00b895a841a7be6b2e78c64e7b042/linkcheck/cookies.py#L27-L45,"def from_file (filename): """"""Parse cookie data from a text file in HTTP header format. @return: list of tuples (headers, scheme, host, path) """""" entries = [] with open(filename) as fd: lines = [] for line in fd.readlines(): line = line.rstrip() if not line: if lines: entries.append(from_headers(""\r\n"".join(lines))) lines = [] else: lines.append(line) if lines: entries.append(from_headers(""\r\n"".join(lines))) return entries",1
1799,Python,concatenate several file remove header lines,https://github.com/konstantinstadler/pymrio/blob/d764aa0dd2150200e867a9713a98ddae203e12d4/pymrio/tools/ioutil.py#L459-L540,"def sniff_csv_format(csv_file, potential_sep=['\t', ',', ';', '|', '-', '_'], max_test_lines=10, zip_file=None): """""" Tries to get the separator, nr of index cols and header rows in a csv file Parameters ---------- csv_file: str Path to a csv file potential_sep: list, optional List of potential separators (delimiters) to test. Default: '\t', ',', ';', '|', '-', '_' max_test_lines: int, optional How many lines to test, default: 10 or available lines in csv_file zip_file: str, optional Path to a zip file containing the csv file (if any, default: None). If a zip file is given, the path given at 'csv_file' is assumed to be the path to the file within the zip_file. Returns ------- dict with sep: string (separator) nr_index_col: int nr_header_row: int Entries are set to None if inconsistent information in the file """""" def read_first_lines(filehandle): lines = [] for i in range(max_test_lines): line = ff.readline() if line == '': break try: line = line.decode('utf-8') except AttributeError: pass lines.append(line[:-1]) return lines if zip_file: with zipfile.ZipFile(zip_file, 'r') as zz: with zz.open(csv_file, 'r') as ff: test_lines = read_first_lines(ff) else: with open(csv_file, 'r') as ff: test_lines = read_first_lines(ff) sep_aly_lines = [sorted([(line.count(sep), sep) for sep in potential_sep if line.count(sep) > 0], key=lambda x: x[0], reverse=True) for line in test_lines] for nr, (count, sep) in enumerate(sep_aly_lines[0]): for line in sep_aly_lines: if line[nr][0] == count: break else: sep = None if sep: break nr_header_row = None nr_index_col = None if sep: nr_index_col = find_first_number(test_lines[-1].split(sep)) if nr_index_col: for nr_header_row, line in enumerate(test_lines): if find_first_number(line.split(sep)) == nr_index_col: break return dict(sep=sep, nr_header_row=nr_header_row, nr_index_col=nr_index_col)",1
933,Python,concatenate several file remove header lines,https://github.com/skoczen/inkblock/blob/099f834c1e9fc0938abaa8824725eeac57603f6c/inkblock/main.py#L151-L172,"def filename_generator(file_parts, new_m_time=None): # print ""filename_generator"" # print file_parts concat = """".join(file_parts) if concat in FILENAMES_GENERATED: # print FILENAMES_GENERATED[concat] return FILENAMES_GENERATED[concat] sha = """" if ""-inkmd"" not in file_parts[0]: for base in MEDIA_ROOTS: try: sha = ""%s-inkmd"" % md5(os.path.join(base, concat)) break except IOError: pass new_name = ''.join([file_parts[0], sha, file_parts[1]]) FILENAMES_GENERATED[concat] = new_name # print new_name return new_name",0
546,Python,buffered file reader read text,https://github.com/hyde/fswrap/blob/41e4ad6f7e9ba73eabe61bd97847cd284e3edbd2/fswrap.py#L282-L289,"def read_all(self, encoding='utf-8'): """""" Reads from the file and returns the content as a string. """""" logger.info(""Reading everything from %s"" % self) with codecs.open(self.path, 'r', encoding) as fin: read_text = fin.read() return read_text",3
310,Python,buffered file reader read text,https://github.com/internetarchive/warc/blob/8f05a000a23bbd6501217e37cfd862ffdf19da7f/warc/warc.py#L260-L263,def reader(self): if self._reader is None: self._reader = WARCReader(self.fileobj) return self._reader,2
657,Python,buffered file reader read text,https://github.com/ray-project/ray/blob/4eade036a0505e244c976f36aaa2d64386b5129b/python/ray/experimental/streaming/batched_queue.py#L222-L227,def read_next(self): if not self.read_buffer: self._read_next_batch() assert self.read_buffer self.read_item_offset += 1 return self.read_buffer.pop(0),2
1544,Python,buffered file reader read text,https://github.com/RaRe-Technologies/smart_open/blob/2dc8d60f223fc7b00a2000c56362a7bd6cd0850e/smart_open/http.py#L104-L134,"def read(self, size=-1): """""" Mimics the read call to a filehandle object. """""" logger.debug(""reading with size: %d"", size) if self.response is None: return b'' if size == 0: return b'' elif size < 0 and len(self._read_buffer) == 0: retval = self.response.raw.read() elif size < 0: retval = self._read_buffer.read() + self.response.raw.read() else: while len(self._read_buffer) < size: logger.debug(""http reading more content at current_pos: %d with size: %d"", self._current_pos, size) bytes_read = self._read_buffer.fill(self._read_iter) if bytes_read == 0: # Oops, ran out of data early. retval = self._read_buffer.read() self._current_pos += len(retval) return retval # If we got here, it means we have enough data in the buffer # to return to the caller. retval = self._read_buffer.read(size) self._current_pos += len(retval) return retval",2
276,Python,buffered file reader read text,https://github.com/saghul/evergreen/blob/22f22f45892f397c23c3e09e6ea1ad4c00b3add8/evergreen/io/stream.py#L96-L102,"def _read_from_buffer(self, delimiter=None, nbytes=None, regex=None): if nbytes is not None: return self._read_buffer.read(nbytes) elif delimiter is not None: return self._read_buffer.read_until(delimiter) elif regex is not None: return self._read_buffer.read_until_regex(regex)",1
1111,Python,buffered file reader read text,https://github.com/LionelAuroux/pyrser/blob/f153a97ef2b6bf915a1ed468c0252a9a59b754d5/pyrser/parsing/base.py#L314-L327,"def read_text(self, text: str) -> bool: """""" Consume a strlen(text) text at current position in the stream else return False. Same as """" in BNF ex : read_text(""ls"");. """""" if self.read_eof(): return False self._stream.save_context() if self.peek_text(text): self._stream.incpos(len(text)) return self._stream.validate_context() return self._stream.restore_context()",1
668,Python,buffered file reader read text,https://github.com/NoneGG/aredis/blob/204caad740ac13e5760d46444a2ba7632982a046/aredis/connection.py#L168-L172,"def __init__(self, read_size): self._stream = None self._buffer = None self._read_size = read_size self.encoding = None",0
802,Python,buffered file reader read text,https://github.com/aragaer/channels/blob/ba6e4bf8a093deb6224a8b5b63ddb328815d1ae6/channels/channel.py#L13-L21,"def __init__(self, *, buffering='bytes'): self._buffer = b'' self._lf = 0 self._buffering = buffering if buffering == 'line': self.read = self._readline else: self.read = self._read",0
879,Python,buffered file reader read text,https://github.com/waqasbhatti/astrobase/blob/2922a14619d183fb28005fa7d02027ac436f2265/astrobase/hatsurveys/texthatlc.py#L54-L167,"def read_original_textlc(lcpath): ''' Read .epdlc, and .tfalc light curves and return a corresponding labelled dict (if LC from <2012) or astropy table (if >=2012). Each has different keys that can be accessed via .keys() Input: lcpath: path (string) to light curve data, which is a textfile with HAT LC data. Example: dat = read_original_textlc('HAT-115-0003266.epdlc') ''' LOGINFO('reading original HAT text LC: {:s}'.format(lcpath)) N_lines_to_parse_comments = 50 with open(lcpath, 'rb') as file: head = [next(file) for ind in range(N_lines_to_parse_comments)] N_comment_lines = len([l for l in head if l.decode('UTF-8')[0] == '#']) # if there are too many comment lines, fail out if N_comment_lines < N_lines_to_parse_comments: LOGERROR( 'LC file {fpath} has too many comment lines'.format(fpath=lcpath) ) return None first_data_line = list( filter(None, head[N_comment_lines].decode('UTF-8').split()) ) N_cols = len(first_data_line) # There are different column formats depending on when HAT pipeline was run # also different formats for different types of LCs: # pre-2012: .epdlc -> 17 columns # pre-2012: .tfalc -> 20 columns # post-2012: .epdlc or .tfalc -> 32 columns if N_cols == 17: colformat = 'pre2012-epdlc' elif N_cols == 20: colformat = 'pre2012-tfalc' elif N_cols == 32: colformat = 'post2012-hatlc' else: LOGERROR(""can't handle this column format yet, "" ""file: {fpath}, ncols: {ncols}"".format(fpath=lcpath, ncols=N_cols)) return None # deal with pre-2012 column format if colformat == 'pre2012-epdlc': col_names = ['framekey','rjd', 'aim_000','aie_000','aiq_000', 'aim_001','aie_001','aiq_001', 'aim_002','aie_002','aiq_002', 'arm_000','arm_001','arm_002', 'aep_000','aep_001','aep_002'] col_dtypes = ['U8',float, float,float,'U1', float,float,'U1', float,float,'U1', float,float,float, float,float,float] dtype_pairs = [el for el in zip(col_names, col_dtypes)] data = np.genfromtxt(lcpath, names=col_names, dtype=col_dtypes, skip_header=N_comment_lines, delimiter=None) out = {} for ix in range(len(data.dtype.names)): out[data.dtype.names[ix]] = data[data.dtype.names[ix]] elif colformat == 'pre2012-tfalc': col_names = ['framekey','rjd', 'aim_000','aie_000','aiq_000', 'aim_001','aie_001','aiq_001', 'aim_002','aie_002','aiq_002', 'arm_000','arm_001','arm_002', 'aep_000','aep_001','aep_002', 'atf_000','atf_001','atf_002'] col_dtypes = ['U8',float, float,float,'U1', float,float,'U1', float,float,'U1', float,float,float, float,float,float, float,float,float] dtype_pairs = [el for el in zip(col_names, col_dtypes)] data = np.genfromtxt(lcpath, names=col_names, dtype=col_dtypes, skip_header=N_comment_lines, delimiter=None) out = {} for ix in range(len(data.dtype.names)): out[data.dtype.names[ix]] = data[data.dtype.names[ix]] elif colformat == 'post2012-hatlc': col_names = ['hatid', 'framekey', 'fld', 'bjd', 'aim_000', 'aie_000', 'aiq_000', 'aim_001', 'aie_001', 'aiq_001', 'aim_002', 'aie_002', 'aiq_002', 'arm_000', 'arm_001', 'arm_002', 'aep_000', 'aep_001', 'aep_002', 'atf_000', 'atf_001', 'atf_002', 'xcc', 'ycc', 'bgv', 'bge', 'fsv', 'fdv', 'fkv', 'iha', 'izd', 'rjd'] out = astascii.read(lcpath, names=col_names, comment='#') return out",0
995,Python,buffered file reader read text,https://github.com/it-geeks-club/pyspectator/blob/356a808b1b29575fd47a85a2611fe50f1afeea8a/pyspectator/temperature_reader.py#L30-L35,"def reader1(cls, file): def reader(file): temperature = open(file).read().strip() temperature = int(temperature) // 1000 return temperature return partial(reader, file)",0
92,Python,binomial distribution,https://github.com/lpantano/seqcluster/blob/774e23add8cd4fdc83d626cea3bd1f458e7d060d/seqcluster/libs/thinkbayes.py#L1530-L1535,"def EvalBinomialPmf(k, n, p): """"""Evaluates the binomial pmf. Returns the probabily of k successes in n trials with probability p. """""" return scipy.stats.binom.pmf(k, n, p)",3
412,Python,binomial distribution,https://github.com/QInfer/python-qinfer/blob/8170c84a0be1723f8c6b09e0d3c7a40a886f1fe3/src/qinfer/distributions.py#L1078-L1082,"def sample(self, n=1): p_vals = self._p_dist.rvs(size=n)[:, np.newaxis] # numpy.random.binomial supports sampling using different p values, # whereas scipy does not. return np.random.binomial(self.n, p_vals)",3
1317,Python,binomial distribution,https://github.com/tisimst/mcerp/blob/2bb8260c9ad2d58a806847f1b627b6451e407de1/mcerp/__init__.py#L1150-L1167,"def Binomial(n, p, tag=None): """""" A Binomial random variate Parameters ---------- n : int The number of trials p : scalar The probability of success """""" assert ( int(n) == n and n > 0 ), 'Binomial number of trials ""n"" must be an integer greater than zero' assert ( 0 < p < 1 ), 'Binomial probability ""p"" must be between zero and one, non-inclusive' return uv(ss.binom(n, p), tag=tag)",3
1624,Python,binomial distribution,https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/qc/utils.py#L30-L40,"def binomial(n,k): """""" Binomial coefficient >>> binomial(5,2) 10 >>> binomial(10,5) 252 """""" if n==k: return 1 assert n>k, ""Attempting to call binomial(%d,%d)"" % (n,k) return factorial(n)//(factorial(k)*factorial(n-k))",3
599,Python,binomial distribution,https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/qc/one.py#L133-L144,"def binomial_prefactor(s,ia,ib,xpa,xpb): """""" The integral prefactor containing the binomial coefficients from Augspurger and Dykstra. >>> binomial_prefactor(0,0,0,0,0) 1 """""" total= 0 for t in range(s+1): if s-ia <= t <= ib: total += binomial(ia,s-t)*binomial(ib,t)* \ pow(xpa,ia-s+t)*pow(xpb,ib-t) return total",2
126,Python,binomial distribution,https://github.com/TeamHG-Memex/MaybeDont/blob/34721f67b69d426adda324a0ed905d3860828af9/maybedont/predict.py#L275-L283,"def get_prob(self): if self.total < 5: return 0. a, b = self.dup + 1, self.nodup + 1 n = a + b p = a / n q = b / n # Lower edge of the 95% confidence interval, binomial distribution return p - 1.96 * math.sqrt(p * q / n)",1
182,Python,binomial distribution,https://github.com/raphaelvallat/pingouin/blob/58b19fa4fffbfe09d58b456e3926a148249e4d9b/pingouin/distribution.py#L274-L319,"def anderson(*args, dist='norm'): """"""Anderson-Darling test of distribution. Parameters ---------- sample1, sample2,... : array_like Array of sample data. May be different lengths. dist : string Distribution ('norm', 'expon', 'logistic', 'gumbel') Returns ------- from_dist : boolean True if data comes from this distribution. sig_level : float The significance levels for the corresponding critical values in %. (See :py:func:`scipy.stats.anderson` for more details) Examples -------- 1. Test that an array comes from a normal distribution >>> from pingouin import anderson >>> x = [2.3, 5.1, 4.3, 2.6, 7.8, 9.2, 1.4] >>> anderson(x, dist='norm') (False, 15.0) 2. Test that two arrays comes from an exponential distribution >>> y = [2.8, 12.4, 28.3, 3.2, 16.3, 14.2] >>> anderson(x, y, dist='expon') (array([False, False]), array([15., 15.])) """""" from scipy.stats import anderson as ads k = len(args) from_dist = np.zeros(k, 'bool') sig_level = np.zeros(k) for j in range(k): st, cr, sig = ads(args[j], dist=dist) from_dist[j] = True if (st > cr).any() else False sig_level[j] = sig[np.argmin(np.abs(st - cr))] if k == 1: from_dist = bool(from_dist) sig_level = float(sig_level) return from_dist, sig_level",1
554,Python,binomial distribution,https://github.com/davidfokkema/artist/blob/26ae7987522622710f2910980770c50012fda47d/demo/demo_histogram_fit.py#L8-L45,"def main(): # Draw random numbers from the normal distribution np.random.seed(1) N = np.random.normal(size=2000) # define bin edges edge = 5 bin_width = .1 bins = np.arange(-edge, edge + .5 * bin_width, bin_width) # build histogram and x, y values at the center of the bins n, bins = np.histogram(N, bins=bins) x = (bins[:-1] + bins[1:]) / 2 y = n # fit normal distribution pdf to data f = lambda x, N, mu, sigma: N * scipy.stats.norm.pdf(x, mu, sigma) popt, pcov = scipy.optimize.curve_fit(f, x, y) print(""Parameters from fit (N, mu, sigma):"", popt) # make graph graph = Plot() # graph histogram graph.histogram(n, bins) # graph model with fit parameters x = np.linspace(-edge, edge, 100) graph.plot(x, f(x, *popt), mark=None) # set labels and limits graph.set_xlabel(""value"") graph.set_ylabel(""count"") graph.set_label(""Fit to data"") graph.set_xlimits(-6, 6) # save graph to file graph.save('histogram-fit')",1
59,Python,binomial distribution,https://github.com/econ-ark/HARK/blob/3d184153a189e618a87c9540df1cd12044039cc5/HARK/utilities.py#L832-L907,"def combineIndepDstns(*distributions): ''' Given n lists (or tuples) whose elements represent n independent, discrete probability spaces (probabilities and values), construct a joint pmf over all combinations of these independent points. Can take multivariate discrete distributions as inputs. Parameters ---------- distributions : [np.array] Arbitrary number of distributions (pmfs). Each pmf is a list or tuple. For each pmf, the first vector is probabilities and all subsequent vectors are values. For each pmf, this should be true: len(X_pmf[0]) == len(X_pmf[j]) for j in range(1,len(distributions)) Returns ------- List of arrays, consisting of: P_out: np.array Probability associated with each point in X_out. X_out: np.array (as many as in *distributions) Discrete points for the joint discrete probability mass function. Written by Nathan Palmer Latest update: 5 July August 2017 by Matthew N White ''' # Very quick and incomplete parameter check: for dist in distributions: assert len(dist[0]) == len(dist[-1]), ""len(dist[0]) != len(dist[-1])"" # Get information on the distributions dist_lengths = () dist_dims = () for dist in distributions: dist_lengths += (len(dist[0]),) dist_dims += (len(dist)-1,) number_of_distributions = len(distributions) # Initialize lists we will use X_out = [] P_temp = [] # Now loop through the distributions, tiling and flattening as necessary. for dd,dist in enumerate(distributions): # The shape we want before we tile dist_newshape = (1,) * dd + (len(dist[0]),) + \ (1,) * (number_of_distributions - dd) # The tiling we want to do dist_tiles = dist_lengths[:dd] + (1,) + dist_lengths[dd+1:] # Now we are ready to tile. # We don't use the np.meshgrid commands, because they do not # easily support non-symmetric grids. # First deal with probabilities Pmesh = np.tile(dist[0].reshape(dist_newshape),dist_tiles) # Tiling flatP = Pmesh.ravel() # Flatten the tiled arrays P_temp += [flatP,] #Add the flattened arrays to the output lists # Then loop through each value variable for n in range(1,dist_dims[dd]+1): Xmesh = np.tile(dist[n].reshape(dist_newshape),dist_tiles) flatX = Xmesh.ravel() X_out += [flatX,] # We're done getting the flattened X_out arrays we wanted. # However, we have a bunch of flattened P_temp arrays, and just want one # probability array. So get the probability array, P_out, here. P_out = np.prod(np.array(P_temp),axis=0) assert np.isclose(np.sum(P_out),1),'Probabilities do not sum to 1!' return [P_out,] + X_out",0
519,Python,binomial distribution,https://github.com/pysal/mapclassify/blob/5b22ec33f5802becf40557614d90cd38efa1676e/mapclassify/classifiers.py#L231-L278,"def bin1d(x, bins): """""" Place values of a 1-d array into bins and determine counts of values in each bin Parameters ---------- x : array (n, 1), values to bin bins : array (k,1), upper bounds of each bin (monotonic) Returns ------- binIds : array 1-d array of integer bin Ids counts : int number of elements of x falling in each bin Examples -------- >>> import numpy as np >>> import mapclassify as mc >>> x = np.arange(100, dtype = 'float') >>> bins = [25, 74, 100] >>> binIds, counts = mc.classifiers.bin1d(x, bins) >>> binIds array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) >>> counts array([26, 49, 25]) """""" left = [-float(""inf"")] left.extend(bins[0:-1]) right = bins cuts = list(zip(left, right)) k = len(bins) binIds = np.zeros(x.shape, dtype='int') while cuts: k -= 1 l, r = cuts.pop(-1) binIds += (x > l) * (x <= r) * k counts = np.bincount(binIds, minlength=len(bins)) return (binIds, counts)",0
643,Python,all permutations of a list,https://github.com/erikrose/more-itertools/blob/6a91b4e25c8e12fcf9fc2b53cf8ee0fba293e6f9/more_itertools/more.py#L528-L566,"def distinct_permutations(iterable): """"""Yield successive distinct permutations of the elements in *iterable*. >>> sorted(distinct_permutations([1, 0, 1])) [(0, 1, 1), (1, 0, 1), (1, 1, 0)] Equivalent to ``set(permutations(iterable))``, except duplicates are not generated and thrown away. For larger input sequences this is much more efficient. Duplicate permutations arise when there are duplicated elements in the input iterable. The number of items returned is `n! / (x_1! * x_2! * ... * x_n!)`, where `n` is the total number of items input, and each `x_i` is the count of a distinct item in the input sequence. """""" def make_new_permutations(permutations, e): """"""Internal helper function. The output permutations are built up by adding element *e* to the current *permutations* at every possible position. The key idea is to keep repeated elements (reverse) ordered: if e1 == e2 and e1 is before e2 in the iterable, then all permutations with e1 before e2 are ignored. """""" for permutation in permutations: for j in range(len(permutation)): yield permutation[:j] + [e] + permutation[j:] if permutation[j] == e: break else: yield permutation + [e] permutations = [[]] for e in iterable: permutations = make_new_permutations(permutations, e) return (tuple(t) for t in permutations)",3
815,Python,all permutations of a list,https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/math/cross.py#L96-L102,def permutations(x): if len(x) > 1: for permutation in permutations(x[1:]): # Stick the first digit in every position. for i in xrange(len(permutation)+1): yield permutation[:i] + x[0:1] + permutation[i:] else: yield x,3
1202,Python,all permutations of a list,https://github.com/CellProfiler/centrosome/blob/7bd9350a2d4ae1b215b81eabcecfe560bbb1f32a/centrosome/filter.py#L1427-L1476,"def permutations(x): '''Given a listlike, x, return all permutations of x Returns the permutations of x in the lexical order of their indices: e.g. >>> x = [ 1, 2, 3, 4 ] >>> for p in permutations(x): >>> print p [ 1, 2, 3, 4 ] [ 1, 2, 4, 3 ] [ 1, 3, 2, 4 ] [ 1, 3, 4, 2 ] [ 1, 4, 2, 3 ] [ 1, 4, 3, 2 ] [ 2, 1, 3, 4 ] ... [ 4, 3, 2, 1 ] ''' # # The algorithm is attributed to Narayana Pandit from his # Ganita Kaumundi (1356). The following is from # # http://en.wikipedia.org/wiki/Permutation#Systematic_generation_of_all_permutations # # 1. Find the largest index k such that a[k] < a[k + 1]. # If no such index exists, the permutation is the last permutation. # 2. Find the largest index l such that a[k] < a[l]. # Since k + 1 is such an index, l is well defined and satisfies k < l. # 3. Swap a[k] with a[l]. # 4. Reverse the sequence from a[k + 1] up to and including the final # element a[n]. # yield list(x) # don't forget to do the first one x = np.array(x) a = np.arange(len(x)) while True: # 1 - find largest or stop ak_lt_ak_next = np.argwhere(a[:-1] < a[1:]) if len(ak_lt_ak_next) == 0: raise StopIteration() k = ak_lt_ak_next[-1, 0] # 2 - find largest a[l] < a[k] ak_lt_al = np.argwhere(a[k] < a) l = ak_lt_al[-1, 0] # 3 - swap a[k], a[l] = (a[l], a[k]) # 4 - reverse if k < len(x)-1: a[k+1:] = a[:k:-1].copy() yield x[a].tolist()",3
268,Python,all permutations of a list,https://github.com/KarchinLab/probabilistic2020/blob/5d70583b0a7c07cfe32e95f3a70e05df412acb84/prob2020/python/permutation.py#L9-L96,"def deleterious_permutation(obs_del, context_counts, context_to_mut, seq_context, gene_seq, num_permutations=10000, stop_criteria=100, pseudo_count=0, max_batch=25000): """"""Performs null-permutations for deleterious mutation statistics in a single gene. Parameters ---------- context_counts : pd.Series number of mutations for each context context_to_mut : dict dictionary mapping nucleotide context to a list of observed somatic base changes. seq_context : SequenceContext Sequence context for the entire gene sequence (regardless of where mutations occur). The nucleotide contexts are identified at positions along the gene. gene_seq : GeneSequence Sequence of gene of interest num_permutations : int, default: 10000 number of permutations to create for null pseudo_count : int, default: 0 Pseudo-count for number of deleterious mutations for each permutation of the null distribution. Increasing pseudo_count makes the statistical test more stringent. Returns ------- del_count_list : list list of deleterious mutation counts under the null """""" mycontexts = context_counts.index.tolist() somatic_base = [base for one_context in mycontexts for base in context_to_mut[one_context]] # calculate the # of batches for simulations max_batch = min(num_permutations, max_batch) num_batches = num_permutations // max_batch remainder = num_permutations % max_batch batch_sizes = [max_batch] * num_batches if remainder: batch_sizes += [remainder] num_sim = 0 null_del_ct = 0 for j, batch_size in enumerate(batch_sizes): # stop iterations if reached sufficient precision if null_del_ct >= stop_criteria: #j = j - 1 break # get random positions determined by sequence context tmp_contxt_pos = seq_context.random_pos(context_counts.iteritems(), batch_size) tmp_mut_pos = np.hstack(pos_array for base, pos_array in tmp_contxt_pos) # determine result of random positions for i, row in enumerate(tmp_mut_pos): # get info about mutations tmp_mut_info = mc.get_aa_mut_info(row, somatic_base, gene_seq) # calc deleterious mutation info tmp_del_count = cutils.calc_deleterious_info(tmp_mut_info['Reference AA'], tmp_mut_info['Somatic AA'], tmp_mut_info['Codon Pos']) # update empricial null distribution if tmp_del_count >= obs_del: null_del_ct += 1 # stop if reach sufficient precision on p-value if null_del_ct >= stop_criteria: break # update number of simulations num_sim += i + 1 #num_sim = j*max_batch + i+1 del_pval = float(null_del_ct) / (num_sim) return del_pval",2
1967,Python,all permutations of a list,https://github.com/GetmeUK/MongoFrames/blob/7d2bd792235dfa77a9deecab5366f5f73480823d/mongoframes/factory/makers/selections.py#L231-L283,"def p(i, sample_size, weights): """""" Given a weighted set and sample size return the probabilty that the weight `i` will be present in the sample. Created to test the output of the `SomeOf` maker class. The math was provided by Andy Blackshaw - thank you dad :) """""" # Determine the initial pick values weight_i = weights[i] weights_sum = sum(weights) # Build a list of weights that don't contain the weight `i`. This list will # be used to build the possible picks before weight `i`. other_weights = list(weights) del other_weights[i] # Calculate the probability probability_of_i = 0 for picks in range(0, sample_size): # Build the list of possible permutations for this pick in the sample permutations = list(itertools.permutations(other_weights, picks)) # Calculate the probability for this permutation permutation_probabilities = [] for permutation in permutations: # Calculate the probability for each pick in the permutation pick_probabilities = [] pick_weight_sum = weights_sum for pick in permutation: pick_probabilities.append(pick / pick_weight_sum) # Each time we pick we update the sum of the weight the next # pick is from. pick_weight_sum -= pick # Add the probability of picking i as the last pick pick_probabilities += [weight_i / pick_weight_sum] # Multiply all the probabilities for the permutation together permutation_probability = reduce( lambda x, y: x * y, pick_probabilities ) permutation_probabilities.append(permutation_probability) # Add together all the probabilities for all permutations together probability_of_i += sum(permutation_probabilities) return probability_of_i",2
230,Python,all permutations of a list,https://github.com/ThreatConnect-Inc/tcex/blob/dd4d7a1ef723af1561687120191886b9a2fd4b47/tcex/tcex_bin_profile.py#L259-L268,"def print_permutations(self): """"""Print all valid permutations."""""" index = 0 permutations = [] for p in self._input_permutations: permutations.append({'index': index, 'args': p}) index += 1 with open('permutations.json', 'w') as fh: json.dump(permutations, fh, indent=2) print('All permutations written to the ""permutations.json"" file.')",1
636,Python,all permutations of a list,https://github.com/mabuchilab/QNET/blob/cc20d26dad78691d34c67173e5cd67dcac94208a/src/qnet/algebra/core/circuit_algebra.py#L822-L834,"def series_with_permutation(self, other): """"""Compute the series product with another channel permutation circuit Args: other (CPermutation): Returns: Circuit: The composite permutation circuit (could also be the identity circuit for n channels) """""" combined_permutation = tuple([self.permutation[p] for p in other.permutation]) return CPermutation.create(combined_permutation)",1
1155,Python,all permutations of a list,https://github.com/tamasgal/km3pipe/blob/7a9b59ac899a28775b5bdc5d391d9a5340d08040/km3pipe/math.py#L224-L229,"def circ_permutation(items): """"""Calculate the circular permutation for a given list of items."""""" permutations = [] for i in range(len(items)): permutations.append(items[i:] + items[:i]) return permutations",1
2042,Python,all permutations of a list,https://github.com/mabuchilab/QNET/blob/cc20d26dad78691d34c67173e5cd67dcac94208a/src/qnet/utils/permutations.py#L166-L184,"def permutation_from_block_permutations(permutations): """"""Reverse operation to :py:func:`permutation_to_block_permutations` Compute the concatenation of permutations ``(1,2,0) [+] (0,2,1) --> (1,2,0,3,5,4)`` :param permutations: A list of permutation tuples ``[t = (t_0,...,t_n1), u = (u_0,...,u_n2),..., z = (z_0,...,z_nm)]`` :type permutations: list of tuples :return: permutation image tuple ``s = t [+] u [+] ... [+] z`` :rtype: tuple """""" offset = 0 new_perm = [] for p in permutations: new_perm[offset: offset +len(p)] = [p_i + offset for p_i in p] offset += len(p) return tuple(new_perm)",1
564,Python,all permutations of a list,https://github.com/awslabs/sockeye/blob/5d64a1ee1ef3cbba17c6d1d94bc061020c43f6ab/sockeye/data_io.py#L1424-L1451,"def permute(self, permutations: List[mx.nd.NDArray]) -> 'ParallelDataSet': """""" Permutes the data within each bucket. The permutation is received as an argument, allowing the data to be unpermuted (i.e., restored) later on. :param permutations: For each bucket, a permutation of the data within that bucket. :return: A new, permuted ParallelDataSet. """""" assert len(self) == len(permutations) source = [] target = [] label = [] for buck_idx in range(len(self)): num_samples = self.source[buck_idx].shape[0] if num_samples: # not empty bucket permutation = permutations[buck_idx] if isinstance(self.source[buck_idx], np.ndarray): source.append(self.source[buck_idx].take(np.int64(permutation.asnumpy()))) else: source.append(self.source[buck_idx].take(permutation)) target.append(self.target[buck_idx].take(permutation)) label.append(self.label[buck_idx].take(permutation)) else: source.append(self.source[buck_idx]) target.append(self.target[buck_idx]) label.append(self.label[buck_idx]) return ParallelDataSet(source, target, label)",0
26,Python,aes encryption,https://github.com/kalefranz/auxlib/blob/6ff2d6b57d128d0b9ed8f01ad83572e938da064f/auxlib/crypt.py#L77-L97,"def aes_encrypt(base64_encryption_key, data): """"""Encrypt data with AES-CBC and sign it with HMAC-SHA256 Arguments: base64_encryption_key (str): a base64-encoded string containing an AES encryption key and HMAC signing key as generated by generate_encryption_key() data (str): a byte string containing the data to be encrypted Returns: str: the encrypted data as a byte string with the HMAC signature appended to the end """""" if isinstance(data, text_type): data = data.encode(""UTF-8"") aes_key_bytes, hmac_key_bytes = _extract_keys(base64_encryption_key) data = _pad(data) iv_bytes = os.urandom(AES_BLOCK_SIZE) cipher = AES.new(aes_key_bytes, mode=AES.MODE_CBC, IV=iv_bytes) data = iv_bytes + cipher.encrypt(data) # prepend init vector hmac_signature = hmac.new(hmac_key_bytes, data, hashlib.sha256).digest() return as_base64(data + hmac_signature)",3
150,Python,aes encryption,https://github.com/jaredLunde/vital-tools/blob/ea924c9bbb6ec22aa66f8095f018b1ee0099ac04/vital/security/__init__.py#L79-L98,"def aes_encrypt(value, secret, block_size=AES.block_size): """""" AES encrypt @value with @secret using the |CFB| mode of AES with a cryptographically secure initialization vector. -> (#bytes) AES encrypted @value .. from vital.security import aes_encrypt, aes_decrypt aes_encrypt(""Hello, world"", ""aLWEFlwgwlreWELFNWEFWLEgwklgbweLKWEBGW"") # -> 'zYgVYMbeOuiHR50aMFinY9JsfyMQCvpzI+LNqNcmZhw=' aes_decrypt( ""zYgVYMbeOuiHR50aMFinY9JsfyMQCvpzI+LNqNcmZhw="", ""aLWEFlwgwlreWELFNWEFWLEgwklgbweLKWEBGW"") # -> 'Hello, world' ..f """""" iv = os.urandom(block_size * 2) cipher = AES.new(secret[:32], AES.MODE_CFB, iv[:block_size]) return b'%s%s' % (iv, cipher.encrypt(value))",3
204,Python,aes encryption,https://github.com/pedroburon/tbk/blob/ecd6741e0bae06269eb4ac885c3ffcb7902ee40e/tbk/webpay/encryption.py#L36-L42,"def encrypt_message(self, signed_message, message, key, iv): raw = signed_message + message block_size = AES.block_size pad = lambda s: s + (block_size - len(s) % block_size) * chr(block_size - len(s) % block_size).encode('utf-8') message_to_encrypt = pad(raw) cipher = AES.new(key, AES.MODE_CBC, iv) return cipher.encrypt(message_to_encrypt)",3
266,Python,aes encryption,https://github.com/raymontag/kppy/blob/a43f1fff7d49da1da4b3d8628a1b3ebbaf47f43a/kppy/database.py#L861-L871,"def _cbc_encrypt(self, content, final_key): """"""This method encrypts the content."""""" aes = AES.new(final_key, AES.MODE_CBC, self._enc_iv) padding = (16 - len(content) % AES.block_size) for _ in range(padding): content += chr(padding).encode() temp = bytes(content) return aes.encrypt(temp)",3
404,Python,aes encryption,https://github.com/ethereum/pyethereum/blob/b704a5c6577863edc539a1ec3d2620a443b950fb/ethereum/tools/keys.py#L56-L61,"def aes_ctr_encrypt(text, key, params): iv = big_endian_to_int(decode_hex(params[""iv""])) ctr = Counter.new(128, initial_value=iv, allow_wraparound=True) mode = AES.MODE_CTR encryptor = AES.new(key, mode, counter=ctr) return encryptor.encrypt(text)",3
1145,Python,aes encryption,https://github.com/konomae/lastpass-python/blob/5063911b789868a1fd9db9922db82cdf156b938a/lastpass/parser.py#L269-L284,"def decode_aes256(cipher, iv, data, encryption_key): """""" Decrypt AES-256 bytes. Allowed ciphers are: :ecb, :cbc. If for :ecb iv is not used and should be set to """". """""" if cipher == 'cbc': aes = AES.new(encryption_key, AES.MODE_CBC, iv) elif cipher == 'ecb': aes = AES.new(encryption_key, AES.MODE_ECB) else: raise ValueError('Unknown AES mode') d = aes.decrypt(data) # http://passingcuriosity.com/2009/aes-encryption-in-python-with-m2crypto/ unpad = lambda s: s[0:-ord(d[-1:])] return unpad(d)",3
1162,Python,aes encryption,https://github.com/jcassee/django-geckoboard/blob/6ebdaa86015fe645360abf1ba1290132de4cf6d6/django_geckoboard/decorators.py#L446-L460,"def _encrypt(data): """"""Equivalent to OpenSSL using 256 bit AES in CBC mode"""""" BS = AES.block_size def pad(s): n = BS - len(s) % BS char = chr(n).encode('utf8') return s + n * char password = settings.GECKOBOARD_PASSWORD salt = Random.new().read(BS - len('Salted__')) key, iv = _derive_key_and_iv(password, salt, 32, BS) cipher = AES.new(key, AES.MODE_CBC, iv) encrypted = b'Salted__' + salt + cipher.encrypt(pad(data)) return base64.b64encode(encrypted)",3
1359,Python,aes encryption,https://github.com/ontio/ontology-python-sdk/blob/ac88bdda941896c5d2ced08422a9c5179d3f9b19/ontology/crypto/aes_handler.py#L72-L76,"def aes_cbc_encrypt(plain_text: bytes, key: bytes, iv: bytes = b''): if len(iv) == 0: iv = AESHandler.generate_iv() cipher = AES.new(key=key, mode=AES.MODE_CBC, iv=iv) return cipher.IV, cipher.encrypt(pad(plain_text, AES.block_size))",3
14,Python,aes encryption,https://github.com/etingof/pysnmp/blob/cde062dd42f67dfd2d7686286a322d40e9c3a4b7/pysnmp/proto/secmod/rfc3414/priv/des.py#L108-L130,"def encryptData(self, encryptKey, privParameters, dataToEncrypt): snmpEngineBoots, snmpEngineTime, salt = privParameters # 8.3.1.1 desKey, salt, iv = self._getEncryptionKey(encryptKey, snmpEngineBoots) # 8.3.1.2 privParameters = univ.OctetString(salt) # 8.1.1.2 plaintext = dataToEncrypt plaintext += univ.OctetString( (0,) * (8 - len(dataToEncrypt) % 8)).asOctets() try: ciphertext = des.encrypt(plaintext, desKey, iv) except PysnmpCryptoError: raise error.StatusInformation( errorIndication=errind.unsupportedPrivProtocol) # 8.3.1.3 & 4 return univ.OctetString(ciphertext), privParameters",0
363,Python,aes encryption,https://github.com/frispete/keyrings.cryptfile/blob/cfa80d4848a5c3c0aeee41a954b2b120c80e69b2/keyrings/cryptfile/cryptfile.py#L132-L162,"def _check_scheme(self, config): """""" check for a valid scheme raise AttributeError if missing raise ValueError if not valid """""" try: scheme = config.get( escape_for_ini('keyring-setting'), escape_for_ini('scheme'), ) except (configparser.NoSectionError, configparser.NoOptionError): raise AttributeError(""Encryption scheme missing"") # extract AES mode aesmode = scheme[-3:] if aesmode not in self._get_mode(): raise ValueError(""Encryption scheme invalid: %s"" % (aesmode)) # setup AES mode self.aesmode = aesmode # remove pointless crypto module name if scheme.startswith('PyCryptodome '): scheme = scheme[13:] # check other scheme properties if scheme != self.scheme: raise ValueError(""Encryption scheme mismatch "" ""(exp.: %s, found: %s)"" % (self.scheme, scheme))",0
